---
title: 'Triage Against the Machine: Can AI Reason Deliberatively?'
author: "Francesco Veri, Gustavo Umbelino"
date: "`r Sys.Date()`"
bibliography: bibliography/refs.bib
link-citations: true
csl: bibliography/apsa.csl
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(reticulate)
library(tidyverse)
library(scales)
library(readxl)
library(psych)
library(boot)
library(car)
library(dunn.test)
library(vote) # might need to manually install: https://cran.r-project.org/doc/manuals/r-release/R-admin.html#Fortran-compiler

SURVEY_FILE <- "data/surveys_v5.xlsx"
EXEC_LOG_FILE <- "llm_data/exec_log.csv"
LLMS_FILE <- "private/llms_v3.csv"
HUMAN_DATA_FILE <- "data/total dataset_clean.csv"
SWISS_DATA_FILE <- "data/CA_PRE_POST.csv"
CASES_FILE <- "data/cases.csv"
EXPENSES_FILE <- "billing/transaction_history.xlsx"
OUTPUT_DIR <- "analysis"

ALPHA_RESULTS_FILE <- "alpha_results.csv"
AGGREGATION_RESULTS_FILE <- "llm_data_aggregated.csv"

MAX_ITERATIONS <- 30


# create output directory if it doesn't exist
if (!dir.exists(OUTPUT_DIR)) {
  dir.create(OUTPUT_DIR, recursive = TRUE)
}

```

```{r helper functions}

create_file_path <- function(provider, model, survey, file_type) {
  file.path("llm_data", provider, model, survey, paste0(file_type, ".csv"))
}

output.file.exists <- function(file_name) {
  file.exists(paste(OUTPUT_DIR, file_name, sep = "/"))
}

read_output_csv <- function(file_name) {
  read_csv(paste(OUTPUT_DIR, file_name, sep = "/"), show_col_types = FALSE)
}

write_output_csv <- function(data, file_name) {
  write_csv(data, paste(OUTPUT_DIR, file_name, sep = "/"))
}

now_utc <- function() {
   now <- Sys.time()
   attr(now, "tzone") <- "UTC"
   now
}

remove_NA_cols <- function(data) {
   data[, colSums(is.na(data)) != nrow(data)]
}

```

# Large-Language Models (LLMs) Preview

```{r models, warning=FALSE}

# get models info
models <- read_csv(LLMS_FILE, show_col_types = FALSE)

models %>%
  select(provider, model, num_parameters, context_length, arch, version) %>% 
  arrange(provider, model) %>%
  knitr::kable(caption = "LLMs", row.names = TRUE, col.names = c("Provider", "Model", "Parameters (B)", "Context Length", "Architecture", "Version"))

```

We started the analysis with `r nrow(models)` models, but some models were dropped after data collection. The models and reason for dropping are discussed later on [Excluded Models].

# Surveys

```{r surveys, warning=FALSE}

# read the sheet names of the Excel file
survey_names <- excel_sheets(SURVEY_FILE)

# remove invalid and "template" 
survey_names <- sort(survey_names[!grepl("^~", survey_names) & survey_names != "template"])

get_surveys <- function(survey_names) {
  
  surveys <- list()
  
  # Iterate over each sheet in the workbook
  for (survey_name in survey_names) {

    # Read the current sheet into a data frame
    df <- read_excel(SURVEY_FILE, sheet = survey_name)
    
    # Check if required columns exist
    required_columns <- c("considerations", "policies", "scale_max", "q-method")
    missing_cols <- setdiff(required_columns, colnames(df))
    if (length(missing_cols) > 0) {
      cat(
        "Sheet",
        survey_name,
        "is missing the following columns:",
        paste(missing_cols, collapse = ", "),
        "\n\n"
      )
      next
    }
    
    # Calculate the number of non-NA rows in "considerations" column
    n_c <- sum(!is.na(df$considerations))
    
    # Calculate the number of non-NA rows in "policies" column
    n_p <- sum(!is.na(df$policies))
    
    # Extract integer values from "scale_max" column, assuming they are already integers
    scale_max <- as.integer(na.omit(df$scale_max))
    
    # Extract logical (boolean) values from "q-method" column
    q_method <- as.logical(na.omit(df$`q-method`))
    
    surveys[[length(surveys) + 1]] <- tibble(
      survey = survey_name,
      considerations = n_c,
      policies = n_p,
      scale_max,
      q_method,
    )
    
  }
  
  surveys <- bind_rows(surveys)
  surveys
  
}


surveys <- get_surveys(survey_names)


# define file types
file_types <- c("considerations", "policies", "reasons")

surveys %>% arrange(survey) %>%
  knitr::kable(caption = "Surveys", row.names = TRUE)

```

# LLM Data Collection

```{r format LLM data}

get_llm_data <- function() {
  
  # initialize an empty list to store the data frames
  data_list <- list()
  
  # iterate over each survey
  for (survey_name in survey_names) {
    
    # iterate over each row in the models data frame
    for (i in 1:nrow(models)) {
      
      provider <- models$provider[i]
      model <- models$model[i]
      min_iterations <- models$min_iterations[i]
      
      # check if any file for the survey exists
      survey_path <- paste0("llm_data/", provider, "/", model, "/", survey_name, "/")
      if (!any(file.exists(paste0(survey_path, file_types, ".csv")))) {
        next
      }
      
      # iterate over each file type
      for (file_type in file_types) {
        # create the file path
        file_path <- create_file_path(provider, model, survey_name, file_type)
        
        # check if the file exists
        if (!file.exists(file_path)) {
          break
        }
        
        # read the CSV file
        temp_data <- read_csv(file_path, show_col_types = FALSE)
        
        # skip file if file exists but has no data
        if (nrow(temp_data) == 0) {
          break
        }
        
        # select the relevant columns based on file type
        if (file_type == "considerations") {
          # initialize survey_data
          survey_data <- temp_data %>%
            rename_with(~ paste0("C", seq_along(.)),
                        starts_with("C", ignore.case = FALSE))
          
          # add column "survey" to meta data
          survey_data <- survey_data %>%
            mutate(survey = survey_name) %>%
            relocate(survey, .after = model)
          
          # ensure survey_data has columns up to C50
          # skip 8 rows of meta data
          for (j in (ncol(survey_data) - 7):50) {
            survey_data[[paste0("C", j)]] <- as.numeric(NA)
          }
          
          # go to next file type
          next
          
        } else if (file_type == "policies") {
          temp_data <- temp_data %>%
            select(cuid, starts_with("P", ignore.case = FALSE)) %>%
            rename_with(~ paste0("P", seq_along(.)),
                        starts_with("P", ignore.case = FALSE))
          
          # ensure temp_data has columns up to P10
          for (j in (ncol(temp_data)):10) {
            temp_data[[paste0("P", j)]] <- as.numeric(NA)
          }
          
        } else if (file_type == "reasons") {
          temp_data <- temp_data %>%
            select(cuid, reason) %>%
            rename(R = reason)
        }
        
        # merge the data frames by 'cuid' and keep all rows
        survey_data <- full_join(survey_data, temp_data, by = c("cuid"))
        
      }
      
      # add the survey_data to the list
      if (exists("survey_data")) {
        data_list[[length(data_list) + 1]] <- survey_data
        
        # remove the survey_data data frame to free up memory
        rm(survey_data)
      }
    }
  }
  
  # Combine all data frames in the list into a single data frame
  llm_data <- bind_rows(data_list)
  
  return(llm_data)
  
}

# get llm data
llm_data <- get_llm_data()

# aggregate llm_data by provider, model, and survey and N the number of rows
llm_surveys <- llm_data %>%
  group_by(provider, model, temperature, survey) %>%
  summarise(
    N = n(),
    .groups = 'drop'
  )

models <- llm_surveys %>%
  group_by(provider, model, temperature) %>%
  summarise(
    surveys_with_data = n(),
    min_iterations_completed = min(N, na.rm = TRUE),
    max_iterations_completed = max(N, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  full_join(models, by = c("provider", "model")) %>%
  mutate(done = (min_iterations <= min_iterations_completed) &
           (surveys_with_data == length(survey_names)))

models["done"][is.na(models["done"])] <- FALSE

# write resutls to file
write_csv(llm_data, paste(OUTPUT_DIR, "llm_data.csv", sep = "/"))
write_csv(llm_surveys, paste(OUTPUT_DIR, "llm_surveys.csv", sep = "/"))

```

## Handle special models

*command-r7b-12-2024-t=1* grok-3-beta-r=TRUE

```{r temperature}

# update list of models 
models <- models %>%
  mutate(model = ifelse(!is.na(temperature) & temperature != 0, paste(model, temperature, sep = "-t="), model))

# update llm_data
llm_data[llm_data$temperature == 1,]$model <- paste(llm_data[llm_data$temperature == 1,]$model, "t=1", sep = "-")

# update llm_surveys
llm_surveys[llm_surveys$temperature == 1,]$model <- paste(llm_surveys[llm_surveys$temperature == 1,]$model, "t=1", sep = "-")

```

We collected a total of `r nrow(llm_data)` valid LLM responses across `r nrow(surveys)` surveys.

## Cost

```{r cost analysis, results = 'asis', warning=FALSE}

# calculate costs in tokens
cost_tokens <- llm_data %>%
  group_by(provider, model, temperature) %>%
  summarise(
    total_iterations_completed = n(),
    total_input_tokens = as.integer(sum(input_tokens)),
    total_output_tokens = as.integer(sum(output_tokens)),
    input_output_ratio = total_input_tokens/total_output_tokens,
    .groups = 'drop'
  )

models <- full_join(models, cost_tokens, by = c("provider", "model")) %>%
  mutate(
    cost_input = (total_input_tokens / 1000000) * price_1M_input,
    cost_output = (total_output_tokens / 1000000) * price_1M_output,
    total_cost = cost_input + cost_output
  )


api_costs <- read_excel(EXPENSES_FILE)

api_costs <- api_costs %>%
  group_by(api) %>%
  summarise(
    credits_paid = sum(as.numeric(credits), na.rm = TRUE),
    total_cost = sum(as.numeric(paid), na.rm = TRUE),
    .groups = "drop"
  )

api_costs <- models %>% 
  group_by(api) %>% 
  summarise(
    num_models = n(),
    credits_used = sum(total_cost, na.rm = TRUE),
    estimate = sum(total_estimate, na.rm = TRUE)) %>% 
  full_join(api_costs, by="api") %>%
  mutate(
    credits_left = credits_paid - credits_used,
  ) %>%
  select(
    api, 
    num_models,
    credits_paid, 
    credits_used, 
    credits_left, 
    total_cost, 
    estimate) %>%
  arrange(desc(credits_paid))

write_csv(api_costs, paste(OUTPUT_DIR, "api_costs.csv", sep = "/"))

cat("We spent a total of", sum(api_costs$credits_paid, na.rm = TRUE), "USD. The cost breakdown per API is below.")

api_costs %>%
  filter(!is.na(api)) %>%
  select(api, num_models, credits_paid) %>%
  arrange(desc(credits_paid)) %>%
  knitr::kable(caption = "Costs by API")

```

## Time

```{r execution analysis, results = 'asis'}

# read exec log
# NOTE: not all executions were logged due to technical issues
# so the number of completions in this log will not add up to those in llm_data
exec_log <- read_csv(EXEC_LOG_FILE, show_col_types = FALSE)

# fill missing columns
exec_log <- exec_log %>%
  arrange(model) %>%
  mutate(model = ifelse(!is.na(temperature) & temperature != 0, paste(model, temperature, sep = "-t="), model)) %>%

  mutate(
    fixed_num_errors = (`num surveys` * `num iterations`) - (`num fail completions` + `num success completions`)
  ) %>%
  relocate(fixed_num_errors, .before = `num errors`) %>%
  
  # remove trial columns 
  filter(is.na(`template success rate (%)`)) %>%
  select(-`template success rate (%)`) %>%
  
  # remove models with no completion data
  filter(`num completions` > 0)
  



exec_models <- exec_log %>%
  group_by(provider, model, temperature) %>%
  summarise(
    num_exec = n(),
    total_cost_USD = sum(`total cost ($)`, na.rm = TRUE),
    total_time_min = sum(`total elapsed time (min)`, na.rm = TRUE),
    num_completions = sum(`num completions`, na.rm = TRUE),
    num_success = sum(`num success completions`, na.rm = TRUE),
    num_error = sum(fixed_num_errors, na.rm = TRUE),
    num_fail = sum(`num fail completions`, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    fail_rate = num_fail / num_completions,
    success_rate = num_success / num_completions,
    cost_per_completion = total_cost_USD / num_completions,
    cost_per_success = total_cost_USD / num_success,
    time_per_completion_s = total_time_min / num_completions * 60
  )

models <- full_join(models, exec_models, by=c("provider", "model"))

exec_models_surveys <- exec_log %>%
  pivot_longer(19:38, names_to = "survey", values_to = "success_rate", values_drop_na = TRUE) %>%
  mutate(
    success_iterations = round(success_rate / 100 * `num iterations`),
    survey = str_extract(survey, "([^ ]+)\\s*") %>%
           trimws()

  ) %>%
  group_by(provider, model, survey) %>%
  summarise(
    num_execs = n(),
    num_success = sum(success_iterations, na.rm = TRUE),
    num_iterations = sum(`num iterations`, na.rm = TRUE),
    success_rate = num_success / num_iterations,
    .groups = "drop"
  )

exec_surveys <- exec_models_surveys %>%
  group_by(survey) %>%
  summarise(
    num_execs = sum(num_execs),
    num_iterations = sum(num_iterations),
    num_success = sum(num_success),
    success_rate = num_success/num_iterations,
    .groups = "drop"
  )

exec_total <- models %>%
  summarise(
    hours = sum(total_time_min, na.rm = TRUE) / 60,
    cost_USD = sum(total_cost, na.rm = TRUE),
    .groups = "drop"
  )

first_date <- min(llm_data$created_at)
last_date <- max(llm_data$created_at)

data_collection_days <- as.numeric(round(difftime(last_date, first_date, units = c("days"))))

cat("It took a total of", round(exec_total$hours), "hours^[Execution data is mostly accurate. Only a few (3-5) executions failed and, as a result, we have no record of it.] across", data_collection_days, "days to complete data collection. Most of it was done in parallel. The first LLM response was collected on", format(min(llm_data$created_at), "%A, %b %d, %Y"), "and latest on", format(max(llm_data$created_at), "%A, %b %d, %Y.\n\n"))

exec_total <- as.data.frame(t(exec_total))
colnames(exec_total) <- c("measure")
exec_total <- rownames_to_column(exec_total, "metric")

# write summary to file
write_csv(exec_models, paste(OUTPUT_DIR, "exec_models.csv", sep = "/"))
write_csv(exec_surveys, paste(OUTPUT_DIR, "exec_surveys.csv", sep = "/"))
write_csv(exec_models_surveys, paste(OUTPUT_DIR, "exec_models_surveys.csv", sep = "/"))
write_csv(exec_total, paste(OUTPUT_DIR, "exec_total.csv", sep = "/"))


llm_data %>%
  mutate(date = as.Date(created_at)) %>%
  group_by(date) %>%
  summarise(count = n()) %>%
  arrange(date) %>%
  mutate(cumulative_count = cumsum(count)) %>%
  ggplot(aes(x = date, y = cumulative_count)) +
  geom_line() +
  labs(x = "Date", y = "Cumulative LLM Data Collected") +
  theme_minimal() +
  scale_x_date(
    breaks = date_breaks("1 day"),
    labels = date_format("%b %d")
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```

```{r data left}

data_left <- models %>%
  filter(!done, included) %>%
  mutate(
    completions_total = min_iterations * length(survey_names),
    completions_left = completions_total - total_iterations_completed,
    time_left_min = (time_per_completion_s * completions_left) / 60,
    cost_left = cost_per_completion * completions_left
  ) %>% 
  select(
    provider, model,
    completions_total,
    completions_left,
    time_left_min,
    cost_left,
  )

write_csv(data_left, paste(OUTPUT_DIR, "data_left.csv", sep = "/"))

```

## Excluded Models

```{r model info, results = 'asis', warning=FALSE}

included_models_summary <- models %>%
  filter(included, done) %>%
  select(
    provider,
    model,
    series,
    version,
    is_reasoner,
    num_parameters,
    context_length,
    arch,
    api,
    done,
    cost_per_completion,
    total_cost,
    fail_rate,
    time_per_completion_s,
    total_time_min,
    comment
  )

# only use included models for analysis
llm_data <- llm_data %>% filter(model %in% included_models_summary$model)
llm_surveys <- llm_surveys %>% filter(model %in% included_models_summary$model)

write_output_csv(included_models_summary, "models_summary.csv")

excluded_models <- models %>% filter(!included) %>% select(provider, model, excluded_reason)

cat("\n", nrow(excluded_models), "out of", nrow(models), "were excluded from the analysis for the following reasons.\n\n")

knitr::kable(excluded_models %>% arrange(provider, model), caption = "Excluded models and reasons", col.names = c("Provider", "Model", "Reason for exclusion"))



```

## Execution Summary Plots

### Fail rate

```{r fail rate, fig.width=10,fig.height=6}


max_value <- max(included_models_summary$fail_rate)
buffer <- (max_value - min(included_models_summary$fail_rate)) * 0.1

included_models_summary %>%
  ggplot(aes(
    x = reorder(model, fail_rate),
    y = fail_rate,
    fill = provider,
  )) +
  geom_bar(stat = "identity") +
  coord_flip() +
  geom_text(aes(label = paste0(round(fail_rate * 100, 1), "%")),
            hjust = -0.1, 
            size = 3) +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "white"),
    legend.position = "bottom right",
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
  ) +
  labs(x = "Models", y = "Fail rate") +
  scale_y_continuous(
    expand = expansion(mult = c(0, 0)),
    breaks = seq(0, max_value + buffer, by = round((
      max_value + buffer
    ) / 10, 2)),
    limits = c(0, max_value + buffer),
  ) -> plot

ggsave(
  paste(OUTPUT_DIR, "plots/fail_rate.png", sep = "/"),
  plot,
  width = 10,
  height = 6
)

plot

```

### Cost per completion

```{r cost-completion, fig.width=10,fig.height=6}


max_value <- max(included_models_summary$cost_per_completion)
buffer <- (max_value - min(included_models_summary$cost_per_completion)) * 0.1

included_models_summary %>%
  ggplot(aes(
    x = reorder(model, cost_per_completion),
    y = cost_per_completion,
    fill = provider,
  )) +
  geom_bar(stat = "identity") +
  coord_flip() +
  geom_text(aes(label = round(cost_per_completion, 4)),
            hjust = -0.1, 
            size = 3) +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "white"),
    legend.position = "bottom right",
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
  ) +
  labs(x = "Models", y = "Cost per completion (USD)") +
  scale_y_continuous(
    expand = expansion(mult = c(0, 0.02)),
    breaks = seq(0, max_value + buffer, by = round((
      max_value + buffer
    ) / 10, 4)),
    limits = c(0, max_value + buffer),
  ) -> plot

ggsave(
  paste(OUTPUT_DIR, "plots/cost_per_completion.png", sep = "/"),
  plot,
  width = 10,
  height = 6
)

plot

```

### Total cost

```{r total cost, fig.width=10,fig.height=6}


max_value <- max(included_models_summary$total_cost)
buffer <- (max_value - min(included_models_summary$total_cost)) * 0.1

included_models_summary %>%
  ggplot(aes(
    x = reorder(model, total_cost),
    y = total_cost,
    fill = provider,
  )) +
  geom_bar(stat = "identity") +
  coord_flip() +
  geom_text(aes(label = round(total_cost, 2)),
            hjust = -0.1, 
            size = 3) +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "white"),
    legend.position = "bottom right",
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
  ) +
  labs(x = "Models", y = "Total cost (USD)") +
  scale_y_continuous(
    expand = expansion(mult = c(0, 0.02)),
    breaks = seq(0, max_value + buffer, by = round((
      max_value + buffer
    ) / 10)),
    limits = c(0, max_value + buffer),
  ) -> plot

ggsave(
  paste(OUTPUT_DIR, "plots/total_cost.png", sep = "/"),
  plot,
  width = 10,
  height = 6
)

plot

```

### Time per completion

```{r time-completion, fig.width=10,fig.height=6}

max_value <- max(included_models_summary$time_per_completion_s)
buffer <- (max_value - min(included_models_summary$time_per_completion_s)) * 0.1

included_models_summary %>%
  ggplot(aes(
    x = reorder(model, time_per_completion_s),
    y = time_per_completion_s,
    fill = provider,
  )) +
  geom_bar(stat = "identity") +
  coord_flip() +
  geom_text(aes(label = round(time_per_completion_s, 2)),
            hjust = -0.1, 
            size = 3) +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "white"),
    legend.position = "bottom right",
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
  ) +
  labs(x = "Models", y = "Time per completion (s)") +
  scale_y_continuous(
    expand = expansion(mult = c(0, 0.02)),
    breaks = seq(0, max_value + buffer, by = round((
      max_value + buffer
    ) / 10)),
    limits = c(0, max_value + buffer),
  ) -> plot

ggsave(
  paste(OUTPUT_DIR, "plots/time_per_completion_s.png", sep = "/"),
  plot,
  width = 10,
  height = 6
)

plot

```

### Cost/Time per completion

```{r cost-time-completion, fig.width=10,fig.height=6, results = 'asis'}

included_models_summary %>%
  group_by(provider, model, .groups = "drop") %>%
  ggplot(aes(x = time_per_completion_s, y = cost_per_completion, color = provider)) +
  geom_point(size = 5) + # Adjust size as needed
  geom_text(aes(label = model), vjust = -2, size = 2) + # Add labels above each dot
  labs(x = "Time per completion (s)", y = "Cost per completion (USD)", color = "Provider") +
  theme_minimal() +
  theme(plot.background = element_rect(fill = "white"),
        legend.position = "none") -> plot 

plot 

cat("\n\nZoomed in to cost < 0.01 USD and time < 12 s.\n\n")

included_models_summary %>%
  filter(cost_per_completion < 0.01 & time_per_completion_s < 12) %>%
  group_by(provider, model, .groups = "drop") %>%
  ggplot(aes(x = time_per_completion_s, y = cost_per_completion, color = provider)) +
  geom_point(size = 5) + # Adjust size as needed
  geom_text(aes(label = model), vjust = -2, size = 2) + # Add labels above each dot
  labs(x = "Time per completion (s)", y = "Cost per completion (USD)", color = "Provider") +
  theme_minimal() +
  theme(plot.background = element_rect(fill = "white"),
        legend.position = "none") -> plot 

plot 
```

## Internal Consistency of Responses

We calculate Cronbach's Alpha from the top `r MAX_ITERATIONS` iterations.

```{r error=FALSE, warning=FALSE, message=FALSE}

# Initialize an empty list to store the alpha results
alpha_results <- list()

models_with_data <- llm_data %>%
  distinct(provider, model) 

if (output.file.exists(ALPHA_RESULTS_FILE)) {
  last_alpha_results <- read_output_csv(ALPHA_RESULTS_FILE)
}

# Iterate over each unique provider/model combination
for (row in 1:nrow(models_with_data)) {
  provider <- models_with_data[row, ]$provider
  model <- models_with_data[row, ]$model
  
  # filter the data for the current provider/model
  provider_model_data <- llm_data %>%
    filter(model == !!model)
  
  # iterate over each survey
  for (survey_name in unique(provider_model_data$survey)) {
    
    # filter the data for the current survey
    survey_data <- provider_model_data %>%
      filter(survey == !!survey_name) %>%
      
      # get only first x iterations, where x = MAX_ITERATIONS
      arrange(created_at) %>%
      head(MAX_ITERATIONS)
    
    # SKIP THIS ITERATION IF PREVIOUS RECORD EXISTS
    # get date of the most data generation
    last_updated <- max(survey_data$created_at, na.rm = TRUE)
    
    # if there is a previous record
    if (exists("last_alpha_results")) {
      last_alpha <- last_alpha_results %>% filter(model == !!model, survey == !!survey_name)
      
      ## and last record is still valid, save it and skip
      ## valid records are more recent than the latest data generated
      if (nrow(last_alpha) == 1 && (last_alpha$created_at > last_updated)) {
        alpha_results[[length(alpha_results) + 1]] <- last_alpha
        next
      } else {
        cat("updating:", model, "/", survey_name, "\n")
      }
    }
    
    # Calculate Cronbach's Alpha for considerations (C1..C50)
    considerations_data <- survey_data %>% select(C1:C50)
    
    if (nrow(considerations_data) > 1) {
      
      # Check if policies are all equal (no variance)
      # this can happen when there are few iterations
      c_all_equal <- all(apply(considerations_data, 1, function(row)
        all(row == considerations_data[1, ], na.rm = TRUE)), na.rm = TRUE)
      
      # TODO: FIXME! 
      # NOTE: assign alpha = 1, which should NOT exist!
      # if (c_all_equal) {
      #   alpha_considerations <- 1
      # } else {
        alpha_considerations <- psych::alpha(
          considerations_data,
          check.keys = TRUE,
          warnings = FALSE,
        )$total$raw_alpha
      # }
    } else {
      alpha_considerations <- NA
    }
    
    # Calculate Cronbach's Alpha for policies (P1..P10)
    policies_data <- survey_data %>% select(P1:P10)
    
    if (nrow(policies_data) > 1) {
      
      # Check if policies are all equal (no variance)
      # this can happen when there are few iterations
      p_all_equal <- all(apply(policies_data, 1, function(row)
        all(row == policies_data[1, ], na.rm = TRUE)), na.rm = TRUE)
      
      # NOTE: assign alpha = 1, which should NOT exist!
      if (p_all_equal) {
        alpha_policies <- 1
      }

      # normal case, calculate alpha
      else {
        alpha_policies <- psych::alpha(
          policies_data,
          check.keys = TRUE,
          warnings = FALSE,
        )$total$raw_alpha
      }
    } else {
      alpha_policies <- NA
    }
    
    if (nrow(policies_data) > 1 && nrow(considerations_data) > 1) {
      all_data <- cbind(considerations_data, policies_data)
      alpha_all <- psych::alpha(
        all_data,
        check.keys = TRUE,
        warnings = FALSE,
      )$total$raw_alpha
    } else {
      alpha_all <- NA
    }
    
    
    # Store the results in the list
    alpha_results[[length(alpha_results) + 1]] <- tibble(
      provider = provider,
      model = model,
      survey = survey_name,
      N = nrow(considerations_data),
      created_at = now_utc(),
      alpha_considerations = alpha_considerations,
      alpha_policies = alpha_policies,
      alpha_all = alpha_all
    )
  }
}

# Combine all results into a single data frame
alpha_results <- bind_rows(alpha_results)

rm(models_with_data)
rm(considerations_data)
rm(survey_data)
rm(policies_data)
rm(provider_model_data)

# write summary to file
write_csv(alpha_results, paste(OUTPUT_DIR, "alpha_results.csv", sep = "/"))

```

### Check alpha results per model

```{r, warning=FALSE}

# Aggregate alpha_results by model and calculate summary statistics
alpha_summary <- alpha_results %>%
  filter(model %in% included_models_summary$model) %>%
  group_by(provider, model) %>%
  summarise(
    N = sum(N),
    mean_alpha_all = mean(alpha_all, na.rm = TRUE),
    min_alpha_considerations = min(alpha_considerations, na.rm = TRUE),
    max_alpha_considerations = max(alpha_considerations, na.rm = TRUE),
    mean_alpha_considerations = mean(alpha_considerations, na.rm = TRUE),
    std_alpha_considerations = sd(alpha_considerations, na.rm = TRUE),
    min_alpha_policies = min(alpha_policies, na.rm = TRUE),
    max_alpha_policies = max(alpha_policies, na.rm = TRUE),
    mean_alpha_policies = mean(alpha_policies, na.rm = TRUE),
    std_alpha_policies = sd(alpha_policies, na.rm = TRUE),
    .groups = "drop"
  )

alpha_summary %>%
  mutate(considerations = round(mean_alpha_considerations,2),
         policies = round(mean_alpha_policies,2),
         all = round(mean_alpha_all, 2)) %>%
  select(provider, model, N, all, considerations, policies) %>%
  arrange(all) %>%
  knitr::kable(caption = "Alpha summary across models, mean across surveys", row.names = TRUE)

```

# Human Data

```{r human data, warning=FALSE}

# Import the CSV file into a data frame
human_data <- read_csv(HUMAN_DATA_FILE, show_col_types = FALSE)

# Rename columns to be consistent with LLM data
human_data <- human_data %>%
  rename_with( ~ sub("^U0|^U", "C", .), U01:U50) %>%
  rename_with( ~ sub("^Pref", "P", .), Pref1:Pref10) %>%
  filter(Study != "Sydney CC Adaptation")

```

## Handle Swiss cases
```{r swiss cases}

# Import the CSV file into a data frame
swiss_data <- read_csv(SWISS_DATA_FILE, show_col_types = FALSE)

# Rename columns to be consistent with LLM data
swiss_data <- swiss_data %>%
  rename_with( ~ sub("^U0|^U", "C", .), everything()) %>%
  rename_with( ~ sub("^RF", "P", .), everything())

# Get valid column names
swiss_C_cols <- swiss_data %>%
  select(C1:C24) %>%
  colnames()

# get swiss LLM data
swiss_llm_data <- llm_data %>%
  filter(survey == "swiss_health")

# remove swiss case from llm data
llm_data <- llm_data %>%
  filter(survey != "swiss_health")

# extract valid C columns
swiss_llm_C_data <- swiss_llm_data %>%
  select(swiss_C_cols)

# rename valid C columns
swiss_llm_C_data <- swiss_llm_C_data %>%
  rename_with(~ paste0("C", seq_along(.)),
              starts_with("C", ignore.case = FALSE))

# ensure data has columns up to C50
for (j in (ncol(swiss_llm_C_data)+1):50) {
  swiss_llm_C_data[[paste0("C", j)]] <- as.numeric(NA)
}

# remove all C data
swiss_llm_data <- swiss_llm_data %>%
  select(-(C1:C50))

# add renamed and valid C data
swiss_llm_data <- bind_cols(swiss_llm_data, swiss_llm_C_data) %>%
  relocate(C1:C50, .before = P1)

# add swiss data back to llm data
llm_data <- bind_rows(llm_data, swiss_llm_data)

# rename valid C columns
swiss_data <- swiss_data %>%
  rename_with(~ paste0("C", seq_along(.)),
              C1:C24)

# rename valid C columns
# swiss_data <- swiss_data %>%
#   rename_with(~ paste0("C", seq_along(.)),
#               starts_with("C", ignore.case = FALSE))

# ensure data has columns up to C50
for (j in 17:50) {
  col <- paste0("C", j)
  swiss_data[[col]] <- as.numeric(NA)
  swiss_data <- swiss_data %>%
    relocate(col, .before = P1)
}

# ensure data has columns up to P10
for (j in 7:10) {
  col <- paste0("P", j)
  swiss_data[[col]] <- as.numeric(NA)
}

# add swiss data to human data
human_data <- bind_rows(human_data, swiss_data)

```


```{r add survey}

# Read the mapping file
study_survey_map <- read_csv("data/study_survey_map.csv", show_col_types = FALSE)

# Add a new column 'Survey' to human_data by matching 'Study' with 'survey'
human_data <- human_data %>%
  left_join(study_survey_map, by = c("Study" = "study")) %>%
  relocate(survey, .after = "Study")


human_data %>%
  group_by(Case, survey) %>%
  summarise(
    participants = n() / 2,  # account for pre/post deliberation
    .groups = "drop"
  ) %>%
  arrange(survey) %>%
  knitr::kable(
    caption = "Number of participants in each case study",
    row.names = TRUE,
    col.names = c("Case", "Survey", "Participants")
    )

```


We collected `r nrow(human_data)` human responses across `r length(unique(human_data$Case))` case studies, including pre-post deliberation responses.

## Excluded cases

```{r exclude cases}

# Import the CSV file into a data frame
cases <- read_csv(CASES_FILE, show_col_types = FALSE)
excluded_cases <- cases %>% filter(!included)

excluded_cases %>%
  select(-included)%>%
  knitr::kable(
    caption = "Excluded cases",
    row.names = TRUE,
    col.names = c("Case", "Survey", "Participants", "Excluded Reason")
  )

```

We excluded `r nrow(excluded_cases)` cases due to the reasons listed above.

```{r final cases}

human_data <- human_data %>%
  filter(!Case %in% excluded_cases$case)

```


# Aggregation

We then aggregated LLM data into 1 response per model/survey. Based on [@motoki_more_2024], we bootstrap considerations 1000 times.

```{r aggregation functions}

# function to calculate mode of data, same as stat_function
calc_mode <- function(data) {
  as.numeric(names(sort(table(data), decreasing = TRUE)[1]))
}

# function to bootstrap mode
bootstrap_mode <- function(data, n_bootstrap = 1000) {
  
  # return NA if data contains any NA
  if (any(is.na(data))) {
    return(NA)
  }
  
  # define the statistic function for bootstrapping to find mode
  stat_function <- function(data, indices) {
    as.numeric(names(sort(table(data[indices]), decreasing = TRUE)[1]))
  }
  
  # perform bootstrap
  results <- boot(data = data,
                  statistic = stat_function,
                  R = n_bootstrap)
  
  # calculate bootstrapped mode
  b_mode <- calc_mode(results$t)
  
  # return the bootstrapped modes
  return(b_mode)
}


aggregate_llm_considerations <- function(considerations) {
  
  # ensure there are at least 2 rows to aggregate
  if (nrow(considerations) < 2) {
    return(considerations)
  }
  
  # Calculate the mode for each column
  mode_considerations <- considerations %>%
    summarise(across(everything(), bootstrap_mode))
  
  return(mode_considerations)
  
}

aggregate_llm_policies <- function(policies) {
  
  # ensure there are at least 2 rows to aggregate
  if (nrow(policies) < 2) {
    return(policies)
  }
  
  # Remove columns with NAs
  valid_policies <- policies[, colSums(is.na(policies)) != nrow(policies)]
  
  # Convert the policies to a ranked matrix
  ranked_matrix <- as.matrix(valid_policies)
  
  # Define the number of winners to all - 1 policies
  # stv complains if winners == all policies
  num_winners <- ncol(valid_policies) - 1
  
  # Run the Single Transferable Vote algorithm
  results <- stv(ranked_matrix, num_winners, quiet = TRUE)
  
  # add last policy to ranked result
  last_policy <- setdiff(colnames(valid_policies), results$elected)
  ranked_policies <- c(results$elected, last_policy)
  
  policy_order <- colnames(valid_policies)
  
  order <- match(policy_order, ranked_policies)
  
  # calculate the number of missing values needed to reach length 10
  missing_columns <- ncol(policies) - length(order)
  
  # fill in the missing values with NA
  order <- c(order, rep(NA, missing_columns))
  
  # create a new data frame with aggregated results
  policy_ranks <- data.frame(t(order))
  colnames(policy_ranks) <- colnames(policies)
  
  return(policy_ranks)
}

```

## Aggregate considerations and preferences

```{r aggregate llm data}


aggregate_llm_data <- function(data) {
  
  # get last aggregation results, if it exists
  if (output.file.exists(AGGREGATION_RESULTS_FILE)) {
    last_aggr_results <- read_output_csv(AGGREGATION_RESULTS_FILE)
  }
  
  # initialize an empty list to store the alpha results
  aggr_results <- list()
  
  # iterate over each unique provider/model/survey combination
  for (row in 1:nrow(llm_surveys)) {
    provider <- llm_surveys[row, ]$provider
    model <- llm_surveys[row, ]$model
    survey <- llm_surveys[row, ]$survey
    N <- llm_surveys[row, ]$N
    
    # filter the data for the current survey
    survey_data <- data %>%
      filter(model == !!model, survey == !!survey) %>%
      
      # get only first x iterations, where x = MAX_ITERATIONS
      arrange(created_at) %>%
      head(MAX_ITERATIONS)
    
    # SKIP THIS ITERATION IF PREVIOUS RECORD EXISTS
    # get date of the most data generation
    last_updated <- max(survey_data$created_at, na.rm = TRUE)
    
    # if there is a previous record
    if (exists("last_aggr_results")) {
      last_aggr <- last_aggr_results %>% filter(model == !!model, survey == !!survey)
      
      ## and last record is still valid, save it and skip
      ## valid records are more recent than the latest data generated
      if (nrow(last_aggr) == 1 &&
          (last_aggr$created_at > last_updated)) {
        aggr_results[[length(aggr_results) + 1]] <- last_aggr
        next
      } else {
        cat("updating:", model, "/", survey, "\n")
      }
    }
    
    # aggregate considerations C1:C50
    considerations_data <- survey_data %>% select(C1:C50)
    aggregated_considerations <- aggregate_llm_considerations(considerations_data)
    
    # aggregate policies P1:P10
    policies_data <- survey_data %>% select(P1:P10)
    aggregated_policies <- aggregate_llm_policies(policies_data)
    
    # store the results in the list
    aggr_result <- tibble(
      provider = provider,
      model = model,
      survey = survey,
      N = N,
      created_at = now_utc(),
      aggregated_considerations,
      aggregated_policies,
    )
    
    aggr_results[[length(aggr_results) + 1]] <- aggr_result
    
  }
  
  # Combine all results into a single data frame
  aggr_results <- bind_rows(aggr_results)
  
  return(aggr_results)
  
}

time_start <- Sys.time()
llm_data_aggregated <- aggregate_llm_data(llm_data)
time_end <- Sys.time()
elapsed_time <- difftime(time_end, time_start, units = "auto")

# write summary to file
write_output_csv(llm_data_aggregated, AGGREGATION_RESULTS_FILE)

```

We aggregated `r nrow(llm_data)` LLM responses into `r nrow(llm_data_aggregated)` responses: 1 response per model per survey.

```{r aggr debugging, , results = 'asis'}

for (i in 1:nrow(llm_data_aggregated)) {
  provider <- llm_data_aggregated[i, ]$provider 
  model <- llm_data_aggregated[i, ]$model
  survey <- llm_data_aggregated[i, ]$survey
  considerations <- llm_data_aggregated[i, ] %>% select(C1:C50)
  considerations <- remove_NA_cols(considerations)
  values <- pivot_longer(considerations, cols = everything(), values_to = "values")
  uniques <- unique(values$values)
  if (length(uniques) == 1) {
    cat("WARNING! All considerations of", paste(provider, model, survey, sep="/"), "were aggregated as", uniques, "\n\n")
  }
}


```



# Randomly Generated Data

```{r generate random}
 
get_random_data <- function(sheet_names, file_path=SURVEY_FILE) {
  
  rand <- list()
  
  # Iterate over each sheet in the workbook
  for (sheet_name in sheet_names) {
    #cat("Processing sheet:", sheet_name, "\n")
    
    # Read the current sheet into a data frame
    df <- read_excel(file_path, sheet = sheet_name)
    
    # Check if required columns exist
    required_columns <- c("considerations", "policies", "scale_max", "q-method")
    missing_cols <- setdiff(required_columns, colnames(df))
    if (length(missing_cols) > 0) {
      cat(
        "Sheet",
        sheet_name,
        "is missing the following columns:",
        paste(missing_cols, collapse = ", "),
        "\n\n"
      )
      next
    }
    
    # Calculate the number of non-NA rows in "considerations" column
    n_c <- sum(!is.na(df$considerations))
    
    ## handle special swiss case manually
    if (sheet_name == "swiss_health") {
      n_c <- 16
    }
    
    # Calculate the number of non-NA rows in "policies" column
    n_p <- sum(!is.na(df$policies))
    
    # Extract integer values from "scale_max" column, assuming they are already integers
    scale_max <- as.integer(na.omit(df$scale_max))
    
    # Extract logical (boolean) values from "q-method" column
    q_method <- as.logical(na.omit(df$`q-method`))
    
    # Print the results for each sheet
    #cat("Sheet:", sheet_name, "\n")
    #cat("Number of considerations:", n_c, "\n")
    #cat("Number of policies:", n_p, "\n")
    #cat("Integer value from 'scale_max' column:", scale_max, "\n")
    #cat("Logical value from 'q-method' column:", q_method, "\n")
    
    
    ### Make random survey
    # Generate data for C1:C50 with Likert scale values [1, scale_max] randomly assigned
    if (q_method) {
      
      # get normally distributed data
      c_df <- data.frame(t(round(rnorm(n = n_c, mean = scale_max / 2, sd = scale_max / 4))))
      
      # replace values below lower and above upper bound
      c_df[c_df < 1] <- 1
      c_df[c_df > scale_max] <- scale_max
      
    } else {
      c_df <- data.frame(t(replicate(n_c, sample(1:scale_max, 1, replace = TRUE))))
    }
    
    # fill and rename columns
    c_df[1, (n_c + 1):50] <- NA
    colnames(c_df) <- paste0("C", 1:50)
    
    # Generate data for P1:P10 with random unique ranks 1 to n_p for each row
    p_df <- data.frame(t(apply(matrix(0, nrow = 1, ncol = n_p), 1, function(x)
      sample(1:n_p))))
    p_df[1, (n_p + 1):10] <- NA
    colnames(p_df) <- paste0("P", 1:10)
    
    # Validate data
    c_valid <- !(any(c_df < 1, na.rm = TRUE) || any(c_df > scale_max, na.rm = TRUE))
    p_valid <- !(any(duplicated(p_df[!is.na(p_df)])) ||
      any(p_df < 1, na.rm = TRUE) ||
      any(p_df > n_p, na.rm = TRUE))
  
    #cat("Valid considerations:", c_valid, "\n")
    #cat("Valid policies:", p_valid, "\n")
    
    if (!c_valid || !p_valid) {
      warning(paste("Random generation produced invalid data for", sheet_name))
    }
    
    # Combine the two datasets into one data frame
    dataset <- as.data.frame(cbind(c_df, p_df))
    dataset <- dataset %>%
      mutate(survey = sheet_name) %>%
      relocate(survey, .before = 1)
    
    rand[[length(rand) + 1]] <- dataset
    
    #cat(rep("-", 40), "\n")
    
    
  }
  
  rand <- bind_rows(rand)
  return(rand)
  
}

# Example usage
rand_data <- get_random_data(survey_names)

write_output_csv(rand_data, "random_data.csv")


```

Then, we generated `r nrow(rand_data)` random reseponses, one for each survey.

```{r q-method}

q_data <- data.frame(rand_data %>%
                       filter(survey == "fremantle") %>%
                       select(C1:C50) %>%
                       t)

colnames(q_data) <- "C"

# Plot the histogram for the distribution of numbers in this row
# q_data %>% ggplot(aes(x = C)) +
#   geom_histogram(bins = 7, fill = "blue", color = "black") +
#   labs(title = "Fremantle Random Distribution",
#        x = "Number",
#        y = "Frequency")

q_data <- data.frame(rand_data %>% filter(survey == "valsamoggia") %>% select(C1:C50) %>% t)

colnames(q_data) <- "C"

# Plot the histogram for the distribution of numbers in this row
# q_data %>% ggplot(aes(x = C)) +
#   geom_histogram(bins = 7, fill = "blue", color = "black") +
#   labs(title = "Valsamoggia Random Distribution",
#        x = "Number",
#        y = "Frequency")

```

# DRI Analysis

We begin by defining DRI calculation functions.

```{r DRI functions, echo=TRUE, warning=FALSE}

# original DRI formula
dri_calc <- function(data, v1, v2) {
  lambda <- 1 - (sqrt(2) / 2)
  dri <- 2 * (((1 - mean(abs((data[[v1]] - data[[v2]]) / sqrt(2)
  ))) - (lambda)) / (1 - (lambda))) - 1
  
  return(dri)
}

# updated DRI formula
# FIXME: only accounts for negligible positive correlations, but not negative ones
dri_calc_v2 <- function(data, v1, v2) {
  # Calculate orthogonal distance for each pair
  d <- abs((data[[v1]] - data[[v2]]) / sqrt(2))
  
  # Define lambda as in the original
  lambda <- 1 - (sqrt(2) / 2)
  
  # Calculate penalty: 0.5 if both correlations are in [0, 0.2], 1 otherwise
  penalty <- ifelse(data[[v1]] >= 0 & data[[v1]] <= 0.2 & #0.3
                      data[[v2]] >= 0 & data[[v2]] <= 0.2, # 0.3
                    0, 1)
  
  # Adjusted consistency per pair
  consistency <- (1 - d) * penalty
  
  # Average consistency across all pairs
  avg_consistency <- mean(consistency)
  
  # Scale to [-1, 1] as in the original
  dri <- 2 * ((avg_consistency - lambda) / (1 - lambda)) - 1
  
  return(dri)
}

# updated DRI formula: penalizes both negligible
# positive and negative correlations in a scalar way.
dri_calc_v3 <- function(data, v1, v2) {
  d <- abs((data[[v1]] - data[[v2]]) / sqrt(2))
  lambda <- 1 - (sqrt(2) / 2)
  
  # Scalar penalty based on strength of signal (|r| and |q|)
  penalty <- ifelse(pmax(abs(data[[v1]]), abs(data[[v2]])) <= 0.2, pmax(abs(data[[v1]]), abs(data[[v2]])) / 0.2, 1)
  
  consistency <- (1 - d) * penalty
  avg_consistency <- mean(consistency)
  
  dri <- 2 * ((avg_consistency - lambda) / (1 - lambda)) - 1
  return(dri)
}


```




```{r DRI}

get_IC <- function(data, survey, case) {
  
  # loop through analysis stages (pre/post)
  for (stage in 1:max(data$StageID)) {
    
    # select specific data to analyse
    data_stage <- data %>% filter(StageID == stage)
    
    # make sure there's data to analyze
    if (nrow(data_stage) > 0) {
      # get participant numbers/ids
      PNums <- data_stage$PNum
      
      # variables for reading COLUMN data
      # Q is a list considerations (Likert scale)
      # - there are up to 50 questions
      # R is a list ratings (rankings)
      Q <- data_stage %>% select(C1:C50)
      R <- data_stage %>% select(P1:P10)
      
      # remove all NA columns (in case there are less than 50
      # consideration questions
      Q <- Q[, colSums(is.na(Q)) != nrow(Q)]
      R <- R[, colSums(is.na(R)) != nrow(R)]
      
      # transpose data
      Q <- t(Q)
      R <- t(R)
      
      # format data as data frame
      Q <- as.data.frame(Q)
      R <- as.data.frame(R)
      
      # name columns with participant numbers
      colnames(Q) <- PNums
      colnames(R) <- PNums
      
      # obtain a list of correlations without duplicates
      # cor() returns a correlation matrix between Var1 and Var2
      # Var1 and Var2 are the variables being correlated
      # Freq is the correlation
      QWrite <- subset(as.data.frame(as.table(cor(Q, method = "spearman"))),
                       match(Var1, names(Q)) > match(Var2, names(Q)))
      
      RWrite <- subset(as.data.frame(as.table(cor(R, method = "spearman"))),
                       match(Var1, names(R)) > match(Var2, names(R)))
      
      # initialize the output in the first iteration
      if (stage == 1) {
        IC <- data.frame("P_P" = paste0(QWrite$Var1, '-', QWrite$Var2))
        IC$P1 <- as.numeric(as.character(QWrite$Var1))
        IC$P2 <- as.numeric(as.character(QWrite$Var2))
      }
      
      # prepare QWrite
      QWrite <- as.data.frame(QWrite$Freq)
      names(QWrite) <- paste0("Q", stage)
      
      # prepare RWrite for merge
      RWrite <- as.data.frame(RWrite$Freq)
      names(RWrite) <- paste0('R', stage)
      
      # merge
      IC <- cbind(IC, QWrite, RWrite)
    }
    
  }
  
  # append case & study info
  IC$survey <- survey
  IC$case <- case
  
  ## IC Points calculations ##
  IC$IC_PRE <- 1 - abs((IC$R1 - IC$Q1) / sqrt(2))
  IC$IC_POST <- 1 - abs((IC$R2 - IC$Q2) / sqrt(2))
  
  return(IC)
}

get_ind_DRI <- function(IC) {
  
  Plist <- unique(c(IC$P1, IC$P2))
  
  Plist <- Plist[order(Plist)]
  
  DRIInd <- data.frame('participant' = Plist)
  DRIInd$survey <- survey
  DRIInd$case <- data_case_study$Case[1]
  DRIInd$created_at = now_utc()
  
  DRIInd <- DRIInd[c("survey", "case", "participant")]
  
  #Add individual-level metrics
  for (i in 1:length(Plist)) {
    DRIInd$DRIPre[i] <- dri_calc(
      data = IC  %>% filter(P1 == Plist[i] | P2 == Plist[i]),
      v1 = 'R1',
      v2 = 'Q1'
    )
    DRIInd$DRIPost[i] <- dri_calc(
      data = IC  %>% filter(P1 == Plist[i] | P2 == Plist[i]),
      v1 = 'R2',
      v2 = 'Q2'
    )
    
    # calculate updated DRI V2
    DRIInd$DRIPreV2[i] <- dri_calc_v2(
      data = IC  %>% filter(P1 == Plist[i] | P2 == Plist[i]),
      v1 = 'R1',
      v2 = 'Q1'
    )
    DRIInd$DRIPostV2[i] <- dri_calc_v2(
      data = IC  %>% filter(P1 == Plist[i] | P2 == Plist[i]),
      v1 = 'R2',
      v2 = 'Q2'
    )
    
    # calculate updated DRI V3
    DRIInd$DRIPreV3[i] <- dri_calc_v3(
      data = IC  %>% filter(P1 == Plist[i] | P2 == Plist[i]),
      v1 = 'R1',
      v2 = 'Q1'
    )
    DRIInd$DRIPostV3[i] <- dri_calc_v3(
      data = IC  %>% filter(P1 == Plist[i] | P2 == Plist[i]),
      v1 = 'R2',
      v2 = 'Q2'
    )
    
  }
  
  return(DRIInd)
  
}

get_case_DRI <- function(IC, type="human_only") {
   
  ## Group DRI level ##
  DRI_PRE <- dri_calc(data = IC, v1 = 'R1', v2 = 'Q1')
  DRI_POST <- dri_calc(data = IC, v1 = 'R2', v2 = 'Q2')
  
  ## Group DRI level V2 ##
  DRI_PRE_V2 <- dri_calc_v2(data = IC, v1 = 'R1', v2 = 'Q1')
  DRI_POST_V2 <- dri_calc_v2(data = IC, v1 = 'R2', v2 = 'Q2')
  
  ## Group DRI level V3 ##
  DRI_PRE_V3 <- dri_calc_v3(data = IC, v1 = 'R1', v2 = 'Q1')
  DRI_POST_V3 <- dri_calc_v3(data = IC, v1 = 'R2', v2 = 'Q2')
  
  # cat(survey, case, type, "\n")
  # cat(DRI_PRE, DRI_POST, "\n")
  # cat(DRI_PRE_V2, DRI_POST_V2, "\n")
  # cat(DRI_PRE_V3, DRI_POST_V3, "\n")
  
  #CaseDRI Dataframe
  DRI.Case <- tibble(
    survey = survey,
    case = case,
    type = type,
    created_at = now_utc(),
    DRI_PRE,
    DRI_PRE_V2,
    DRI_PRE_V3,
    DRI_POST,
    DRI_POST_V2,
    DRI_POST_V3,
  )
  
  #Tests for groups
  DRIOverallSig <- wilcox.test(IC$IC_POST,
                               IC$IC_PRE,
                               paired = TRUE,
                               alternative = "greater")
  DRIOverallSig_twoside <- wilcox.test(IC$IC_POST,
                                       IC$IC_PRE,
                                       paired = TRUE,
                                       alternative = "two.side")

  #Adding the results to case data
  DRI.Case$DRI_one_tailed_p <- DRIOverallSig$p.value
  DRI.Case$DRI_twoside_p <- DRIOverallSig_twoside$p.value
  
  return(DRI.Case)
  
}

mini_publics <- human_data %>%
  group_by(survey, Case) %>%
  summarise(.groups = "drop")

# get_llm_data <- function(provider, model, survey) {
#   llm_participant <- llm_data_aggregated %>%
#     filter(provider == !!provider, model == !!model, survey == !!survey)
#   return(llm_participant)
# }

get_ind_LLM_DRI <- function(data, provider, model) {
  
  llm_DRI <- data %>%
    filter(participant == 0) %>%
    select(-participant) %>%
    mutate(provider = !!provider, model = !!model) %>%
    relocate(provider, model, .before = 1)
  
  return(llm_DRI)
}

get_ind_rand_DRI <- function(data) {
  
  rand_DRI <- data %>%
    filter(participant == -1) %>%
    select(-participant)

  return(rand_DRI)
}

add_random_participant <- function(data, survey) {
  
  # get random data
  rand_participant <- rand_data %>%
    filter(survey == !!survey)
  
  # create 2 participants, PRE and POST
  rand_participants <- bind_rows(rand_participant, rand_participant)
  rand_participants$PNum <- -1 # PNum = -1 is random
  rand_participants$StageID <- c(1,2)
  
  data_with_rand <- bind_rows(data, rand_participants)
  
  return(data_with_rand)
  
}

add_llm_participant <- function(data, provider, model, survey) {
  
  # get llm data
  llm_participant <- llm_data_aggregated %>%
    filter(provider == !!provider, model == !!model, survey == !!survey)
  
  # check if it exists
  if (nrow(llm_participant) == 0) {
    warning(paste("No participant found for", paste(provider, model, survey, sep = "/")))
  }
  
  # create 2 participants, PRE and POST
  llm_participants <- bind_rows(llm_participant, llm_participant)
  llm_participants$PNum <- 0 # PNum = 0 is LLM
  llm_participants$StageID <- c(1,2)
  
  data_with_llm <- bind_rows(data, llm_participants)
  
  return(data_with_llm)
  
}

add_single_llm_participant <- function(llm_survey_data, it, data, provider, model, survey) {
  
  # get llm data
  llm_participant <- llm_survey_data[it, ]
  
  # check if it exists
  if (nrow(llm_participant) == 0) {
    warning(paste("No participant found for", paste(provider, model, survey, sep = "/")))
  }
  
  # create 2 participants, PRE and POST
  llm_participants <- bind_rows(llm_participant, llm_participant)
  llm_participants$PNum <- 0 # PNum = 0 is LLM
  llm_participants$StageID <- c(1,2)
  
  data_with_llm <- bind_rows(data, llm_participants)
  
  return(data_with_llm)
  
}


DRIInd.LLMs <- list()
DRIInd.LLMs.it <- list()
DRIInd.Rands <- list()

# FIXME: for debugging
mini_publics <- mini_publics %>% filter(survey == "swiss_health")

# for each study [1:N], N = 26
for (case_study in 1:nrow(mini_publics)) {

  # select study data
  survey <- mini_publics[case_study, ]$survey
  case <- mini_publics[case_study, ]$Case
  
  # get human data for this case study
  data_case_study <- human_data %>% filter(survey == !!survey &
                                             Case == !!case)
  
  ### HUMAN-ONLY DRI ###
  # intersubject correlations (IC)
  IC <- get_IC(data_case_study, survey, case)
  DRI.Case <- get_case_DRI(IC) # group DRI
  DRIInd <- get_ind_DRI(IC) # individual DRI
  
  # get human average
  # NOTE: this should be the same as human_only group DRI
  human_ind_DRI_mean <- tibble(
    DRIPre = mean(DRIInd$DRIPre),
    DRIPost = mean(DRIInd$DRIPost)
  )
  
  human_ind_DRI_meanV2 <- tibble(
    DRIPreV2 = mean(DRIInd$DRIPreV2),
    DRIPostV2 = mean(DRIInd$DRIPostV2)
  )
  
  human_ind_DRI_meanV3 <- tibble(
    DRIPreV3 = mean(DRIInd$DRIPreV3),
    DRIPostV3 = mean(DRIInd$DRIPostV3)
  )
  
  # Global dataframes for depositing results
  # initialize *.Global
  if (case_study == 1) {
    IC.Global <- IC
    DRIInd.Global <- DRIInd
    DRI.Global <- DRI.Case
  }
  
  # append to *.Global
  else {
    IC.Global <- rbind(IC.Global, IC)
    DRIInd.Global <- rbind(DRIInd.Global, DRIInd)
    DRI.Global <- rbind(DRI.Global, DRI.Case)
  }
  
  ##W RANDOM DRI ###
  type <- "human+random"
    
  data_with_random <- add_random_participant(data_case_study, survey)
  
  IC.Rand <- get_IC(data_with_random, survey, case)
  DRI.Case.Rand <- get_case_DRI(IC.Rand, type)
  DRIInd.Rand <- get_ind_DRI(IC.Rand)
  DRIInd.Rand.Survey <- get_ind_rand_DRI(DRIInd.Rand)
  
  DRIInd.Rands[[length(DRIInd.Rands) + 1]] <- DRIInd.Rand.Survey
  
  DRI.Global <- rbind(DRI.Global, DRI.Case.Rand)

  
  ### LLM DRI ###
  # check if there are LLM data for this survey
  llms <- llm_surveys %>% filter(survey == !!survey)
  if (nrow(llms) == 0) {
    next
  }

  # iterate through each llm
  for (llm in 1:nrow(llms)) {

    provider <- llms[llm,]$provider
    model <- llms[llm,]$model
    
    ## get relevant llm data
    llm_survey_data <- llm_data %>%
      filter(model == !!model, survey == !!survey) %>%
      arrange(created_at) %>%
      head(MAX_ITERATIONS)
    
    ## do DRI for each iteration
    for (it in 1:nrow(llm_survey_data)) {
      type <- paste0("human+", paste(provider, model, sep = "/"), "+", it)
      
      data_with_llm <- add_single_llm_participant(llm_survey_data, it, data_case_study, provider, model, survey)
      
      IC.LLM <- get_IC(data_with_llm, survey, case)
      DRI.Case.LLM <- get_case_DRI(IC.LLM, type)
      DRIInd.LLM <- get_ind_DRI(IC.LLM)
      DRIInd.LLM.Model <- get_ind_LLM_DRI(DRIInd.LLM, provider, model)
      
      ## DRI V1
      DRIInd.LLM.Model$human_only_DRIPre_mean <- human_ind_DRI_mean$DRIPre
      DRIInd.LLM.Model$human_only_DRIPost_mean <- human_ind_DRI_mean$DRIPost
      
      ## DRI V2
      DRIInd.LLM.Model$human_only_DRIPre_meanV2 <- human_ind_DRI_meanV2$DRIPreV2
      DRIInd.LLM.Model$human_only_DRIPost_meanV2 <- human_ind_DRI_meanV2$DRIPostV2
      
      ## DRI V3
      DRIInd.LLM.Model$human_only_DRIPre_meanV3 <- human_ind_DRI_meanV3$DRIPreV3
      DRIInd.LLM.Model$human_only_DRIPost_meanV3 <- human_ind_DRI_meanV3$DRIPostV3
      
      get_bm_index <- function(diff) {
        bm_index <- (diff + 2) / 4
        return(bm_index)
      }
      
      DRIInd.LLM.Model <- DRIInd.LLM.Model %>%
        mutate(
          DRIPre_diff = DRIPre - human_only_DRIPre_mean,
          DRIPost_diff = DRIPost - human_only_DRIPost_mean
        ) %>%
        mutate(bm_index = get_bm_index(DRIPost_diff)) %>%
        
        ## benchmark V2
        mutate(
          DRIPre_diffV2 = DRIPreV2 - human_only_DRIPre_meanV2,
          DRIPost_diffV2 = DRIPostV2 - human_only_DRIPost_meanV2
        ) %>%
        mutate(bm_indexV2 = get_bm_index(DRIPost_diffV2)) %>%
        
        ## benchmark V3
        mutate(
          DRIPre_diffV3 = DRIPreV3 - human_only_DRIPre_meanV3,
          DRIPost_diffV3 = DRIPostV3 - human_only_DRIPost_meanV3
        ) %>%
        mutate(bm_indexV3 = get_bm_index(DRIPost_diffV3))
      
      DRIInd.LLM.Model$iteration <- it
      DRIInd.LLMs.it[[length(DRIInd.LLMs.it) + 1]] <- DRIInd.LLM.Model
      
      DRI.Global <- rbind(DRI.Global, DRI.Case.LLM)
      
    }
    
    ## do DRI for aggregated LLM data
    type <- paste0("human+",paste(provider, model, sep = "/"))
    
    data_with_llm <- add_llm_participant(data_case_study, provider, model, survey)
    
    IC.LLM <- get_IC(data_with_llm, survey, case)
    DRI.Case.LLM <- get_case_DRI(IC.LLM, type)
    DRIInd.LLM <- get_ind_DRI(IC.LLM)
    DRIInd.LLM.Model <- get_ind_LLM_DRI(DRIInd.LLM, provider, model)
    
    ## DRI V1
    DRIInd.LLM.Model$human_only_DRIPre_mean <- human_ind_DRI_mean$DRIPre
    DRIInd.LLM.Model$human_only_DRIPost_mean <- human_ind_DRI_mean$DRIPost
    
    ## DRI V2
    DRIInd.LLM.Model$human_only_DRIPre_meanV2 <- human_ind_DRI_meanV2$DRIPreV2
    DRIInd.LLM.Model$human_only_DRIPost_meanV2 <- human_ind_DRI_meanV2$DRIPostV2
    
    ## DRI V3
    DRIInd.LLM.Model$human_only_DRIPre_meanV3 <- human_ind_DRI_meanV3$DRIPreV3
    DRIInd.LLM.Model$human_only_DRIPost_meanV3 <- human_ind_DRI_meanV3$DRIPostV3
    
    get_bm_index <- function(diff) {
      bm_index <- (diff + 2) / 4
      return(bm_index)
    }
    
    DRIInd.LLM.Model <- DRIInd.LLM.Model %>%
      mutate(DRIPre_diff = DRIPre - human_only_DRIPre_mean,
             DRIPost_diff = DRIPost - human_only_DRIPost_mean) %>%
      mutate(bm_index = get_bm_index(DRIPost_diff)) %>%
    
      ## benchmark V2
      mutate(DRIPre_diffV2 = DRIPreV2 - human_only_DRIPre_meanV2,
             DRIPost_diffV2 = DRIPostV2 - human_only_DRIPost_meanV2) %>%
      mutate(bm_indexV2 = get_bm_index(DRIPost_diffV2)) %>%
    
      ## benchmark V3
      mutate(DRIPre_diffV3 = DRIPreV3 - human_only_DRIPre_meanV3,
             DRIPost_diffV3 = DRIPostV3 - human_only_DRIPost_meanV3) %>%
      mutate(bm_indexV3 = get_bm_index(DRIPost_diffV3))
      
    
    DRIInd.LLMs[[length(DRIInd.LLMs) + 1]] <- DRIInd.LLM.Model
    
    DRI.Global <- rbind(DRI.Global, DRI.Case.LLM)

  }
  
} # end for each case study

DRIInd.LLMs.it <- bind_rows(DRIInd.LLMs.it)
DRIInd.LLMs <- bind_rows(DRIInd.LLMs)
DRIInd.Rands <- bind_rows(DRIInd.Rands)

# aggregate iteration results
DRIInd.LLMs.it.summ <- DRIInd.LLMs.it %>%
  group_by(provider, model, survey, case) %>%
  summarise(
    DRIPostV3_mean = mean(DRIPostV3, na.rm = TRUE),
    DRIPostV3_sd = sd(DRIPostV3, na.rm = TRUE),
    DRIPostV3_mean = mean(DRIPostV3, na.rm = TRUE),
    bm_indexV3_mean = mean(bm_indexV3, na.rm = TRUE), 
    n_it = n(),
  )

## add iteration results to DRIInd.LLMs
DRIInd.LLMs <- DRIInd.LLMs.it.summ %>%
  full_join(DRIInd.LLMs, by=c("provider", "model", "survey", "case")) %>%
  relocate(DRIPostV3_mean, .after = DRIPostV3) %>%
  relocate(DRIPostV3_sd, .after = DRIPostV3_mean) %>%
  relocate(n_it, .after = DRIPostV3_sd)

## add alpha results to DRIInd.LLMs
LLM_alpha_DRI <- alpha_results %>%
  select(provider,
         model,
         survey,
         alpha_all,
         alpha_considerations,
         alpha_policies) %>%
  full_join(DRIInd.LLMs, by = c("provider", "model", "survey")) %>%
  filter(model %in% included_models_summary$model)

missing <- setdiff(unique(llm_data$survey), unique(DRIInd.LLMs$survey))

if (length(missing) > 0) {
  warning(paste("Missing", missing, "from DRIInd.LLMs!"))
}

# add delta column
DRI.Global <- DRI.Global %>%
  mutate(DRI_DELTA = DRI_POST - DRI_PRE)

# write summary to file
write_output_csv(LLM_alpha_DRI, "LLM_alpha_DRI.csv")

write_output_csv(DRIInd.LLMs.it, "DRIInd_LLMs_iterations.csv")

write_csv(DRIInd.LLMs, paste(OUTPUT_DIR, "DRIInd_LLMs.csv", sep = "/"))
write_csv(DRI.Global, paste(OUTPUT_DIR, "DRI_global.csv", sep = "/"))

```

# Select Dependent Variable for Analysis

We are using the average DRI calculated across the iterations of LLM (DRIIndV3_mean).

```{r}

DRIInd.LLMs$dv <- DRIInd.LLMs$DRIPostV3_mean

```


# Hypotheses Testing

## H1. DRI scores of LLMs do not significantly differ from those produced by a random generation process.

### Testing assumptions

We employed a one-way ANOVA (or a Kruskal-Wallis test, depending on the results of the exploratory analysis) between subjects to analyze our results. If normality and homogeneity of variance assumptions are met, we will use ANOVA followed by Tukey’s HSD post-hoc test for pairwise comparisons between LLM/version DRI and random DRI. If assumptions are violated, we will use the non-parametric Kruskal-Wallis test, followed by Dunn’s post-hoc test with Bonferroni correction.

The independent variable is be the type of participant (e.g., random, model). The dependent variable is the individual-level DRI score.

```{r H1-assumptions, fig.width=10,fig.height=6}

## get data for analysis
DRIInd.ALL <- DRIInd.LLMs %>%
  mutate(source = model) %>%
  mutate(source_type = "llm") %>%
  select(source, source_type, survey, case, DRIPreV3, dv) %>%
  rename(DRIPostV3 = dv)

DRIInd.ALL <- DRIInd.Rands %>%
  mutate(source = "random") %>%
  mutate(source_type = source) %>%
  select(source, source_type, survey, case, DRIPreV3, DRIPostV3) %>%
  bind_rows(DRIInd.ALL)

DRIInd.ALL <- DRI.Global %>%
  filter(type == "human_only") %>%
  mutate(
    source = "human_only_mean",
    source_type = "human",
    DRIPreV3 = DRI_PRE_V3,
    DRIPostV3 = DRI_POST_V3
  ) %>%
  select(source, source_type, survey, case, DRIPreV3, DRIPostV3) %>%
  bind_rows(DRIInd.ALL)

# clean data
DRIInd.ALL <- DRIInd.ALL %>%
  mutate(
    source = as.factor(source),
    source_type = as.factor(source_type),
    survey = as.factor(survey),
    case = as.factor(case)
  )


write_output_csv(DRIInd.ALL, "DRIInd_all.csv")

df <- DRIInd.ALL %>% filter(source != "human_only_mean")

# 1. Normality assumption: Shapiro-Wilk test for each group
shapiro_results <- df %>%
  group_by(source) %>%
  summarise(shapiro_p = shapiro.test(DRIPostV3)$p.value)

# 2. Homogeneity of variances: Levene's Test
levene_test_result <- leveneTest(DRIPostV3 ~ source, data = df)

# plot distributions
df %>%
  full_join(shapiro_results, by = "source") %>%
  mutate(signif = shapiro_p > 0.05) %>%

  ggplot(aes(x = DRIPostV3, fill = source_type)) +
  geom_histogram(bins = 30,
                 alpha = 0.6,
                 position = "identity") +
  facet_wrap(~ source) +
  labs(title = "Distribution of DRIPostV3 for Each Source Type", 
       x = "DRIPostV3", 
       y = "Frequency") +
  theme_minimal() + theme(legend.position = "none") +
  geom_point(
    data = . %>% filter(signif),
    aes(x = -1, y = 5), color = "red", shape = 8, size = 5)

# Assuming your dataframe is named `df`
df %>% arrange(source_type, source) %>%
ggplot(aes(x = source, y = DRIPostV3, fill = source_type)) +
  geom_boxplot() +
  labs(title = "Distribution of DRIPostV3 for Each Source Type",
       x = "Source",
       y = "DRIPostV3") +
  theme_minimal() + 
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = 90, hjust = 1))


```

### Testing hypothesis

```{r H1-test}

kruskal_test_result <- kruskal.test(DRIPostV3 ~ source, data = df)
kruskal_test_result

```

### Post-hoc tests

```{r H1-posthoc}

if (kruskal_test_result$p.value < 0.05) {
  # Perform Dunn's test if Kruskal-Wallis test is significant
  dunn_test_result <- dunn.test(df$DRIPostV3, df$source, method = "bonferroni", kw = FALSE, table = FALSE)
  
  dunn_test_result <- as.data.frame(dunn_test_result)
  
  random_comp <- dunn_test_result %>%
    filter(grepl("random", comparisons)) %>%
    mutate(P.rounded = round(P.adjusted, 3)) %>%
    mutate(P.formated = ifelse(P.rounded < 0.05, paste0(P.rounded, "*"), P.rounded))
  
  # Print the results of Dunn's test with Bonferroni correction
  random_comp %>%
    arrange(P.adjusted) %>% 
    mutate(model = str_remove(comparisons, " - random")) %>%
    select(model, P.formated) %>%
    knitr::kable(
      caption = "Models compared to random",
      col.names = c("Model", "P-adjusted"),
      digits = 3
    )
  
} else {
  message("Kruskal-Wallis test is not significant; no need for post-hoc testing.")
}

```

Some models, `r nrow(random_comp %>% filter(P.adjusted < 0.05))` out of `r nrow(random_comp)`, are significantly different than random.

## H2. LLMs' DRI scores will be significantly lower than those obtained from human participants after deliberation.

### Testing assumptions

```{r H2-assumptions, fig.width=10,fig.height=6}

df <- DRIInd.ALL %>% filter(source != "random")

# 1. Normality assumption: Shapiro-Wilk test for each group
shapiro_results <- df %>%
  group_by(source) %>%
  summarise(shapiro_p = shapiro.test(DRIPostV3)$p.value)

# 2. Homogeneity of variances: Levene's Test
levene_test_result <- leveneTest(DRIPostV3 ~ source, data = df)

# plot distributions
df %>%
  full_join(shapiro_results, by = "source") %>%
  mutate(signif = shapiro_p > 0.05) %>%
  ggplot(aes(x = DRIPostV3, fill = source_type)) +
  geom_histogram(bins = 30,
                 alpha = 0.6,
                 position = "identity") +
  facet_wrap(~ source) +
  labs(title = "Distribution of DRIPostV3 for Each Source Type", 
       x = "DRIPostV3", 
       y = "Frequency") +
  theme_minimal() + theme(legend.position = "none") +
  geom_point(
    data = . %>% filter(signif),
    aes(x = -1, y = 5), color = "red", shape = 8, size = 5)

# Assuming your dataframe is named `df`
df %>% arrange(source_type, source) %>%
ggplot(aes(x = source, y = DRIPostV3, fill = source_type)) +
  geom_boxplot() +
  labs(title = "Distribution of DRIPostV3 for Each Source Type",
       x = "Source",
       y = "DRIPostV3") +
  theme_minimal() + 
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = 90, hjust = 1))


```

### Testing hypothesis

To test H2, we will compare the average individual-level, post-deliberation DRI scores obtained by human participants with the individual-level DRI scores obtained by LLMs both across case studies and across LLM/version.

First, for each case study, we will employ a t-test (or non-parametric equivalent, depending on the results of the exploratory analysis) to analyze our results across case studies. The independent variable is participant type (human-only vs. LLM) and the dependent variable is the individual-level DRI scores.

For each case study...

human average

Second, for each LLM/version, we will employ a t-test (or non-parametric equivalent, depending on the results of the exploratory analysis) to analyze our results across LLM/version. The independent variable is participant type (human-only vs. LLM/version) and the dependent variable is the individual-level DRI scores.

```{r H2-test}

kruskal_test_result <- kruskal.test(DRIPostV3 ~ source, data = df)
kruskal_test_result

```

### Post-hoc tests

```{r H2-posthoc}

if (kruskal_test_result$p.value < 0.05) {
  # Perform Dunn's test if Kruskal-Wallis test is significant
  dunn_test_result <- dunn.test(df$DRIPostV3, df$source, method = "bonferroni", kw = FALSE, table = FALSE)
  
  dunn_test_result <- as.data.frame(dunn_test_result)
  
  human_comp <- dunn_test_result %>%
    filter(grepl("human", comparisons)) %>%
    mutate(P.rounded = round(P.adjusted, 3)) %>%
    mutate(P.formated = ifelse(P.rounded < 0.05, paste0(P.rounded, "*"), P.rounded))
  
  # Print the results of Dunn's test with Bonferroni correction
  human_comp %>%
    arrange(P.adjusted) %>% 
    mutate(model = str_remove(comparisons, " - human_only_mean")) %>%
    select(model, P.formated) %>%
    knitr::kable(
      caption = "Models compared to human",
      col.names = c("Model", "P-adjusted"),
      digits = 3
    )
  
} else {
  message("Kruskal-Wallis test is not significant; no need for post-hoc testing.")
}


```

## H3. LLMs’ DRI scores are improving over time, across each version.

Random slope --

Assume each case Multilevel analysis -- each case behave differently

LMER --

To test H3, we will conduct a repeated measures ANOVA (or Friedman test if the assumptions of normality or sphericity are violated) to test for differences in the mean DRI across all versions (e.g., v1, v2, v3) of an LLM across each case study. We will treat different LLM versions as related groups and the individual-level LLM DRI in each case study as a subject. In this within-subjects design, we can assess whether more recent versions of LLMs have a significant impact on the DRI scores they produce.

We want to assess the effects of Case and Series on weight loss in 10 sedentary individuals.

Dependent variable: - DRIPostV3

Independent variables: - [LLM series (moderator) -- which llm?] - [case (moderator) -- which case?] -- [LATER] - version (focal)

```{r H3-assumptions, fig.width=10,fig.height=6}

library(rstatix)
library(ggpubr)



v_series <- unique(included_models_summary$series)

df <- included_models_summary %>%
  full_join(DRIInd.LLMs, by=join_by(provider, model)) %>%
  select(provider, model, series, version, case, DRIPostV3) %>%
  arrange(version)

df <- df %>%
  filter(series == "open-qwen") %>%
  mutate(id = as.factor(paste(series, case, sep = "/"))) %>%
  select(id, version, DRIPostV3)

v_vals <- sort(unique(df$version))

df <- df %>%
  mutate(
    v = as.factor(match(version, v_vals))
  )





df %>%
  group_by(v) %>%
  get_summary_stats(DRIPostV3, type = "common")

# plot data
bxp <- ggboxplot(df, x = "v", y = "DRIPostV3", add = "point")
bxp

# identify outliers
df %>%
  group_by(v) %>%
  identify_outliers(DRIPostV3)


# normality assumption
df %>%
  group_by(v) %>%
  shapiro_test(DRIPostV3)

ggqqplot(df, "DRIPostV3", facet.by = "v")

# not ANOVA!
# res.aov <- anova_test(data = df, dv = DRIPostV3, wid = id, within = v)
# get_anova_table(res.aov)

res.fried <- df %>% friedman_test(DRIPostV3 ~ v |id)
res.fried

```

If a significant difference is found, we will conduct a post-hoc analysis using paired t-tests (or Wilcoxon signed-rank tests) for pairwise comparisons, with adjustments for multiple comparisons.

# DRI Benchmark

```{r alpha correlations, echo=FALSE}

# testing some correlations
models %>%
  full_join(alpha_summary, by=c("provider", "model")) %>%
  filter(
    model %in% included_models_summary$model,
    context_length < 2000000) %>%
  select(model, context_length, mean_alpha_all) %>%
  ggplot(aes(x = context_length, y = mean_alpha_all)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Correlation between Context Length and Mean Alpha All",
       x = "Context Length",
       y = "Mean Alpha All") +
  theme_minimal()

```

```{r benchmark index, fig.width=10,fig.height=6, warning=FALSE}

DRI_benchmark <- DRIInd.LLMs %>%
  filter(model %in% included_models_summary$model) %>%
  group_by(provider, model) %>%
  summarise(
    N = n(),
    .groups = "drop",
    agg_bm_index = mean(bm_indexV3, na.rm = TRUE)
  ) %>%
  filter(N == max(N)) %>% # only include models with all surveys
  arrange(desc(agg_bm_index))

included_models_summary <- included_models_summary %>%
  full_join(DRI_benchmark, by=c("provider", "model"))

DRI_benchmark %>%
  mutate(label = paste(provider, model, sep = "/")) %>%
  ggplot(aes(
    x = reorder(label, agg_bm_index),
    y = agg_bm_index,
    fill = provider
  )) +
  geom_bar(stat = "identity") +
  coord_flip() +
  geom_text(aes(label = round(agg_bm_index, 3)), hjust = -0.3, size = 3) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.background = element_rect(fill = "white"),
    legend.position = "none"
  ) +
  scale_y_continuous(limits = c(0, 1)) +
  labs(x = "", y = "DRI benchmark") -> plot


ggsave(
  paste(OUTPUT_DIR, "benchmarkV1.png", sep = "/"),
  plot,
  width = 10,
  height = 6
)

DRIInd.LLMs %>%
  filter(model %in% DRI_benchmark$model) %>% # only include models with all surveys
  mutate(label = paste(provider, model, sep = "/")) %>%
  ggplot(aes(
    x = reorder(label, bm_indexV3, FUN = median, na.rm = TRUE),
    y = bm_indexV2,
    color = provider
  )) +
  geom_boxplot() +
  coord_flip() +
  theme_minimal() +
  theme(plot.background = element_rect(fill = "white"),
        legend.position = "none") + scale_y_continuous(limits = c(-0.1, 1)) +
  labs(x = "", y = "DRI benchmark") -> plot

ggsave(
  paste(OUTPUT_DIR, "benchmarkV2.png", sep = "/"),
  plot,
  width = 10,
  height = 6
)


## Trying a 2-dimension benchmark
DRI_benchmark <- full_join(alpha_results, DRIInd.LLMs, by = c("provider", "model", "survey")) %>%
  filter(model %in% DRI_benchmark$model) # only include models with all surveys

DRI_benchmark %>%
  #  filter(provider == "google") %>%
  #mutate(label = paste(provider, model, sep = "/")) %>%
  group_by(provider, model, .groups = "drop") %>%
  summarise(
    mean_alpha = mean(alpha_all, na.rm = TRUE),
    mean_bm_index = mean(bm_indexV3, na.rm = TRUE),
    se_alpha = sd(alpha_all, na.rm = TRUE),
    se_bm_index = sd(bm_indexV3, na.rm = TRUE)
    
  ) %>%
  ggplot(aes(x = mean_alpha, y = mean_bm_index, color = provider)) +
  geom_point(size = 5) + # Adjust size as needed
  geom_text(aes(label = model), vjust = -2, size = 2) + # Add labels above each dot
  labs(x = "Cronbach's alpha (mean)", y = "DRI Benchmark Index (mean)", color = "Provider") +
  theme_minimal() +
  theme(plot.background = element_rect(fill = "white"),
        legend.position = "none") -> plot # Remove text from color legend

ggsave(
  paste(OUTPUT_DIR, "benchmarkV3.png", sep = "/"),
  plot,
  width = 10,
  height = 6
)

```

```{r francescos analysis}

#library(viridis)
library(grid)
library(Metrics)  # For MAE and RMSE
library(patchwork)

DATA_LLM <- LLM_alpha_DRI

DATA_LLM <- DATA_LLM %>%
  rename(
    DRIPostV3_aggr = DRIPostV3,
    DRIPostV3 = DRIPostV3_mean
  )

# Convert case and model to factors
DATA_LLM$case <- as.factor(DATA_LLM$case)
DATA_LLM$model <- as.factor(DATA_LLM$model)
#delta DRIPOSTV3-DRI_POST_HUMAN#
DATA_LLM$delta_post <- DATA_LLM$DRIPostV3 - DATA_LLM$human_only_DRIPost_meanV3
DATA_LLM$delta_pre <- DATA_LLM$DRIPreV3 - DATA_LLM$human_only_DRIPre_meanV3

#no outlier humans
DATA_LLM <- DATA_LLM %>%
  filter(!(case %in% c("Thalwil", "HGE Control Group", "GBR", "UPSA Control Group")))


# Subset where DRI is low
DATA_LLM_hiDRI <- subset(DATA_LLM, human_only_DRIPost_meanV3 > 0.1999999) #below randomness

# Subset where DRI is high
DATA_LLM_lowDRI <- subset(DATA_LLM, human_only_DRIPost_meanV3 <0.2)

```

## Comparison PRE and POST DRI by Provider

```{r dri provider, fig.width=10,fig.height=6}

#provider
DATA_LLM %>%
  group_by(provider) %>%
  summarise(
    DRI_POST = first(DRIPostV3),
    DRI_PRE = first(DRIPreV3)) %>%
ggplot(aes(x = DRI_PRE, y = DRI_POST, color = provider, label = provider)) +
  geom_point(size = 5) +  # Plot points
  geom_text(vjust = -1, hjust = 0.5, size = 4) +  # Add case labels
  theme_minimal() + 
  labs(
       x = "DRI_PRE",
       y = "DRI_POST",
       color = "provider") +
  theme(legend.position = "none")
```

## Comparison PRE and POST DRI by Model

```{r dri model, fig.width=10,fig.height=6}

DATA_LLM %>%
  group_by(model) %>%
  summarise(DRI_POST = first(DRIPostV3),
            DRI_PRE = first(DRIPreV3)) %>%
  
  ggplot(aes(
    x = DRI_PRE,
    y = DRI_POST,
    color = model,
    label = model
  )) +
  geom_point(size = 4) +
  geom_text(vjust = -1,
            hjust = 0.5,
            size = 3) + 
  theme_minimal() +
  labs(
    x = "DRI_PRE",
    y = "DRI_POST",
    color = "Model"
  ) +
  coord_fixed(xlim = c(-0.6, 0.8), ylim = c(-0.6, 0.8)) + 
  theme(legend.position = "none")

```

## Heatmap of DRI Scores by Case and Model

```{r dri heat, fig.width=10,fig.height=6}

DATA_LLM %>%
  group_by(model, case) %>%
  summarise(
    DRI_PRE = first(DRIPreV3),
    DRI_POST = first(DRIPostV3),
    .groups = "drop"
  ) %>%
  # Pivot data
  pivot_longer(
    cols = c(DRI_PRE, DRI_POST),
    names_to = "Metric",
    values_to = "Score"
  ) %>%
  
  # Plot
  ggplot(aes(x = case, y = model, fill = Score)) +
  geom_tile() +
  facet_wrap( ~ Metric) +
  scale_fill_viridis_c() +
  labs(
       x = "Case",
       y = "Model",
       fill = "Score") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(
      angle = 90,
      vjust = 0.5,
      size = 10,
      hjust = 1
    ),
    # Larger, clearer x-axis text
    axis.text.y = element_text(size = 6, hjust = 1),
    # Larger, clearer y-axis text
    axis.title.x = element_text(size = 8, margin = margin(t = 10)),
    # Larger x-axis title, spaced
    axis.title.y = element_text(size = 8, margin = margin(r = 10)),
    # Larger y-axis title, spaced
    plot.margin = margin(15, 15, 15, 15)                                        # Extra margin around plot
  )

```

## Boxplot of LLM DRI Post by Case

```{r dri post human, fig.width=10,fig.height=6, warning=FALSE}

DATA_LLM %>% 
  ggplot(aes(x = case, y = DRIPostV3)) +
  geom_boxplot(fill = "lightblue") +
  geom_hline(yintercept = 0, linetype = "dotted", color = "red", linewidth = 1) +
  
  # Add red diamond with color mapping for legend
  stat_summary(
    aes(y = human_only_DRIPost_meanV3, color = "Human DRI mean"),
    fun = mean,
    geom = "point",
    shape = 18,
    size = 3.5
  ) +
  
  # Define color in legend
  scale_color_manual(
    name = "",
    values = c("Human DRI mean" = "red")
  ) +
  
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top"
  ) +
  labs(
    x = "Case",
    y = "DRIPost"
  )
```

## LLM Performance Metrics Against Human DRI Post-Scores

```{r llm performance, warning=FALSE}

llm_scores <- DATA_LLM %>%
  filter(
    !is.na(DRIPostV3) &
      !is.na(human_only_DRIPost_meanV3) &
      human_only_DRIPost_meanV3 != 0
  ) %>%
  mutate(
    delta_post = as.numeric(delta_post),
    perc_error = abs((DRIPostV3 - human_only_DRIPost_meanV3) / human_only_DRIPost_meanV3
    )
  ) %>%
  group_by(model) %>%
  summarise(
    MAE = mae(DRIPostV3, human_only_DRIPost_meanV3),
    RMSE = rmse(DRIPostV3, human_only_DRIPost_meanV3),
    MAPE = mean(perc_error) * 100,
    # Mean Absolute Percentage Error
    human_range = max(human_only_DRIPost_meanV3) - min(human_only_DRIPost_meanV3),
    NMAE = MAE / human_range,
    NRMSE = RMSE / human_range,
    Spearman = cor(DRIPostV3, human_only_DRIPost_meanV3, method = "spearman"),
    delta = mean(delta_post, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(NRMSE)

knitr::kable(llm_scores, caption = "LLM Performance Metrics Against Human DRI Post-Scores", digits = 3, col.names = c(
    model = "Model",
    MAE = "MAE",
    RMSE = "RMSE",
    MAPE = "MAPE (%)",
    human_range = "Human Range",
    NMAE = "NMAE",
    NRMSE = "NRMSE",
    Spearman = "Spearman",
    delta = "Delta"
  ), label = "LLM_metrics")

```

## PRE vs. POST Aggregate Scores Correlation Across LLMs

```{r llm metrics normalize, fig.width=10,fig.height=6, warning=FALSE}

######LLM score normalization####
llm_scorespre <- DATA_LLM %>%
  filter(!is.na(DRIPreV3) & !is.na(human_only_DRIPre_meanV3) & human_only_DRIPre_meanV3 != 0) %>%
  mutate(
    delta_post = as.numeric(delta_pre),
    perc_error = abs((DRIPreV3 - human_only_DRIPre_meanV3) / human_only_DRIPre_meanV3)
  ) %>%
  group_by(model) %>%
  summarise(
    MAE = mae(DRIPreV3, human_only_DRIPre_meanV3),
    RMSE = rmse(DRIPreV3, human_only_DRIPre_meanV3),
    MAPE = mean(perc_error) * 100,  # Mean Absolute Percentage Error
    human_range = max(human_only_DRIPre_meanV3) - min(human_only_DRIPre_meanV3),
    NMAE = MAE / human_range,
    NRMSE = RMSE / human_range,
    Spearman = cor(DRIPreV3, human_only_DRIPre_meanV3, method = "spearman"),
    delta = mean(delta_pre, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(NRMSE)

###llm normalize against human whete human RNRMSE = 0, delta = 0, Spearman = 1
# Lower is better, human ideal = 0

# Lower is better, human ideal = 0, min = 0, max = 1 (values > 1 = 0)
normalize_lower_better_human_ref <- function(x, human_value = 0, min_value = 0, max_value = 1) {
  # Clip values to min range, set > max_value to 0 later
  x <- pmax(min_value, x)
  # Normalize: 1 at human_value (0), 0 at max_value (1) or beyond
  score <- 1 - (x - human_value) / (max_value - human_value)
  score[x > max_value] <- 0  # Anything above 1 gets 0
  score
}

# Higher is better, human ideal = 1, min = -1, max = 1
normalize_higher_better_human <- function(x, human_value = 1, min_value = -1, max_value = 1) {
  # Clip values to theoretical range [-1, 1]
  x <- pmax(min_value, pmin(max_value, x))
  # Normalize: 1 at human_value (1), 0 at min_value (-1)
  (x - min_value) / (human_value - min_value)
}


normalize_delta_beyond_human <- function(x, human_value = 0) {
  dev <- abs(x - human_value)
  baseline_dev <- max(dev, na.rm = TRUE)
  score <- 1 - dev / baseline_dev
  # Adjust for values beyond human (e.g., positive delta)
  score[x > human_value] <- 1 + (x[x > human_value] - human_value) / baseline_dev
  score[x < -human_value] <- 1 - (abs(x[x < -human_value]) - human_value) / baseline_dev
  score
}


# Then apply:

human_delta <- 0


# Apply to LLM scores
llm_scores$normalized_NRMSE <- normalize_lower_better_human_ref(llm_scores$NRMSE, human_value = 0)
llm_scores$normalized_Spearman <- normalize_higher_better_human(llm_scores$Spearman, human_value = 1)
llm_scores$normalized_Delta <- normalize_delta_beyond_human(llm_scores$delta, human_value = human_delta)
# lower delta is better


llm_scorespre$normalized_NRMSE <- normalize_lower_better_human_ref(llm_scorespre$NRMSE, human_value = 0)
llm_scorespre$normalized_Spearman <- normalize_higher_better_human(llm_scorespre$Spearman, human_value = 1)
llm_scorespre$normalized_Delta <- normalize_delta_beyond_human(llm_scorespre$delta, human_value = human_delta) 
# Compute aggregate score (closer to 1 = more human-like)
llm_scores$aggregate_index <- rowMeans(llm_scores[, c("normalized_NRMSE", "normalized_Spearman", "normalized_Delta")], na.rm = TRUE)

llm_scorespre$aggregate_index <- rowMeans(llm_scorespre[, c("normalized_NRMSE", "normalized_Spearman", "normalized_Delta")], na.rm = TRUE)

# Sort and display
llm_scores_sorted <- llm_scores[order(llm_scores$aggregate_index, decreasing = TRUE), ]

llm_scores_sortedpre <- llm_scorespre[order(llm_scorespre$aggregate_index, decreasing = TRUE), ]



#  Combine PRE and POST data
combined_scores <- llm_scores_sorted %>%
  select(model, aggregate_index) %>%
  rename(POST = aggregate_index) %>%
  left_join(
    llm_scores_sortedpre %>%
      select(model, aggregate_index) %>%
      rename(PRE = aggregate_index),
    by = "model"
  )

# KEEP INTO THE REPORT Create XY scatter plot without legend
ggplot(combined_scores, aes(x = PRE, y = POST, color = model)) +
  # Points for each model
  geom_point(size = 3) +
  # Model names as labels
  geom_text(aes(label = model), vjust = -1, hjust = 0.5, size = 3) +
  # Human benchmark lines
  geom_vline(xintercept = 1, linetype = "dashed", color = "black", size = 0.5, alpha = 0.7) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "black", size = 0.5, alpha = 0.7) +
  # Customize
  labs(
       x = "PRE Aggregate Index (Human = 1)",
       y = "POST Aggregate Index (Human = 1)") +
  theme_minimal() +
  # Remove legend
  theme(legend.position = "none") +
  # Adjust axis limits for visibility
  scale_x_continuous(limits = c(min(combined_scores$PRE, na.rm = TRUE) - 0.1, max(combined_scores$PRE, na.rm = TRUE) + 0.1)) +
  scale_y_continuous(limits = c(min(combined_scores$POST, na.rm = TRUE) - 0.1, max(combined_scores$POST, na.rm = TRUE) + 0.1)) +
  # Diagonal reference line (PRE = POST)
  geom_abline(slope = 1, intercept = 0, linetype = "dotted", color = "grey", size = 0.5)




```

## Human-Normalized Performance

```{r normalized, fig.width=10,fig.height=6}

# Reshape to long format

  plot_long <- llm_scores_sorted %>%
  select(model, normalized_NRMSE, normalized_Spearman, normalized_Delta) %>%
  pivot_longer(
    cols = starts_with("normalized_"),
    names_to = "Metric",
    values_to = "Score"
  ) %>%
  mutate(
    Metric = dplyr::recode(Metric,
                    normalized_NRMSE = "NRMSE (Normalized)",
                    normalized_Spearman = "Spearman (Normalized)",
                    normalized_Delta = "Delta (Normalized)"),
    model = factor(model, levels = rev(llm_scores_sorted$model))  # maintain order
  )

###KEEP INTO THE REPORT#####
ggplot(plot_long, aes(x = model, y = Score, color = Metric, group = Metric)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  # Human benchmark line (red dotted)
  geom_hline(yintercept = 1, linetype = "dotted", color = "red", linewidth = 1) +
  labs(
    subtitle = "Red dotted line = Human benchmark (Normalized Score for each indicators = 1)",
    x = "Model",
    y = "Normalized Score (0-1 scale)",
    color = "Metric"
  ) +
  scale_color_manual(values = c(
    "NRMSE (Normalized)" = "#FF6666",
    "Spearman (Normalized)" = "#66CC66",
    "Delta (Normalized)" = "#3399FF"
  )) +
  theme_minimal(base_size = 13) +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 10),
    axis.text.y = element_text(face = "bold"),
    legend.position = "top"
  ) +
  ylim(0, 1.09)
```

## LLM Performance by Reasoner Classification

Architecture types:

-   Transformer-based models [@vaswani2017].

Some models are considered "reasoning" models, like , reason using chain-of-thought (CoT) -- this is not a difference in architecture

```{r reasoner, fig.width=10,fig.height=6}

##FIXME: add this to model info


llm_scores_reason <- llm_scores %>%
  full_join(included_models_summary, by="model") %>%
  mutate(
    is_reasoner = case_when(is_reasoner == TRUE ~ TRUE, .default = FALSE)
  )

# Run one-tailed Wilcoxon test
wilcox_res <- wilcox.test(
  aggregate_index ~ is_reasoner,
  data = llm_scores_reason,
  alternative = "less"  # or "less", depending on your hypothesis
)

# Format p-value for annotation
p_val <- wilcox_res$p.value
p_label <- paste0("Wilcoxon test (one-tailed)\np = ", signif(p_val, 3))
ggplot(llm_scores_reason, aes(x = is_reasoner, y = aggregate_index, fill = is_reasoner)) +
  geom_boxplot(show.legend = TRUE, alpha = 0.8) +
  geom_jitter(width = 0.2, alpha = 0.6) +
  geom_hline(yintercept = 1.2, linetype = "dotted", color = "red", linewidth = 1) +
  annotate("text", 
           x = 1.5,  # Adjust for better placement if needed
           y = max(llm_scores$aggregate_index) * 1.05,
           label = p_label,
           size = 4.2, hjust = 0.5) +
  labs(
    subtitle = "Aggregate Index (Higher = More Human-like; Red line = Human level)",
    x = "Reasoning LLM architecture",
    y = "Aggregate Performance Index"
  ) +
  theme_minimal(base_size = 13) +
  coord_flip()
```

### References
