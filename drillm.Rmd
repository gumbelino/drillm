---
title: 'Triage Against the Machine: Can AI Reason Deliberatively?'
author: "Francesco Veri, Gustavo Umbelino"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(reticulate)
library(tidyverse)
library(readxl)
library(rlang)
library(psych)
library(bootstrap)

```


## Define functions
Maybe move this to it's own package...
```{r}

create_file_path <- function(provider, model, survey, file_type) {
  file.path("llm_data", provider, model, paste0(survey, "_", file_type, ".csv"))
}

```


## Get available LLMs
```{r }

# Read the CSV file into a data frame and remove duplicates
models <- read_csv("private/llms_v2.csv", show_col_types = FALSE) %>%
  distinct(provider, model)

# Initialize a vector to store the 'has_data' values
has_data_flags <- logical(nrow(models))

# Iterate over each row in the models data frame
for (i in 1:nrow(models)) {
  provider <- models$provider[i]
  model <- models$model[i]
  
  # Create the path
  path <- paste0("llm_data/", provider, "/", model)
  
  # Check if the path exists and set the 'has_data' flag accordingly
  has_data_flags[i] <- file.exists(path)
}

# Add the 'has_data' column to the models data frame
models <- models %>%
  mutate(has_data = has_data_flags)

# Print rows where has_data is TRUE
if (any(models$has_data)) {
  print(models %>% filter(has_data == TRUE))
} else {
  warn("No data available!")
}

```


## Get available surveys
```{r survey names}

# Read the sheet names of the Excel file
survey_names <- excel_sheets("data/surveys_v2.xlsx")

print(survey_names)

# Define the file types
file_types <- c("considerations", "policies", "reasons")

```


## Read and format LLM data
```{r format LLM data}

# initialize an empty list to store the data frames
data_list <- list()
index <- 0

# iterate over each survey
for (survey_name in survey_names) {
  
  # iterate over each row in the models data frame where has_data is TRUE
  for (i in 1:nrow(models)) {
    if (models$has_data[i]) {
      provider <- models$provider[i]
      model <- models$model[i]
      
      # check if any file for the survey exists
      survey_path <- paste0("llm_data/", provider, "/", model, "/", survey_name)
      if (!any(file.exists(paste0(
        survey_path, "_", file_types, ".csv"
      )))) {
        next
      }
      
      # Iterate over each file type
      for (file_type in file_types) {
        # Create the file path
        file_path <- create_file_path(provider, model, survey_name, file_type)
        index <- index + 1
        
        # Check if the file exists
        if (file.exists(file_path)) {
          # Read the CSV file
          temp_data <- read_csv(file_path, show_col_types = FALSE)
          
          # Skip file if file exists but has no data
          if (nrow(temp_data) == 0) {
            warn(paste0(file_path, " exists but has no data!"))
            break
          }
          
          meta <- c(
            "cuid",
            "created_at",
            "provider",
            "model",
            "input_tokens",
            "output_tokens"
          )
          
          # Select the relevant columns based on file type
          if (file_type == "considerations") {
            survey_data <- temp_data %>%
              rename_with( ~ paste0("C", seq_along(.)),
                           starts_with("C", ignore.case = FALSE))
            
            # add column "survey" to meta data
            survey_data <- survey_data %>%
              mutate(survey = survey_name) %>%
              relocate(survey, .after = model)
            meta <- c(meta, "survey")
            
            # Ensure survey_data has columns up to C50
            for (j in (ncol(survey_data) - length(meta) + 1):50) {
              survey_data[[paste0("C", j)]] <- as.numeric(NA)
            }
            
            # go to next file type
            next
            
          } else if (file_type == "policies") {
            temp_data <- temp_data %>%
              select(cuid, starts_with("P", ignore.case = FALSE)) %>%
              rename_with( ~ paste0("P", seq_along(.)),
                           starts_with("P", ignore.case = FALSE))
            
            # Ensure temp_data has columns up to C50
            for (j in (ncol(temp_data)):10) {
              temp_data[[paste0("P", j)]] <- as.numeric(NA)
            }
            
          } else if (file_type == "reasons") {
            temp_data <- temp_data %>%
              select(cuid, reason) %>%
              rename(R = reason)
          }
          
          # merge the data frames by 'cuid' and keep all rows
          survey_data <- full_join(survey_data, temp_data, by = c("cuid"))
          
        }
      }
      
      # Add the survey_data data frame to the list
      if (exists("survey_data")) {
        data_list[[length(data_list) + 1]] <- survey_data
        
        # Remove the survey_data data frame to free up memory
        rm(survey_data)
      }
      
    }
  }
}

# Combine all data frames in the list into a single data frame
llm_data <- bind_rows(data_list)

# delete data_list from memory
rm(data_list)
rm(temp_data)

# Aggregate llm_data by provider, model, and survey and N the number of rows
llm_data_summary <- llm_data %>%
  group_by(provider, model, survey) %>%
  summarise(N = n(), .groups = 'drop')

# Print the summary
print(llm_data_summary)

```


## Calculate Cronbach's Alpha
```{r echo=FALSE}

# Initialize an empty list to store the alpha results
alpha_results <- list()

# Iterate over each unique provider/model combination
for (provider_model in unique(paste(llm_data$provider, llm_data$model, sep = "/"))) {
  # Filter the data for the current provider/model
  provider_model_data <- llm_data %>% filter(paste(provider, model, sep = "/") == provider_model)
  
  # Iterate over each survey
  for (survey_name in unique(provider_model_data$survey)) {
    # Filter the data for the current survey
    survey_data <- provider_model_data %>% filter(survey == !!survey_name)
    
    # Calculate Cronbach's Alpha for considerations (C1..C50)
    considerations_data <- survey_data %>% select(starts_with("C", ignore.case = FALSE))
    
    #considerations_data <- considerations_data[, colSums(is.na(considerations_data)) != nrow(considerations_data)]

    if (ncol(considerations_data) > 1) {
      alpha_considerations <- alpha(considerations_data, check.keys = TRUE)$total$raw_alpha
    } else {
      alpha_considerations <- NA
    }
    
    # Calculate Cronbach's Alpha for policies (P1..P10)
    policies_data <- survey_data %>% select(starts_with("P", ignore.case = FALSE))
    
    #policies_data <- policies_data[, colSums(is.na(policies_data)) != nrow(policies_data)]

    
    if (ncol(policies_data) > 1) {
      alpha_policies <- alpha(policies_data, check.keys = TRUE)$total$raw_alpha
    } else {
      alpha_policies <- NA
    }
    
    # Store the results in the list
    alpha_results[[length(alpha_results) + 1]] <- tibble(
      provider_model = provider_model,
      survey = survey_name,
      N = nrow(considerations_data),
      alpha_considerations = alpha_considerations,
      alpha_policies = alpha_policies
    )
  }
}

# Combine all results into a single data frame
alpha_results <- bind_rows(alpha_results)

rm(considerations_data)
rm(survey_data)
rm(policies_data)
rm(provider_model_data)

# Print the results
print(alpha_results)

```

## Check alpha results per model
```{r}

# Aggregate alpha_results by model and calculate summary statistics
alpha_summary <- alpha_results %>%
  group_by(provider_model) %>%
  summarise(
    min_alpha_considerations = min(alpha_considerations, na.rm = TRUE),
    max_alpha_considerations = max(alpha_considerations, na.rm = TRUE),
    mean_alpha_considerations = mean(alpha_considerations, na.rm = TRUE),
    std_alpha_considerations = sd(alpha_considerations, na.rm = TRUE),
    min_alpha_policies = min(alpha_policies, na.rm = TRUE),
    max_alpha_policies = max(alpha_policies, na.rm = TRUE),
    mean_alpha_policies = mean(alpha_policies, na.rm = TRUE),
    std_alpha_policies = sd(alpha_policies, na.rm = TRUE)
  )

# Print the summary
print(alpha_summary)
```

## Aggregate considerations and preferences
```{r}

# Initialize an empty list to store the alpha results
aggregation_results <- list()

calculate_mode <- function(x) {
  if (length(x) == 0) {
    return(NA)
  }
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}


aggregate_llm_considerations <- function(considerations) {
  # Ensure there are columns to aggregate
  if (ncol(considerations) == 0) {
    return(tibble())
  }

  # Calculate the mode for each column
  mode_considerations <- considerations %>%
    summarise(across(everything(), calculate_mode))

  return(mode_considerations)
  
}

aggregate_llm_policies_mode <- function(policies) {
  # Ensure there are columns to aggregate
  if (ncol(policies) == 0) {
    return(tibble())
  }
  
  # Calculate the mode for each column
  mode_policies <- policies %>%
    summarise(across(everything(), calculate_mode))
  
  return(mode_policies)
}

aggregate_llm_policies_first <- function(policies) {
  if (nrow(policies) == 0) {
    return(tibble())
  }
  
  # return first row of dataset
  return(policies[1])
}

# Iterate over each unique provider/model combination
for (provider_model in unique(paste(llm_data$provider, llm_data$model, sep = "/"))) {
  # Filter the data for the current provider/model
  provider_model_data <- llm_data %>% filter(paste(provider, model, sep = "/") == provider_model)
  
  # Iterate over each survey
  for (survey_name in unique(provider_model_data$survey)) {
    # Filter the data for the current survey
    survey_data <- provider_model_data %>% filter(survey == !!survey_name)
    
    # Calculate Cronbach's Alpha for considerations (C1..C50)
    considerations_data <- survey_data %>% select(starts_with("C", ignore.case = FALSE))
    
    aggregated_considerations <- aggregate_llm_considerations(considerations_data)
    
    # Calculate Cronbach's Alpha for policies (P1..P10)
    policies_data <- survey_data %>% select(starts_with("P", ignore.case = FALSE))
    
    aggregated_policies <- aggregate_llm_policies_mode(policies_data)

    # store the results in the list
    aggregation_result <- tibble(
      provider_model = provider_model,
      survey = survey_name,
      N = nrow(considerations_data)
    )

    aggregation_result <- aggregation_result %>%
      bind_cols(aggregated_considerations) %>%
      bind_cols(aggregated_policies)

    aggregation_results[[length(aggregation_results) + 1]] <- aggregation_result

  }
}

# Combine all results into a single data frame
aggregation_results <- bind_rows(aggregation_results)

rm(aggregation_result)
rm(considerations_data)
rm(policies_data)
rm(provider_model)
rm(aggregated_considerations)
rm(aggregated_policies)

print(aggregation_results)

```


## Read and format human data
```{r}

get_human_data <- function(data,
                           survey_name = "Uppsala Speaks",
                           case_name = "Activate",
                           stage_analysis = "Post-Delib") {
  # Filter data for "Upsala Speaks" and "Post-Delib"
  filtered_human_data <- data %>%
    filter(Study == survey_name &
             Stage_Analysis == Stage_Analysis,
           Case == case_name) %>%
    select(PNum,
           matches("C\\d+"),
           matches("P\\d+")) %>%
    rename(subjectID = PNum) %>%
    mutate(subject = "human") %>%
    relocate(subject, subjectID, .before = 1)
  
  return(filtered_human_data)
  
}


# Import the CSV file into a data frame
human_data <- read_csv("data/total dataset_clean.csv", show_col_types = FALSE)

# Rename columns to be consistent with LLM data
human_data <- human_data %>%
  rename_with( ~ sub("^U0|^U", "C", .), starts_with("U", ignore.case = FALSE)) %>%
  rename_with( ~ sub("^Pref", "P", .), starts_with("Pref", ignore.case = FALSE))

# Print the first few rows of the data frame
print(head(human_data))

```

## Merge LLM and human data 

```{r}

filtered_human_data <- human_data %>% get_human_data()

get_DRI(filtered_human_data)


get_LLM_data <- function(data,
                         survey_name = "1.Uppsala Speaks",
                         provider_model = "google/gemma2") {
  # Filter aggregation_results for survey "1.Uppsala Speaks"
  filtered_aggregation_results <- data %>%
    filter(survey == survey_name &
             provider_model == !!provider_model) %>%
    mutate(subject = "llm") %>%
    rename(subjectID = provider_model) %>%
    select(subject, subjectID, matches("C\\d+"), matches("P\\d+"))
  
  return(filtered_aggregation_results)
  
}

filtered_llm_data <- aggregation_results %>% get_LLM_data()

filtered_human_data <- filtered_human_data %>%
  transform(subjectID = as.character(subjectID))

# Append the values
appended_data <- bind_rows(filtered_human_data, filtered_llm_data)

# Print the appended data
print(appended_data)
```


### Calculate 1.Uppsala Speaks human and LLM DRI
```{r}

dri_calc <- function(data, v1, v2) {
  lambda <- 1 - (sqrt(2) / 2)
  dri <- 2 * (((1 - mean(abs((data[[v1]] - data[[v2]]) / sqrt(2)
  ))) - (lambda)) / (1 - (lambda))) - 1
  
  return(dri)
}


get_DRI <- function(data) {
  
  data <- appended_data
  # make sure there's data to analyze
  if (nrow(data) > 0) {
    # get participant numbers/ids
    PNums <- data$subjectID
    
    # variables for reading COLUMN data
    # Q is a list considerations (Likert scale)
    # - there are up to 50 questions
    # R is a list ratings (rankings)
    Q <- data %>% select(starts_with("C", ignore.case = FALSE))
    R <- data %>% select(starts_with("P", ignore.case = FALSE))
    
    # remove all NA columns (in case there are less than 50
    # consideration questions
    Q <- Q[, colSums(is.na(Q)) != nrow(Q)]
    R <- R[, colSums(is.na(R)) != nrow(R)]
    
    # transpose data
    Q <- t(Q)
    R <- t(R)
    
    # format data as data frame
    Q <- as.data.frame(Q)
    R <- as.data.frame(R)
    
    # name columns with participant numbers
    colnames(Q) <- PNums
    colnames(R) <- PNums
    
    # obtain a list of correlations without duplicates
    # cor() returns a correlation matrix between Var1 and Var2
    # Var1 and Var2 are the variables being correlated
    # Freq is the correlation
    QWrite <- subset(as.data.frame(as.table(cor(Q, method = "spearman"))),
                     match(Var1, names(Q)) > match(Var2, names(Q)))
    
    RWrite <- subset(as.data.frame(as.table(cor(R, method = "spearman"))),
                     match(Var1, names(R)) > match(Var2, names(R)))
    
    # initialize the output in the first iteration
    IC <- data.frame("P_P" = paste0(QWrite$Var1, '-', QWrite$Var2))
    IC$P1 <- QWrite$Var1
    IC$P2 <- QWrite$Var2
    
    
    # prepare QWrite
    QWrite <- as.data.frame(QWrite$Freq)
    names(QWrite) <- "Q"
    
    # prepare RWrite for merge
    RWrite <- as.data.frame(RWrite$Freq)
    names(RWrite) <- "R"
    
    # merge
    IC <- cbind(IC, QWrite, RWrite)
  }
  
  
  # append case & study info
  IC$Study <- "Uppsala Speaks"
  IC$Case <- "Test"
  
  ### Analysis ###
  
  ## IC Points calculations ##
  IC$IC <- 1 - abs((IC$R - IC$Q) / sqrt(2))
  
  ## Group DRI level ##
  DRI <- dri_calc(data = IC, v1 = 'R', v2 = 'Q')
  
  ## INDIVIDUAL DRI ##
  
    Plist <- unique(c(IC$P1, IC$P2))
    
    Plist <- Plist[order(Plist)]

    DRIInd <- data.frame('Participant' = Plist)

    #Add individual-level metrics
    for(i in 1:length(Plist)){
      DRIInd$DRI[i] <- dri_calc(data = IC  %>% filter(P1 == Plist[i] | P2 == Plist[i]), 
                                   v1 = 'R', v2 = 'Q')
    }
    
    return(DRI)

  
}

# get human-only DRI
get_DRI(filtered_human_data)

# get DRI with LLM
get_DRI(appended_data)


```

```{r}





# Append the values
appended_data <- bind_rows(filtered_human_data, filtered_aggregation_results)

# Print the appended data
print(appended_data)


```

