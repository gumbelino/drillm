---
title: 'Triage Against the Machine: Can AI Reason Deliberatively?'
author: "Francesco Veri, Gustavo Umbelino"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(reticulate)
library(tidyverse)
library(readxl)
library(rlang)
library(psych)
library(boot)
library(vote) # might need to manually install: https://cran.r-project.org/doc/manuals/r-release/R-admin.html#Fortran-compiler

SURVEY_FILE <- "data/surveys_v4.xlsx"
OUTPUT_DIR <- "analysis"

# create output directory if it doesn't exist
if (!dir.exists(OUTPUT_DIR)) {
  dir.create(OUTPUT_DIR, recursive = TRUE)
}

```


## Define functions
Maybe move this to it's own package...
```{r}

create_file_path <- function(provider, model, survey, file_type) {
  file.path("llm_data", provider, model, survey, paste0(file_type, ".csv"))
}

```


## Get available LLMs
```{r }

# Read the CSV file into a data frame and remove duplicates
models <- read_csv("private/llms_v2.csv", show_col_types = FALSE) %>%
  distinct(provider, model)

# Initialize a vector to store the 'has_data' values
has_data_flags <- logical(nrow(models))

# Iterate over each row in the models data frame
for (i in 1:nrow(models)) {
  provider <- models$provider[i]
  model <- models$model[i]
  
  # Create the path
  path <- paste0("llm_data/", provider, "/", model)
  
  # Check if the path exists and set the 'has_data' flag accordingly
  has_data_flags[i] <- file.exists(path)
}

# Add the 'has_data' column to the models data frame
models <- models %>%
  mutate(has_data = has_data_flags)

# Print rows where has_data is TRUE
if (any(models$has_data)) {
  print(models %>% filter(has_data == TRUE))
} else {
  warn("No data available!")
}

```


## Get available surveys
```{r survey names}

# Read the sheet names of the Excel file
survey_names <- excel_sheets(SURVEY_FILE)

# remove invalid and "template" 
survey_names <- survey_names[!grepl("^~", survey_names) & survey_names != "template"]

print(survey_names)

# Define the file types
file_types <- c("considerations", "policies", "reasons")

```


## Read and format LLM data
```{r format LLM data}

# initialize an empty list to store the data frames
data_list <- list()
index <- 0

# iterate over each survey
for (survey_name in survey_names) {
  
  # iterate over each row in the models data frame where has_data is TRUE
  for (i in 1:nrow(models)) {
    if (models$has_data[i]) {
      provider <- models$provider[i]
      model <- models$model[i]
      
      # check if any file for the survey exists
      survey_path <- paste0("llm_data/", provider, "/", model, "/", survey_name, "/")
      if (!any(file.exists(paste0(survey_path, file_types, ".csv")))) {
        next
      }
      
      # Iterate over each file type
      for (file_type in file_types) {
        # Create the file path
        file_path <- create_file_path(provider, model, survey_name, file_type)
        index <- index + 1
        
        # Check if the file exists
        if (file.exists(file_path)) {
          # Read the CSV file
          temp_data <- read_csv(file_path, show_col_types = FALSE)
          
          # Skip file if file exists but has no data
          if (nrow(temp_data) == 0) {
            warn(paste0(file_path, " exists but has no data!"))
            break
          }
          
          meta <- c(
            "cuid",
            "created_at",
            "provider",
            "model",
            "temperature",
            "input_tokens",
            "output_tokens"
          )
          
          # Select the relevant columns based on file type
          if (file_type == "considerations") {
            survey_data <- temp_data %>%
              rename_with( ~ paste0("C", seq_along(.)),
                           starts_with("C", ignore.case = FALSE))
            
            # add column "survey" to meta data
            survey_data <- survey_data %>%
              mutate(survey = survey_name) %>%
              relocate(survey, .after = model)
            meta <- c(meta, "survey")
            
            # Ensure survey_data has columns up to C50
            for (j in (ncol(survey_data) - length(meta) + 1):50) {
              survey_data[[paste0("C", j)]] <- as.numeric(NA)
            }
            
            # go to next file type
            next
            
          } else if (file_type == "policies") {
            temp_data <- temp_data %>%
              select(cuid, starts_with("P", ignore.case = FALSE)) %>%
              rename_with( ~ paste0("P", seq_along(.)),
                           starts_with("P", ignore.case = FALSE))
            
            # Ensure temp_data has columns up to C50
            for (j in (ncol(temp_data)):10) {
              temp_data[[paste0("P", j)]] <- as.numeric(NA)
            }
            
          } else if (file_type == "reasons") {
            temp_data <- temp_data %>%
              select(cuid, reason) %>%
              rename(R = reason)
          }
          
          # merge the data frames by 'cuid' and keep all rows
          survey_data <- full_join(survey_data, temp_data, by = c("cuid"))
          
        }
      }
      
      # Add the survey_data data frame to the list
      if (exists("survey_data")) {
        data_list[[length(data_list) + 1]] <- survey_data
        
        # Remove the survey_data data frame to free up memory
        rm(survey_data)
      }
      
    }
  }
}

# Combine all data frames in the list into a single data frame
llm_data <- bind_rows(data_list)

# delete data_list from memory
rm(data_list)
rm(temp_data)

# Aggregate llm_data by provider, model, and survey and N the number of rows
llm_surveys <- llm_data %>%
  group_by(provider, model, survey) %>%
  summarise(N = n(), .groups = 'drop')

# Print the summary
print(llm_surveys)

# write summary to file
write_csv(llm_surveys, paste(OUTPUT_DIR, "llm_surveys.csv", sep = "/"))

```


## Calculate Cronbach's Alpha
```{r echo=FALSE}

# Initialize an empty list to store the alpha results
alpha_results <- list()

# Iterate over each unique provider/model combination
for (provider_model in unique(paste(llm_data$provider, llm_data$model, sep = "/"))) {
  # Filter the data for the current provider/model
  provider_model_data <- llm_data %>% filter(paste(provider, model, sep = "/") == provider_model)
  
  # Iterate over each survey
  for (survey_name in unique(provider_model_data$survey)) {
    # Filter the data for the current survey
    survey_data <- provider_model_data %>% filter(survey == !!survey_name)
    
    # Calculate Cronbach's Alpha for considerations (C1..C50)
    considerations_data <- survey_data %>% select(starts_with("C", ignore.case = FALSE))
    
    if (nrow(considerations_data) > 1) {
      alpha_considerations <- alpha(considerations_data, check.keys = TRUE, warnings = FALSE)$total$raw_alpha
    } else {
      alpha_considerations <- NA
    }
    
    # Calculate Cronbach's Alpha for policies (P1..P10)
    policies_data <- survey_data %>% select(starts_with("P", ignore.case = FALSE))
  
    if (nrow(policies_data) > 1) {
      alpha_policies <- alpha(policies_data, check.keys = TRUE, warnings = FALSE)$total$raw_alpha
    } else {
      alpha_policies <- NA
    }
    
    # Store the results in the list
    alpha_results[[length(alpha_results) + 1]] <- tibble(
      provider_model = provider_model,
      survey = survey_name,
      N = nrow(considerations_data),
      alpha_considerations = alpha_considerations,
      alpha_policies = alpha_policies
    )
  }
}

# Combine all results into a single data frame
alpha_results <- bind_rows(alpha_results)

rm(considerations_data)
rm(survey_data)
rm(policies_data)
rm(provider_model_data)

# Print the results
print(alpha_results)

# write summary to file
write_csv(alpha_results, paste(OUTPUT_DIR, "alpha_results.csv", sep = "/"))

```

## Check alpha results per model
```{r}

# Aggregate alpha_results by model and calculate summary statistics
alpha_summary <- alpha_results %>%
  group_by(provider_model) %>%
  summarise(
    min_alpha_considerations = min(alpha_considerations, na.rm = TRUE),
    max_alpha_considerations = max(alpha_considerations, na.rm = TRUE),
    mean_alpha_considerations = mean(alpha_considerations, na.rm = TRUE),
    std_alpha_considerations = sd(alpha_considerations, na.rm = TRUE),
    min_alpha_policies = min(alpha_policies, na.rm = TRUE),
    max_alpha_policies = max(alpha_policies, na.rm = TRUE),
    mean_alpha_policies = mean(alpha_policies, na.rm = TRUE),
    std_alpha_policies = sd(alpha_policies, na.rm = TRUE)
  )

# Print the summary
print(alpha_summary)
```

## Define aggregation functions
```{r aggregation functions}

# Function to calculate mode of data, same as stat_function
calc_mode <- function(data) {
  as.numeric(names(sort(table(data), decreasing = TRUE)[1]))
}

bootstrap_mode <- function(data, n_bootstrap = 1000) {
  
  # Return NA if data contains any NA
  if (any(is.na(data))) {
    return(NA)
  }
  
  # Define the statistic function for bootstrapping to find mode
  stat_function <- function(data, indices) {
    as.numeric(names(sort(table(data[indices]), decreasing = TRUE)[1]))
  }
  
  # Perform bootstrap
  results <- boot(data = data, statistic = stat_function, R = n_bootstrap)
  
  # Calculate bootstrapped mode
  b_mode <- calc_mode(results$t)
  
  # Return the bootstrapped modes
  return(b_mode)
}

calculate_mode <- function(x) {
  if (length(x) == 0) {
    return(NA)
  }
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

aggregate_llm_considerations <- function(considerations) {
  # Ensure there are columns to aggregate
  if (ncol(considerations) == 0) {
    return(tibble())
  }
  
  # Calculate the mode for each column
  mode_considerations <- considerations %>%
    summarise(across(everything(), bootstrap_mode))
  
  return(mode_considerations)
  
}

aggregate_llm_policies <- function(policies) {
  # Ensure there are columns to aggregate
  if (nrow(policies) == 0) {
    return(tibble())
  } else if (nrow(policies) == 1) {
    return(policies)
  }
  
  # Remove columns with NAs
  valid_policies <- policies[, colSums(is.na(policies)) != nrow(policies)]
  
  
  # Convert the policies to a ranked matrix
  ranked_matrix <- as.matrix(valid_policies)
  
  # Define the number of winners to all - 1 policies
  # stv complains if winners == all policies
  num_winners <- ncol(valid_policies) - 1
  
  # Run the Single Transferable Vote algorithm
  results <- stv(ranked_matrix, num_winners, quiet = TRUE)
  
  # add last policy to ranked result
  last_policy <- setdiff(colnames(valid_policies), results$elected)
  ranked_policies <- c(results$elected, last_policy)
  
  policy_order <- colnames(valid_policies)
  
  order <- match(policy_order, ranked_policies)
  
  # Calculate the number of missing values needed to reach length 10
  missing_columns <- ncol(policies) - length(order)
  
  # Fill in the missing values with NA
  order <- c(order, rep(NA, missing_columns))
  
  # Create a new data.frame with aggregated results
  policy_ranks <- data.frame(t(order))
  colnames(policy_ranks) <- colnames(policies)
  
  return(policy_ranks)
}

```


## Aggregate considerations and preferences
```{r}

aggregate_llm_data <- function(data) {
  
  # Initialize an empty list to store the alpha results
  aggregation_results <- list()
  
  # Iterate over each unique provider/model/survey combination
  for (row in 1:nrow(llm_surveys)) {
    provider <- llm_surveys[row, ]$provider
    model <- llm_surveys[row, ]$model
    survey <- llm_surveys[row, ]$survey
    N <- llm_surveys[row, ]$N
    
    # Filter the data for the current survey
    survey_data <- data %>%
      filter(provider == !!provider, model == !!model, survey == !!survey)
    
    # Calculate Cronbach's Alpha for considerations (C1..C50)
    considerations_data <- survey_data %>% select(starts_with("C", ignore.case = FALSE))
    
    aggregated_considerations <- aggregate_llm_considerations(considerations_data)
    
    # Calculate Cronbach's Alpha for policies (P1..P10)
    policies_data <- survey_data %>% select(starts_with("P", ignore.case = FALSE))
    
    aggregated_policies <- aggregate_llm_policies(policies_data)
    
    # store the results in the list
    aggregation_result <- tibble(
      provider = provider,
      model = model,
      survey = survey,
      N = N
    )
    
    aggregation_result <- aggregation_result %>%
      bind_cols(aggregated_considerations) %>%
      bind_cols(aggregated_policies)
    
    aggregation_results[[length(aggregation_results) + 1]] <- aggregation_result
    
  }
  
  
  # Combine all results into a single data frame
  aggregation_results <- bind_rows(aggregation_results)
  
  return(aggregation_results)
  
}

time_start <- Sys.time()
llm_data_aggregated <- aggregate_llm_data(llm_data)
time_end <- Sys.time()
elapsed_time <- difftime(time_end, time_start, units = "auto")

print(paste("LLM data aggregation completed in", round(as.numeric(elapsed_time),2), units(elapsed_time)))

print(llm_data_aggregated)

output_path <- paste(OUTPUT_DIR, "llm_data_aggregated.csv", sep = "/")

# write summary to file
write_csv(llm_data_aggregated, output_path)
print(paste("Results written to", output_path))

```


## Read and format human data
```{r}

# Import the CSV file into a data frame
human_data <- read_csv("data/total dataset_clean.csv", show_col_types = FALSE)

# Rename columns to be consistent with LLM data
human_data <- human_data %>%
  rename_with( ~ sub("^U0|^U", "C", .), starts_with("U", ignore.case = FALSE)) %>%
  rename_with( ~ sub("^Pref", "P", .), starts_with("Pref", ignore.case = FALSE)) %>%
  filter(Study != "Sydney CC Adaptation" & Study != "WA Biobank")

# Read the mapping file
study_survey_map <- read_csv("data/study_survey_map.csv", show_col_types = FALSE)

# Add a new column 'Survey' to human_data by matching 'Study' with 'survey'
human_data <- human_data %>%
  left_join(study_survey_map, by = c("Study" = "study")) %>%
  relocate(survey, .after = "Study")

# rename column names for consistency
# colnames(human_data) <- lapply(colnames(human_data), tolower)

human_data


```



## Original DRI analysis
```{r}

dri_calc <- function(data, v1, v2) {
  lambda <- 1 - (sqrt(2) / 2)
  dri <- 2 * (((1 - mean(abs((data[[v1]] - data[[v2]]) / sqrt(2)
  ))) - (lambda)) / (1 - (lambda))) - 1
  
  return(dri)
}

get_IC <- function(data, survey, case) {
  
  # loop through analysis stages (pre/post)
  for (stage in 1:max(data$StageID)) {
    
    # select specific data to analyse
    data_stage <- data %>% filter(StageID == stage)
    
    # make sure there's data to analyze
    if (nrow(data_stage) > 0) {
      # get participant numbers/ids
      PNums <- data_stage$PNum
      
      # variables for reading COLUMN data
      # Q is a list considerations (Likert scale)
      # - there are up to 50 questions
      # R is a list ratings (rankings)
      Q <- data_stage %>% select(C1:C50)
      R <- data_stage %>% select(P1:P10)
      
      # remove all NA columns (in case there are less than 50
      # consideration questions
      Q <- Q[, colSums(is.na(Q)) != nrow(Q)]
      R <- R[, colSums(is.na(R)) != nrow(R)]
      
      # transpose data
      Q <- t(Q)
      R <- t(R)
      
      # format data as data frame
      Q <- as.data.frame(Q)
      R <- as.data.frame(R)
      
      # name columns with participant numbers
      colnames(Q) <- PNums
      colnames(R) <- PNums
      
      # obtain a list of correlations without duplicates
      # cor() returns a correlation matrix between Var1 and Var2
      # Var1 and Var2 are the variables being correlated
      # Freq is the correlation
      QWrite <- subset(as.data.frame(as.table(cor(Q, method = "spearman"))),
                       match(Var1, names(Q)) > match(Var2, names(Q)))
      
      RWrite <- subset(as.data.frame(as.table(cor(R, method = "spearman"))),
                       match(Var1, names(R)) > match(Var2, names(R)))
      
      # initialize the output in the first iteration
      if (stage == 1) {
        IC <- data.frame("P_P" = paste0(QWrite$Var1, '-', QWrite$Var2))
        IC$P1 <- as.numeric(as.character(QWrite$Var1))
        IC$P2 <- as.numeric(as.character(QWrite$Var2))
      }
      
      # prepare QWrite
      QWrite <- as.data.frame(QWrite$Freq)
      names(QWrite) <- paste0("Q", stage)
      
      # prepare RWrite for merge
      RWrite <- as.data.frame(RWrite$Freq)
      names(RWrite) <- paste0('R', stage)
      
      # merge
      IC <- cbind(IC, QWrite, RWrite)
    }
    
  }
  
  # append case & study info
  IC$survey <- survey
  IC$case <- case
  
  ## IC Points calculations ##
  IC$IC_PRE <- 1 - abs((IC$R1 - IC$Q1) / sqrt(2))
  IC$IC_POST <- 1 - abs((IC$R2 - IC$Q2) / sqrt(2))
  
  return(IC)
}

get_ind_DRI <- function(IC) {
  
  Plist <- unique(c(IC$P1, IC$P2))
  
  Plist <- Plist[order(Plist)]
  
  DRIInd <- data.frame('participant' = Plist)
  DRIInd$survey <- survey
  DRIInd$case <- data_case_study$Case[1]
  
  DRIInd <- DRIInd[c("survey", "case", "participant")]
  
  #Add individual-level metrics
  for (i in 1:length(Plist)) {
    DRIInd$DRIPre[i] <- dri_calc(
      data = IC  %>% filter(P1 == Plist[i] | P2 == Plist[i]),
      v1 = 'R1',
      v2 = 'Q1'
    )
    DRIInd$DRIPost[i] <- dri_calc(
      data = IC  %>% filter(P1 == Plist[i] | P2 == Plist[i]),
      v1 = 'R2',
      v2 = 'Q2'
    )
  }
  
  return(DRIInd)
  
}

get_case_DRI <- function(IC, type="human_only") {
   
  ## Group DRI level ##
  DRI_PRE <- dri_calc(data = IC, v1 = 'R1', v2 = 'Q1')
  DRI_POST <- dri_calc(data = IC, v1 = 'R2', v2 = 'Q2')
  
  #CaseDRI Dataframe
  DRI.Case <- data.frame(
    survey = survey,
    case = case,
    type = type,
    DRI_PRE,
    DRI_POST
  )
  
  #Tests for groups
  DRIOverallSig <- wilcox.test(IC$IC_POST,
                               IC$IC_PRE,
                               paired = TRUE,
                               alternative = "greater")
  DRIOverallSig_twoside <- wilcox.test(IC$IC_POST,
                                       IC$IC_PRE,
                                       paired = TRUE,
                                       alternative = "two.side")
  #cumdist_pre_post <- cvm_test(IC$IC_PRE, IC$IC_POST, nboots = 1000)not necessary
  
  #Adding the results to case data
  DRI.Case$DRI_one_tailed_p <- DRIOverallSig$p.value
  DRI.Case$DRI_twoside_p <- DRIOverallSig_twoside$p.value
  
  return(DRI.Case)
  
}

mini_publics <- human_data %>%
  group_by(survey, Case) %>%
  summarise(.groups = "drop")

get_llm_data <- function(provider, model, survey) {
  llm_participant <- llm_data_aggregated %>%
    filter(provider == !!provider, model == !!model, survey == !!survey)
  return(llm_participant)
}

get_ind_LLM_DRI <- function(data, provider, model) {
  
  llm_DRI <- data %>%
    filter(participant == 0) %>%
    select(-participant) %>%
    mutate(provider = !!provider, model = !!model) %>%
    relocate(provider, model, .before = 1)
  
  return(llm_DRI)
}

add_llm_participant <- function(data, provider, model, survey) {
  
  # print(paste("adding", paste(provider, model, survey, sep = "/"), "to human data."))
  
  # get llm data
  llm_participant <- llm_data_aggregated %>%
    filter(provider == !!provider, model == !!model, survey == !!survey)
  
  # check if it exists
  if (nrow(llm_participant) == 0) {
    warn(paste("No human participant found for", paste(provider, model, survey, sep = "/")))
  }
  
  # create 2 participants, PRE and POST
  llm_participants <- bind_rows(llm_participant, llm_participant)
  llm_participants$PNum <- 0 # PNum = 0 is LLM
    llm_participants$StageID <- c(1,2)
  
  data_with_llm <- bind_rows(data, llm_participants)
  
  return(data_with_llm)
  
}


DRIInd.LLMs <- list()

# for each study [1:N], N = 26
for (case_study in 1:nrow(mini_publics)) {

  # select study data
  survey <- mini_publics[case_study, ]$survey
  case <- mini_publics[case_study, ]$Case
  
  # get human data for this case study
  data_case_study <- human_data %>% filter(survey == !!survey &
                                             Case == !!case)
  
  # intersubject correlations (IC)
  IC <- get_IC(data_case_study, survey, case)

  ## GROUP DRI ##
  DRI.Case <- get_case_DRI(IC)
 
  ## INDIVIDUAL DRI ##
  DRIInd <- get_ind_DRI(IC)
  
  # get human average
  # NOTE: this should be the same as human_only group DRI
  human_ind_DRI_mean <- tibble(
    DRIPre = mean(DRIInd$DRIPre),
    DRIPost = mean(DRIInd$DRIPost)
  )
  
  # Global dataframes for depositing results
  # initialize *.Global
  if (case_study == 1) {
    IC.Global <- IC
    DRIInd.Global <- DRIInd
    DRI.Global <- DRI.Case
  }
  
  # append to *.Global
  else {
    IC.Global <- rbind(IC.Global, IC)
    DRIInd.Global <- rbind(DRIInd.Global, DRIInd)
    DRI.Global <- rbind(DRI.Global, DRI.Case)
  }
  
  # check if there are LLM data for this survey
  llms <- llm_surveys %>% filter(survey == !!survey)
  if (nrow(llms) == 0) {
    next
  }
  
  # TODO: skip problematic surveys for now
  if (survey == "zh_winterthur") {
    next
  }

  for (llm in 1:nrow(llms)) {
    
    provider <- llms[llm,]$provider
    model <- llms[llm,]$model
    type <- paste0("human+",paste(provider, model, sep = "/"))
    
    data_with_llm <- add_llm_participant(data_case_study, provider, model, survey)
    
    IC.LLM <- get_IC(data_with_llm, survey, case)
    DRI.Case.LLM <- get_case_DRI(IC.LLM, type)
    DRIInd.LLM <- get_ind_DRI(IC.LLM)
    DRIInd.LLM.Model <- get_ind_LLM_DRI(DRIInd.LLM, provider, model)
    
    DRIInd.LLM.Model$human_only_DRIPre_mean <- human_ind_DRI_mean$DRIPre
    DRIInd.LLM.Model$human_only_DRIPost_mean <- human_ind_DRI_mean$DRIPost
    
    get_bm_index <- function(diff) {
      bm_index <- (diff + 2) / 4
      return(bm_index)
    }
    
    DRIInd.LLM.Model <- DRIInd.LLM.Model %>%
      mutate(DRIPre_diff = DRIPre - human_only_DRIPre_mean,
             DRIPost_diff = DRIPost - human_only_DRIPost_mean) %>%
      
      # benchmark index = use DRIPost & normalize it to be >= 0
      mutate(bm_index = get_bm_index(DRIPost_diff))
      
    
    DRIInd.LLMs[[length(DRIInd.LLMs) + 1]] <- DRIInd.LLM.Model
    
    DRI.Global <- rbind(DRI.Global, DRI.Case.LLM)

  }
  
} # end for each case study


DRIInd.LLMs <- bind_rows(DRIInd.LLMs)

# add delta column
DRI.Global <- DRI.Global %>%
  mutate(DRI_DELTA = DRI_POST - DRI_PRE)

# write summary to file
write_csv(DRIInd.LLMs, paste(OUTPUT_DIR, "DRIInd_LLMs.csv", sep = "/"))
write_csv(DRI.Global, paste(OUTPUT_DIR, "DRI_global.csv", sep = "/"))


models %>%
  group_by(provider) %>%
  summarize()


```

```{r benchmark index}

DRI_benchmark <- DRIInd.LLMs %>%
  group_by(provider, model) %>%
  summarise(N = n(), 
            agg_bm_index = mean(bm_index)) %>%
  arrange(desc(agg_bm_index))

llm_sd <- DRIInd.LLMs %>%
  group_by(survey, case) %>%
  summarise(N = n(), 
            llm_sd = sd(bm_index)) %>%
  arrange(desc(llm_sd))

write_csv(llm_sd, paste(OUTPUT_DIR, "llm_sd.csv", sep = "/"))

DRI_benchmark %>%
  mutate(label = paste(provider, model, sep="/")) %>%
  ggplot(aes(x = reorder(label, desc(agg_bm_index)), y = agg_bm_index)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(agg_bm_index, 3)), vjust = -0.3) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.background = element_rect(fill = "white")) + scale_y_continuous(limits = c(0, 1)) +
  labs(x = "", y = "DRI benchmark") -> plot

ggsave(paste(OUTPUT_DIR, "benchmark.png", sep = "/"), plot, width = 10, height = 6)



```

