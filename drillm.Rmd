---
title: 'Triage Against the Machine: Can AI Reason Deliberatively?'
author: "Francesco Veri, Gustavo Umbelino"
date: "`r Sys.Date()`"
bibliography: bibliography/refs.bib
link-citations: true
csl: bibliography/apsa.csl
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(knitr)
library(reticulate)
library(tidyverse)
library(scales)
library(readxl)
library(rlang)
library(psych)
library(boot)
library(vote) # might need to manually install: https://cran.r-project.org/doc/manuals/r-release/R-admin.html#Fortran-compiler

SURVEY_FILE <- "data/surveys_v5.xlsx"
EXEC_LOG_FILE <- "llm_data/exec_log.csv"
LLMS_FILE <- "private/llms_v3.csv"
HUMAN_DATA_FILE <- "data/total dataset_clean.csv"
EXPENSES_FILE <- "billing/transaction_history.xlsx"
OUTPUT_DIR <- "analysis"

ALPHA_RESULTS_FILE <- "alpha_results.csv"
AGGREGATION_RESULTS_FILE <- "llm_data_aggregated.csv"

MAX_ITERATIONS <- 30


# create output directory if it doesn't exist
if (!dir.exists(OUTPUT_DIR)) {
  dir.create(OUTPUT_DIR, recursive = TRUE)
}

```

```{r helper functions}

create_file_path <- function(provider, model, survey, file_type) {
  file.path("llm_data", provider, model, survey, paste0(file_type, ".csv"))
}

output.file.exists <- function(file_name) {
  file.exists(paste(OUTPUT_DIR, file_name, sep = "/"))
}

read_output_csv <- function(file_name) {
  read_csv(paste(OUTPUT_DIR, file_name, sep = "/"), show_col_types = FALSE)
}

write_output_csv <- function(data, file_name) {
  write_csv(data, paste(OUTPUT_DIR, file_name, sep = "/"))
}

now_utc <- function() {
   now <- Sys.time()
   attr(now, "tzone") <- "UTC"
   now
}

```

# Large-Language Models (LLMs) Preview

```{r models}

# get models info
models <- read_csv(LLMS_FILE, show_col_types = FALSE)

models %>%
  select(provider, model, type, version) %>% 
  arrange(provider, model) %>%
  kable(caption = "LLMs", row.names = TRUE)

```

We started the analysis with `r nrow(models)` models, but some models were dropped after data collection. The models and reason for dropping are discussed later on [Excluded Models].

# Surveys

```{r surveys}

# read the sheet names of the Excel file
survey_names <- excel_sheets(SURVEY_FILE)

# remove invalid and "template" 
survey_names <- sort(survey_names[!grepl("^~", survey_names) & survey_names != "template"])

get_surveys <- function(survey_names) {
  
  surveys <- list()
  
  # Iterate over each sheet in the workbook
  for (survey_name in survey_names) {

    # Read the current sheet into a data frame
    df <- read_excel(SURVEY_FILE, sheet = survey_name)
    
    # Check if required columns exist
    required_columns <- c("considerations", "policies", "scale_max", "q-method")
    missing_cols <- setdiff(required_columns, colnames(df))
    if (length(missing_cols) > 0) {
      cat(
        "Sheet",
        survey_name,
        "is missing the following columns:",
        paste(missing_cols, collapse = ", "),
        "\n\n"
      )
      next
    }
    
    # Calculate the number of non-NA rows in "considerations" column
    n_c <- sum(!is.na(df$considerations))
    
    # Calculate the number of non-NA rows in "policies" column
    n_p <- sum(!is.na(df$policies))
    
    # Extract integer values from "scale_max" column, assuming they are already integers
    scale_max <- as.integer(na.omit(df$scale_max))
    
    # Extract logical (boolean) values from "q-method" column
    q_method <- as.logical(na.omit(df$`q-method`))
    
    surveys[[length(surveys) + 1]] <- tibble(
      survey = survey_name,
      considerations = n_c,
      policies = n_p,
      scale_max,
      q_method,
    )
    
  }
  
  surveys <- bind_rows(surveys)
  surveys
  
}


surveys <- get_surveys(survey_names)


# define file types
file_types <- c("considerations", "policies", "reasons")

surveys %>% arrange(survey) %>%
  kable(caption = "Surveys", row.names = TRUE)

```

# LLM Data Collection

```{r format LLM data}

get_llm_data <- function() {
  
  # initialize an empty list to store the data frames
  data_list <- list()
  
  # iterate over each survey
  for (survey_name in survey_names) {
    
    # iterate over each row in the models data frame
    for (i in 1:nrow(models)) {
      
      provider <- models$provider[i]
      model <- models$model[i]
      min_iterations <- models$min_iterations[i]
      
      # check if any file for the survey exists
      survey_path <- paste0("llm_data/", provider, "/", model, "/", survey_name, "/")
      if (!any(file.exists(paste0(survey_path, file_types, ".csv")))) {
        next
      }
      
      # iterate over each file type
      for (file_type in file_types) {
        # create the file path
        file_path <- create_file_path(provider, model, survey_name, file_type)
        
        # check if the file exists
        if (!file.exists(file_path)) {
          break
        }
        
        # read the CSV file
        temp_data <- read_csv(file_path, show_col_types = FALSE)
        
        # skip file if file exists but has no data
        if (nrow(temp_data) == 0) {
          break
        }
        
        # select the relevant columns based on file type
        if (file_type == "considerations") {
          # initialize survey_data
          survey_data <- temp_data %>%
            rename_with(~ paste0("C", seq_along(.)),
                        starts_with("C", ignore.case = FALSE))
          
          # add column "survey" to meta data
          survey_data <- survey_data %>%
            mutate(survey = survey_name) %>%
            relocate(survey, .after = model)
          
          # ensure survey_data has columns up to C50
          # skip 8 rows of meta data
          for (j in (ncol(survey_data) - 7):50) {
            survey_data[[paste0("C", j)]] <- as.numeric(NA)
          }
          
          # go to next file type
          next
          
        } else if (file_type == "policies") {
          temp_data <- temp_data %>%
            select(cuid, starts_with("P", ignore.case = FALSE)) %>%
            rename_with(~ paste0("P", seq_along(.)),
                        starts_with("P", ignore.case = FALSE))
          
          # ensure temp_data has columns up to P10
          for (j in (ncol(temp_data)):10) {
            temp_data[[paste0("P", j)]] <- as.numeric(NA)
          }
          
        } else if (file_type == "reasons") {
          temp_data <- temp_data %>%
            select(cuid, reason) %>%
            rename(R = reason)
        }
        
        # merge the data frames by 'cuid' and keep all rows
        survey_data <- full_join(survey_data, temp_data, by = c("cuid"))
        
      }
      
      # add the survey_data to the list
      if (exists("survey_data")) {
        data_list[[length(data_list) + 1]] <- survey_data
        
        # remove the survey_data data frame to free up memory
        rm(survey_data)
      }
    }
  }
  
  # Combine all data frames in the list into a single data frame
  llm_data <- bind_rows(data_list)
  
  return(llm_data)
  
}

# get llm data
llm_data <- get_llm_data()

# aggregate llm_data by provider, model, and survey and N the number of rows
llm_surveys <- llm_data %>%
  group_by(provider, model, survey) %>%
  summarise(
    N = n(),
    mean_input_tokens = as.integer(mean(input_tokens)),
    mean_output_tokens = as.integer(mean(output_tokens)),
    .groups = 'drop'
  )

models <- llm_surveys %>%
  group_by(provider, model) %>%
  summarise(
    surveys_with_data = n(),
    min_iterations_completed = min(N, na.rm = TRUE),
    max_iterations_completed = max(N, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  full_join(models, by = c("provider", "model")) %>%
  mutate(done = (min_iterations <= min_iterations_completed) &
           (surveys_with_data == length(survey_names)))

models["done"][is.na(models["done"])] <- FALSE

# write resutls to file
write_csv(llm_data, paste(OUTPUT_DIR, "llm_data.csv", sep = "/"))
write_csv(llm_surveys, paste(OUTPUT_DIR, "llm_surveys.csv", sep = "/"))

```

We collected a total of `r nrow(llm_data)` valid LLM responses across `r nrow(surveys)` surveys.

## Cost

```{r cost analysis, results = 'asis'}

# calculate costs in tokens
cost_tokens <- llm_data %>%
  group_by(provider, model) %>%
  summarise(
    total_iterations_completed = n(),
    total_input_tokens = as.integer(sum(input_tokens)),
    total_output_tokens = as.integer(sum(output_tokens)),
    input_output_ratio = total_input_tokens/total_output_tokens,
    .groups = 'drop'
  )

models <- full_join(models, cost_tokens, by = c("provider", "model")) %>%
  mutate(
    cost_input = (total_input_tokens / 1000000) * price_1M_input,
    cost_output = (total_output_tokens / 1000000) * price_1M_output,
    total_cost = cost_input + cost_output
  )


api_costs <- read_excel(EXPENSES_FILE)

api_costs <- api_costs %>%
  group_by(api) %>%
  summarise(
    credits_paid = sum(as.numeric(credits), na.rm = TRUE),
    total_cost = sum(as.numeric(paid), na.rm = TRUE),
    .groups = "drop"
  )

api_costs <- models %>% 
  group_by(api) %>% 
  summarise(
    num_models = n(),
    credits_used = sum(total_cost, na.rm = TRUE),
    estimate = sum(total_estimate, na.rm = TRUE)) %>% 
  full_join(api_costs, by="api") %>%
  mutate(
    credits_left = credits_paid - credits_used,
  ) %>%
  select(
    api, 
    num_models,
    credits_paid, 
    credits_used, 
    credits_left, 
    total_cost, 
    estimate) %>%
  arrange(desc(credits_paid))

write_csv(api_costs, paste(OUTPUT_DIR, "api_costs.csv", sep = "/"))

cat("We spent a total of", sum(api_costs$credits_paid, na.rm = TRUE), "USD. The cost breakdown per API is below.")

api_costs %>%
  filter(!is.na(api)) %>%
  select(api, num_models, credits_paid) %>%
  arrange(desc(credits_paid)) %>%
  kable(caption = "Costs by API")

```

## Time

```{r execution analysis, results = 'asis'}

# read exec log
# NOTE: not all executions were logged due to technical issues
# so the number of completions in this log will not add up to those in llm_data
exec_log <- read_csv(EXEC_LOG_FILE, show_col_types = FALSE)

# fill missing columns
exec_log <- exec_log %>%
  arrange(model) %>%
  mutate(
    fixed_num_errors = (`num surveys` * `num iterations`) - (`num fail completions` + `num success completions`)
  ) %>%
  relocate(fixed_num_errors, .before = `num errors`) %>%
  
  # remove trial columns 
  filter(is.na(`template success rate (%)`)) %>%
  select(-`template success rate (%)`) %>%
  
  # remove models with no completion data
  filter(`num completions` > 0)
  

exec_models <- exec_log %>%
  group_by(provider, model) %>%
  summarise(
    num_exec = n(),
    total_cost_USD = sum(`total cost ($)`, na.rm = TRUE),
    total_time_min = sum(`total elapsed time (min)`, na.rm = TRUE),
    num_completions = sum(`num completions`, na.rm = TRUE),
    num_success = sum(`num success completions`, na.rm = TRUE),
    num_error = sum(fixed_num_errors, na.rm = TRUE),
    num_fail = sum(`num fail completions`, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    fail_rate = num_fail / num_completions,
    success_rate = num_success / num_completions,
    cost_per_completion = total_cost_USD / num_completions,
    cost_per_success = total_cost_USD / num_success,
    time_per_completion_s = total_time_min / num_completions * 60
  )

models <- full_join(models, exec_models, by=c("provider", "model"))

exec_models_surveys <- exec_log %>%
  pivot_longer(19:38, names_to = "survey", values_to = "success_rate", values_drop_na = TRUE) %>%
  mutate(
    success_iterations = round(success_rate / 100 * `num iterations`),
    survey = str_extract(survey, "([^ ]+)\\s*") %>%
           trimws()

  ) %>%
  group_by(provider, model, survey) %>%
  summarise(
    num_execs = n(),
    num_success = sum(success_iterations, na.rm = TRUE),
    num_iterations = sum(`num iterations`, na.rm = TRUE),
    success_rate = num_success / num_iterations,
    .groups = "drop"
  )

exec_surveys <- exec_models_surveys %>%
  group_by(survey) %>%
  summarise(
    num_execs = sum(num_execs),
    num_iterations = sum(num_iterations),
    num_success = sum(num_success),
    success_rate = num_success/num_iterations,
    .groups = "drop"
  )

exec_total <- models %>%
  summarise(
    hours = sum(total_time_min, na.rm = TRUE) / 60,
    cost_USD = sum(total_cost, na.rm = TRUE),
    .groups = "drop"
  )

first_date <- min(llm_data$created_at)
last_date <- max(llm_data$created_at)

data_collection_days <- as.numeric(round(difftime(last_date, first_date, units = c("days"))))

cat("It took a total of", round(exec_total$hours), "hours^[Execution data is mostly accurate. Only a few (3-5) executions failed and, as a result, we have no record of it.] across", data_collection_days, "days to complete data collection. Most of it was done in parallel. The first LLM response was collected on", format(min(llm_data$created_at), "%A, %b %d, %Y"), "and latest on", format(max(llm_data$created_at), "%A, %b %d, %Y.\n\n"))

exec_total <- as.data.frame(t(exec_total))
colnames(exec_total) <- c("measure")
exec_total <- rownames_to_column(exec_total, "metric")

# write summary to file
write_csv(exec_models, paste(OUTPUT_DIR, "exec_models.csv", sep = "/"))
write_csv(exec_surveys, paste(OUTPUT_DIR, "exec_surveys.csv", sep = "/"))
write_csv(exec_models_surveys, paste(OUTPUT_DIR, "exec_models_surveys.csv", sep = "/"))
write_csv(exec_total, paste(OUTPUT_DIR, "exec_total.csv", sep = "/"))


llm_data %>%
  mutate(date = as.Date(created_at)) %>%
  group_by(date) %>%
  summarise(count = n()) %>%
  arrange(date) %>%
  mutate(cumulative_count = cumsum(count)) %>%
  ggplot(aes(x = date, y = cumulative_count)) +
  geom_line() +
  labs(x = "Date", y = "Cumulative LLM Data Collected") +
  theme_minimal() +
  scale_x_date(
    breaks = date_breaks("1 day"),
    labels = date_format("%b %d")
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```

```{r data left}

data_left <- models %>%
  filter(!done, included) %>%
  mutate(
    completions_total = min_iterations * length(survey_names),
    completions_left = completions_total - total_iterations_completed,
    time_left_min = (time_per_completion_s * completions_left) / 60,
    cost_left = cost_per_completion * completions_left
  ) %>% 
  select(
    provider, model,
    completions_total,
    completions_left,
    time_left_min,
    cost_left,
  )

write_csv(data_left, paste(OUTPUT_DIR, "data_left.csv", sep = "/"))

```

## Excluded Models

```{r model info, results = 'asis'}

included_models_summary <- models %>%
  filter(included, done) %>%
  select(
    provider,
    model,
    type,
    api,
    done,
    cost_per_completion,
    total_cost,
    fail_rate,
    time_per_completion_s,
    total_time_min,
    comment
  )

# only use included models for analysis
llm_data <- llm_data %>% filter(model %in% included_models_summary$model)

write_output_csv(included_models_summary, "models_summary.csv")

excluded_models <- models %>% filter(!included) %>% select(provider, model, reason)

cat("\n", nrow(excluded_models), "out of", nrow(models), "were excluded from the analysis for the following reasons.\n\n")

kable(excluded_models %>% arrange(provider, model), caption = "Excluded models and reasons")



```

## Execution Summary Plots

### Fail rate

```{r fail rate, fig.width=10,fig.height=6}


max_value <- max(included_models_summary$fail_rate)
buffer <- (max_value - min(included_models_summary$fail_rate)) * 0.1

included_models_summary %>%
  ggplot(aes(
    x = reorder(model, fail_rate),
    y = fail_rate,
    fill = provider,
  )) +
  geom_bar(stat = "identity") +
  coord_flip() +
  geom_text(aes(label = paste0(round(fail_rate * 100, 1), "%")),
            hjust = -0.1, 
            size = 3) +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "white"),
    legend.position = "bottom right",
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
  ) +
  labs(x = "Models", y = "Fail rate") +
  scale_y_continuous(
    expand = expansion(mult = c(0, 0)),
    breaks = seq(0, max_value + buffer, by = round((
      max_value + buffer
    ) / 10, 2)),
    limits = c(0, max_value + buffer),
  ) -> plot

ggsave(
  paste(OUTPUT_DIR, "plots/fail_rate.png", sep = "/"),
  plot,
  width = 10,
  height = 6
)

plot

```

### Cost per completion

```{r cost-completion, fig.width=10,fig.height=6}


max_value <- max(included_models_summary$cost_per_completion)
buffer <- (max_value - min(included_models_summary$cost_per_completion)) * 0.1

included_models_summary %>%
  ggplot(aes(
    x = reorder(model, cost_per_completion),
    y = cost_per_completion,
    fill = provider,
  )) +
  geom_bar(stat = "identity") +
  coord_flip() +
  geom_text(aes(label = round(cost_per_completion, 4)),
            hjust = -0.1, 
            size = 3) +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "white"),
    legend.position = "bottom right",
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
  ) +
  labs(x = "Models", y = "Cost per completion (USD)") +
  scale_y_continuous(
    expand = expansion(mult = c(0, 0.02)),
    breaks = seq(0, max_value + buffer, by = round((
      max_value + buffer
    ) / 10, 4)),
    limits = c(0, max_value + buffer),
  ) -> plot

ggsave(
  paste(OUTPUT_DIR, "plots/cost_per_completion.png", sep = "/"),
  plot,
  width = 10,
  height = 6
)

plot

```

### Total cost

```{r total cost, fig.width=10,fig.height=6}


max_value <- max(included_models_summary$total_cost)
buffer <- (max_value - min(included_models_summary$total_cost)) * 0.1

included_models_summary %>%
  ggplot(aes(
    x = reorder(model, total_cost),
    y = total_cost,
    fill = provider,
  )) +
  geom_bar(stat = "identity") +
  coord_flip() +
  geom_text(aes(label = round(total_cost, 2)),
            hjust = -0.1, 
            size = 3) +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "white"),
    legend.position = "bottom right",
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
  ) +
  labs(x = "Models", y = "Total cost (USD)") +
  scale_y_continuous(
    expand = expansion(mult = c(0, 0.02)),
    breaks = seq(0, max_value + buffer, by = round((
      max_value + buffer
    ) / 10)),
    limits = c(0, max_value + buffer),
  ) -> plot

ggsave(
  paste(OUTPUT_DIR, "plots/total_cost.png", sep = "/"),
  plot,
  width = 10,
  height = 6
)

plot

```

### Time per completion

```{r time-completion, fig.width=10,fig.height=6}

max_value <- max(included_models_summary$time_per_completion_s)
buffer <- (max_value - min(included_models_summary$time_per_completion_s)) * 0.1

included_models_summary %>%
  ggplot(aes(
    x = reorder(model, time_per_completion_s),
    y = time_per_completion_s,
    fill = provider,
  )) +
  geom_bar(stat = "identity") +
  coord_flip() +
  geom_text(aes(label = round(time_per_completion_s, 2)),
            hjust = -0.1, 
            size = 3) +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "white"),
    legend.position = "bottom right",
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
  ) +
  labs(x = "Models", y = "Time per completion (s)") +
  scale_y_continuous(
    expand = expansion(mult = c(0, 0.02)),
    breaks = seq(0, max_value + buffer, by = round((
      max_value + buffer
    ) / 10)),
    limits = c(0, max_value + buffer),
  ) -> plot

ggsave(
  paste(OUTPUT_DIR, "plots/time_per_completion_s.png", sep = "/"),
  plot,
  width = 10,
  height = 6
)

plot

```

### Cost/Time per completion

```{r cost-time-completion, fig.width=10,fig.height=6, results = 'asis'}

included_models_summary %>%
  group_by(provider, model, .groups = "drop") %>%
  ggplot(aes(x = time_per_completion_s, y = cost_per_completion, color = provider)) +
  geom_point(size = 5) + # Adjust size as needed
  geom_text(aes(label = model), vjust = -2, size = 2) + # Add labels above each dot
  labs(x = "Time per completion (s)", y = "Cost per completion (USD)", color = "Provider") +
  theme_minimal() +
  theme(plot.background = element_rect(fill = "white"),
        legend.position = "none") -> plot 

plot 

cat("\n\nZoomed in to cost < 0.01 USD and time < 12 s.\n\n")

included_models_summary %>%
  filter(cost_per_completion < 0.01 & time_per_completion_s < 12) %>%
  group_by(provider, model, .groups = "drop") %>%
  ggplot(aes(x = time_per_completion_s, y = cost_per_completion, color = provider)) +
  geom_point(size = 5) + # Adjust size as needed
  geom_text(aes(label = model), vjust = -2, size = 2) + # Add labels above each dot
  labs(x = "Time per completion (s)", y = "Cost per completion (USD)", color = "Provider") +
  theme_minimal() +
  theme(plot.background = element_rect(fill = "white"),
        legend.position = "none") -> plot 

plot 
```

## Internal Consistency of Responses

We calculate Cronbach's Alpha from the top `r MAX_ITERATIONS` iterations.

```{r error=FALSE, warning=FALSE, message=FALSE}

# Initialize an empty list to store the alpha results
alpha_results <- list()

models_with_data <- llm_data %>%
  distinct(provider, model) 

if (output.file.exists(ALPHA_RESULTS_FILE)) {
  last_alpha_results <- read_output_csv(ALPHA_RESULTS_FILE)
}

# Iterate over each unique provider/model combination
for (row in 1:nrow(models_with_data)) {
  provider <- models_with_data[row, ]$provider
  model <- models_with_data[row, ]$model
  
  # filter the data for the current provider/model
  provider_model_data <- llm_data %>%
    filter(model == !!model)
  
  # iterate over each survey
  for (survey_name in unique(provider_model_data$survey)) {
    
    # filter the data for the current survey
    survey_data <- provider_model_data %>%
      filter(survey == !!survey_name) %>%
      
      # get only first x iterations, where x = MAX_ITERATIONS
      arrange(created_at) %>%
      head(MAX_ITERATIONS)
    
    # SKIP THIS ITERATION IF PREVIOUS RECORD EXISTS
    # get date of the most data generation
    last_updated <- max(survey_data$created_at, na.rm = TRUE)
    
    # if there is a previous record
    if (exists("last_alpha_results")) {
      last_alpha <- last_alpha_results %>% filter(model == !!model, survey == !!survey_name)
      
      ## and last record is still valid, save it and skip
      ## valid records are more recent than the latest data generated
      if (nrow(last_alpha) == 1 && (last_alpha$created_at > last_updated)) {
        alpha_results[[length(alpha_results) + 1]] <- last_alpha
        next
      } else {
        cat("updating:", model, "/", survey_name, "\n")
      }
    }
    
    # Calculate Cronbach's Alpha for considerations (C1..C50)
    considerations_data <- survey_data %>% select(C1:C50)
    
    if (nrow(considerations_data) > 1) {
      
      # Check if policies are all equal (no variance)
      # this can happen when there are few iterations
      c_all_equal <- all(apply(considerations_data, 1, function(row)
        all(row == considerations_data[1, ], na.rm = TRUE)), na.rm = TRUE)
      
      # TODO: FIXME! 
      # NOTE: assign alpha = 1, which should NOT exist!
      # if (c_all_equal) {
      #   alpha_considerations <- 1
      # } else {
        alpha_considerations <- psych::alpha(
          considerations_data,
          check.keys = TRUE,
          warnings = FALSE,
        )$total$raw_alpha
      # }
    } else {
      alpha_considerations <- NA
    }
    
    # Calculate Cronbach's Alpha for policies (P1..P10)
    policies_data <- survey_data %>% select(P1:P10)
    
    if (nrow(policies_data) > 1) {
      
      # Check if policies are all equal (no variance)
      # this can happen when there are few iterations
      p_all_equal <- all(apply(policies_data, 1, function(row)
        all(row == policies_data[1, ], na.rm = TRUE)), na.rm = TRUE)
      
      # NOTE: assign alpha = 1, which should NOT exist!
      if (p_all_equal) {
        alpha_policies <- 1
      }

      # normal case, calculate alpha
      else {
        alpha_policies <- psych::alpha(
          policies_data,
          check.keys = TRUE,
          warnings = FALSE,
        )$total$raw_alpha
      }
    } else {
      alpha_policies <- NA
    }
    
    if (nrow(policies_data) > 1 && nrow(considerations_data) > 1) {
      all_data <- cbind(considerations_data, policies_data)
      alpha_all <- psych::alpha(
        all_data,
        check.keys = TRUE,
        warnings = FALSE,
      )$total$raw_alpha
    } else {
      alpha_all <- NA
    }
    
    
    # Store the results in the list
    alpha_results[[length(alpha_results) + 1]] <- tibble(
      provider = provider,
      model = model,
      survey = survey_name,
      N = nrow(considerations_data),
      created_at = now_utc(),
      alpha_considerations = alpha_considerations,
      alpha_policies = alpha_policies,
      alpha_all = alpha_all
    )
  }
}

# Combine all results into a single data frame
alpha_results <- bind_rows(alpha_results)

rm(models_with_data)
rm(considerations_data)
rm(survey_data)
rm(policies_data)
rm(provider_model_data)

# write summary to file
write_csv(alpha_results, paste(OUTPUT_DIR, "alpha_results.csv", sep = "/"))

```

### Check alpha results per model

```{r}

# Aggregate alpha_results by model and calculate summary statistics
alpha_summary <- alpha_results %>%
  filter(model %in% included_models_summary$model) %>%
  group_by(provider, model) %>%
  summarise(
    N = sum(N),
    mean_alpha_all = mean(alpha_all, na.rm = TRUE),
    min_alpha_considerations = min(alpha_considerations, na.rm = TRUE),
    max_alpha_considerations = max(alpha_considerations, na.rm = TRUE),
    mean_alpha_considerations = mean(alpha_considerations, na.rm = TRUE),
    std_alpha_considerations = sd(alpha_considerations, na.rm = TRUE),
    min_alpha_policies = min(alpha_policies, na.rm = TRUE),
    max_alpha_policies = max(alpha_policies, na.rm = TRUE),
    mean_alpha_policies = mean(alpha_policies, na.rm = TRUE),
    std_alpha_policies = sd(alpha_policies, na.rm = TRUE),
    .groups = "drop"
  )


alpha_summary %>%
  mutate(considerations = round(mean_alpha_considerations,2),
         policies = round(mean_alpha_policies,2),
         all = round(mean_alpha_all, 2)) %>%
  select(provider, model, N, all, considerations, policies) %>%
  arrange(all) %>%
  kable(caption = "Alpha summary across models, mean across surveys", row.names = TRUE)

```

# Aggregation

We then aggregated LLM data into 1 response per model/survey. Based on [@motoki_more_2024], we bootstrap considerations 1000 times.

```{r aggregation functions}

# function to calculate mode of data, same as stat_function
calc_mode <- function(data) {
  as.numeric(names(sort(table(data), decreasing = TRUE)[1]))
}

# function to bootstrap mode
bootstrap_mode <- function(data, n_bootstrap = 1000) {
  
  # return NA if data contains any NA
  if (any(is.na(data))) {
    return(NA)
  }
  
  # define the statistic function for bootstrapping to find mode
  stat_function <- function(data, indices) {
    as.numeric(names(sort(table(data[indices]), decreasing = TRUE)[1]))
  }
  
  # perform bootstrap
  results <- boot(data = data,
                  statistic = stat_function,
                  R = n_bootstrap)
  
  # calculate bootstrapped mode
  b_mode <- calc_mode(results$t)
  
  # return the bootstrapped modes
  return(b_mode)
}


aggregate_llm_considerations <- function(considerations) {
  
  # ensure there are at least 2 rows to aggregate
  if (nrow(considerations) < 2) {
    return(considerations)
  }
  
  # Calculate the mode for each column
  mode_considerations <- considerations %>%
    summarise(across(everything(), bootstrap_mode))
  
  return(mode_considerations)
  
}

aggregate_llm_policies <- function(policies) {
  
  # ensure there are at least 2 rows to aggregate
  if (nrow(policies) < 2) {
    return(policies)
  }
  
  # Remove columns with NAs
  valid_policies <- policies[, colSums(is.na(policies)) != nrow(policies)]
  
  # Convert the policies to a ranked matrix
  ranked_matrix <- as.matrix(valid_policies)
  
  # Define the number of winners to all - 1 policies
  # stv complains if winners == all policies
  num_winners <- ncol(valid_policies) - 1
  
  # Run the Single Transferable Vote algorithm
  results <- stv(ranked_matrix, num_winners, quiet = TRUE)
  
  # add last policy to ranked result
  last_policy <- setdiff(colnames(valid_policies), results$elected)
  ranked_policies <- c(results$elected, last_policy)
  
  policy_order <- colnames(valid_policies)
  
  order <- match(policy_order, ranked_policies)
  
  # calculate the number of missing values needed to reach length 10
  missing_columns <- ncol(policies) - length(order)
  
  # fill in the missing values with NA
  order <- c(order, rep(NA, missing_columns))
  
  # create a new data frame with aggregated results
  policy_ranks <- data.frame(t(order))
  colnames(policy_ranks) <- colnames(policies)
  
  return(policy_ranks)
}

```

## Aggregate considerations and preferences

```{r aggregate llm data}


aggregate_llm_data <- function(data) {
  
  # get last aggregation results, if it exists
  if (output.file.exists(AGGREGATION_RESULTS_FILE)) {
    last_aggr_results <- read_output_csv(AGGREGATION_RESULTS_FILE)
  }
  
  # initialize an empty list to store the alpha results
  aggr_results <- list()
  
  # iterate over each unique provider/model/survey combination
  for (row in 1:nrow(llm_surveys)) {
    provider <- llm_surveys[row, ]$provider
    model <- llm_surveys[row, ]$model
    survey <- llm_surveys[row, ]$survey
    N <- llm_surveys[row, ]$N
    
    # filter the data for the current survey
    survey_data <- data %>%
      filter(model == !!model, survey == !!survey) %>%
      
      # get only first x iterations, where x = MAX_ITERATIONS
      arrange(created_at) %>%
      head(MAX_ITERATIONS)
    
    # SKIP THIS ITERATION IF PREVIOUS RECORD EXISTS
    # get date of the most data generation
    last_updated <- max(survey_data$created_at, na.rm = TRUE)
    
    # if there is a previous record
    if (exists("last_aggr_results")) {
      last_aggr <- last_aggr_results %>% filter(model == !!model, survey == !!survey)
      
      ## and last record is still valid, save it and skip
      ## valid records are more recent than the latest data generated
      if (nrow(last_aggr) == 1 &&
          (last_aggr$created_at > last_updated)) {
        aggr_results[[length(aggr_results) + 1]] <- last_aggr
        next
      } else {
        cat("updating:", model, "/", survey, "\n")
      }
    }
    
    # aggregate considerations C1:C50
    considerations_data <- survey_data %>% select(C1:C50)
    aggregated_considerations <- aggregate_llm_considerations(considerations_data)
    
    # aggregate policies P1:P10
    policies_data <- survey_data %>% select(P1:P10)
    aggregated_policies <- aggregate_llm_policies(policies_data)
    
    # store the results in the list
    aggr_result <- tibble(
      provider = provider,
      model = model,
      survey = survey,
      N = N,
      created_at = now_utc(),
      aggregated_considerations,
      aggregated_policies,
    )
    
    aggr_results[[length(aggr_results) + 1]] <- aggr_result
    
  }
  
  # Combine all results into a single data frame
  aggr_results <- bind_rows(aggr_results)
  
  return(aggr_results)
  
}

time_start <- Sys.time()
llm_data_aggregated <- aggregate_llm_data(llm_data)
time_end <- Sys.time()
elapsed_time <- difftime(time_end, time_start, units = "auto")

# write summary to file
write_output_csv(llm_data_aggregated, AGGREGATION_RESULTS_FILE)

```
We aggregated `r nrow(llm_data)` LLM responses into `r nrow(llm_data_aggregated)` responses: 1 response per model per survey.


# Human Data

```{r human data}

# Import the CSV file into a data frame
human_data <- read_csv(HUMAN_DATA_FILE, show_col_types = FALSE)

# Rename columns to be consistent with LLM data
human_data <- human_data %>%
  rename_with( ~ sub("^U0|^U", "C", .), starts_with("U", ignore.case = FALSE)) %>%
  rename_with( ~ sub("^Pref", "P", .), starts_with("Pref", ignore.case = FALSE)) %>%
  filter(Study != "Sydney CC Adaptation")

# Read the mapping file
study_survey_map <- read_csv("data/study_survey_map.csv", show_col_types = FALSE)

# Add a new column 'Survey' to human_data by matching 'Study' with 'survey'
human_data <- human_data %>%
  left_join(study_survey_map, by = c("Study" = "study")) %>%
  relocate(survey, .after = "Study")




human_data %>%
  group_by(Case, survey) %>%
  summarise(
    participants = n() / 2,  # pre/post deliberation
    .groups = "drop"
  ) %>%
  arrange(survey) %>%
  kable(caption = "Number of participants in each case study", row.names = TRUE)
```

We collected `r nrow(human_data)` human responses across `r length(unique(human_data$Case))` case studies, including pre-post deliberation responses.

# Randomly Generated Data

```{r generate random}
 

get_random_data <- function(sheet_names, file_path=SURVEY_FILE) {
  
  rand <- list()
  
  # Iterate over each sheet in the workbook
  for (sheet_name in sheet_names) {
    #cat("Processing sheet:", sheet_name, "\n")
    
    # Read the current sheet into a data frame
    df <- read_excel(file_path, sheet = sheet_name)
    
    # Check if required columns exist
    required_columns <- c("considerations", "policies", "scale_max", "q-method")
    missing_cols <- setdiff(required_columns, colnames(df))
    if (length(missing_cols) > 0) {
      cat(
        "Sheet",
        sheet_name,
        "is missing the following columns:",
        paste(missing_cols, collapse = ", "),
        "\n\n"
      )
      next
    }
    
    # Calculate the number of non-NA rows in "considerations" column
    n_c <- sum(!is.na(df$considerations))
    
    # Calculate the number of non-NA rows in "policies" column
    n_p <- sum(!is.na(df$policies))
    
    # Extract integer values from "scale_max" column, assuming they are already integers
    scale_max <- as.integer(na.omit(df$scale_max))
    
    # Extract logical (boolean) values from "q-method" column
    q_method <- as.logical(na.omit(df$`q-method`))
    
    # Print the results for each sheet
    #cat("Sheet:", sheet_name, "\n")
    #cat("Number of considerations:", n_c, "\n")
    #cat("Number of policies:", n_p, "\n")
    #cat("Integer value from 'scale_max' column:", scale_max, "\n")
    #cat("Logical value from 'q-method' column:", q_method, "\n")
    
    
    ### Make random survey
    # Generate data for C1:C50 with Likert scale values [1, scale_max] randomly assigned
    if (q_method) {
      
      # get normally distributed data
      c_df <- data.frame(t(round(rnorm(n = n_c, mean = scale_max / 2, sd = scale_max / 4))))
      
      # replace values below lower and above upper bound
      c_df[c_df < 1] <- 1
      c_df[c_df > scale_max] <- scale_max
      
    } else {
      c_df <- data.frame(t(replicate(n_c, sample(1:scale_max, 1, replace = TRUE))))
    }
    
    # fill and rename columns
    c_df[1, (n_c + 1):50] <- NA
    colnames(c_df) <- paste0("C", 1:50)
    
    # Generate data for P1:P10 with random unique ranks 1 to n_p for each row
    p_df <- data.frame(t(apply(matrix(0, nrow = 1, ncol = n_p), 1, function(x)
      sample(1:n_p))))
    p_df[1, (n_p + 1):10] <- NA
    colnames(p_df) <- paste0("P", 1:10)
    
    # Validate data
    c_valid <- !(any(c_df < 1, na.rm = TRUE) || any(c_df > scale_max, na.rm = TRUE))
    p_valid <- !(any(duplicated(p_df[!is.na(p_df)])) ||
      any(p_df < 1, na.rm = TRUE) ||
      any(p_df > n_p, na.rm = TRUE))
  
    #cat("Valid considerations:", c_valid, "\n")
    #cat("Valid policies:", p_valid, "\n")
    
    if (!c_valid || !p_valid) {
      warn(paste("Random generation produced invalid data for", sheet_name))
    }
    
    # Combine the two datasets into one data frame
    dataset <- as.data.frame(cbind(c_df, p_df))
    dataset <- dataset %>%
      mutate(survey = sheet_name) %>%
      relocate(survey, .before = 1)
    
    rand[[length(rand) + 1]] <- dataset
    
    #cat(rep("-", 40), "\n")
    
    
  }
  
  rand <- bind_rows(rand)
  return(rand)
  
}

# Example usage
rand_data <- get_random_data(survey_names)


```

Then, we generated `r nrow(rand_data)` random reseponses for each survey. 

```{r q-method}

q_data <- data.frame(rand_data %>% filter(survey == "fremantle") %>% select(C1:C50) %>% t)

colnames(q_data) <- "C"

# Plot the histogram for the distribution of numbers in this row
q_data %>% ggplot(aes(x = C)) +
  geom_histogram(bins = 7, fill = "blue", color = "black") +
  labs(title = "Fremantle Random Distribution",
       x = "Number",
       y = "Frequency")

q_data <- data.frame(rand_data %>% filter(survey == "valsamoggia") %>% select(C1:C50) %>% t)

colnames(q_data) <- "C"

# Plot the histogram for the distribution of numbers in this row
q_data %>% ggplot(aes(x = C)) +
  geom_histogram(bins = 7, fill = "blue", color = "black") +
  labs(title = "Valsamoggia Random Distribution",
       x = "Number",
       y = "Frequency")

```


# DRI Analysis

We begin by defining DRI calculation functions.


```{r DRI functions, echo=TRUE}

# original DRI formula
dri_calc <- function(data, v1, v2) {
  lambda <- 1 - (sqrt(2) / 2)
  dri <- 2 * (((1 - mean(abs((data[[v1]] - data[[v2]]) / sqrt(2)
  ))) - (lambda)) / (1 - (lambda))) - 1
  
  return(dri)
}

# updated DRI formula
# FIXME: only accounts for negligible positive correlations, but not negative ones
dri_calc_v2 <- function(data, v1, v2) {
  # Calculate orthogonal distance for each pair
  d <- abs((data[[v1]] - data[[v2]]) / sqrt(2))
  
  # Define lambda as in the original
  lambda <- 1 - (sqrt(2) / 2)
  
  # Calculate penalty: 0.5 if both correlations are in [0, 0.2], 1 otherwise
  penalty <- ifelse(data[[v1]] >= 0 & data[[v1]] <= 0.2 & #0.3
                      data[[v2]] >= 0 & data[[v2]] <= 0.2, # 0.3
                    0, 1)
  
  # Adjusted consistency per pair
  consistency <- (1 - d) * penalty
  
  # Average consistency across all pairs
  avg_consistency <- mean(consistency)
  
  # Scale to [-1, 1] as in the original
  dri <- 2 * ((avg_consistency - lambda) / (1 - lambda)) - 1
  
  return(dri)
}

# updated DRI formula: penalizes both negligible positive and negative correlations in a scalar way.
dri_calc_v3 <- function(data, v1, v2){
  d <- abs((data[[v1]] - data[[v2]]) / sqrt(2))
  lambda <- 1 - (sqrt(2) / 2)
  
  # Scalar penalty based on strength of signal (|r| and |q|)
  penalty <- ifelse(pmax(abs(data[[v1]]), abs(data[[v2]])) <= 0.2,
                    pmax(abs(data[[v1]]), abs(data[[v2]])) / 0.2,
                    1)
  
  consistency <- (1 - d) * penalty
  avg_consistency <- mean(consistency)
  
  dri <- 2 * ((avg_consistency - lambda) / (1 - lambda)) - 1
  return(dri)
}


```

```{r DRI}

get_IC <- function(data, survey, case) {
  
  # loop through analysis stages (pre/post)
  for (stage in 1:max(data$StageID)) {
    
    # select specific data to analyse
    data_stage <- data %>% filter(StageID == stage)
    
    # make sure there's data to analyze
    if (nrow(data_stage) > 0) {
      # get participant numbers/ids
      PNums <- data_stage$PNum
      
      # variables for reading COLUMN data
      # Q is a list considerations (Likert scale)
      # - there are up to 50 questions
      # R is a list ratings (rankings)
      Q <- data_stage %>% select(C1:C50)
      R <- data_stage %>% select(P1:P10)
      
      # remove all NA columns (in case there are less than 50
      # consideration questions
      Q <- Q[, colSums(is.na(Q)) != nrow(Q)]
      R <- R[, colSums(is.na(R)) != nrow(R)]
      
      # transpose data
      Q <- t(Q)
      R <- t(R)
      
      # format data as data frame
      Q <- as.data.frame(Q)
      R <- as.data.frame(R)
      
      # name columns with participant numbers
      colnames(Q) <- PNums
      colnames(R) <- PNums
      
      # obtain a list of correlations without duplicates
      # cor() returns a correlation matrix between Var1 and Var2
      # Var1 and Var2 are the variables being correlated
      # Freq is the correlation
      QWrite <- subset(as.data.frame(as.table(cor(Q, method = "spearman"))),
                       match(Var1, names(Q)) > match(Var2, names(Q)))
      
      RWrite <- subset(as.data.frame(as.table(cor(R, method = "spearman"))),
                       match(Var1, names(R)) > match(Var2, names(R)))
      
      # initialize the output in the first iteration
      if (stage == 1) {
        IC <- data.frame("P_P" = paste0(QWrite$Var1, '-', QWrite$Var2))
        IC$P1 <- as.numeric(as.character(QWrite$Var1))
        IC$P2 <- as.numeric(as.character(QWrite$Var2))
      }
      
      # prepare QWrite
      QWrite <- as.data.frame(QWrite$Freq)
      names(QWrite) <- paste0("Q", stage)
      
      # prepare RWrite for merge
      RWrite <- as.data.frame(RWrite$Freq)
      names(RWrite) <- paste0('R', stage)
      
      # merge
      IC <- cbind(IC, QWrite, RWrite)
    }
    
  }
  
  # append case & study info
  IC$survey <- survey
  IC$case <- case
  
  ## IC Points calculations ##
  IC$IC_PRE <- 1 - abs((IC$R1 - IC$Q1) / sqrt(2))
  IC$IC_POST <- 1 - abs((IC$R2 - IC$Q2) / sqrt(2))
  
  return(IC)
}

get_ind_DRI <- function(IC) {
  
  Plist <- unique(c(IC$P1, IC$P2))
  
  Plist <- Plist[order(Plist)]
  
  DRIInd <- data.frame('participant' = Plist)
  DRIInd$survey <- survey
  DRIInd$case <- data_case_study$Case[1]
  
  DRIInd <- DRIInd[c("survey", "case", "participant")]
  
  #Add individual-level metrics
  for (i in 1:length(Plist)) {
    DRIInd$DRIPre[i] <- dri_calc(
      data = IC  %>% filter(P1 == Plist[i] | P2 == Plist[i]),
      v1 = 'R1',
      v2 = 'Q1'
    )
    DRIInd$DRIPost[i] <- dri_calc(
      data = IC  %>% filter(P1 == Plist[i] | P2 == Plist[i]),
      v1 = 'R2',
      v2 = 'Q2'
    )
    
    # calculate updated DRI V2
    DRIInd$DRIPreV2[i] <- dri_calc_v2(
      data = IC  %>% filter(P1 == Plist[i] | P2 == Plist[i]),
      v1 = 'R1',
      v2 = 'Q1'
    )
    DRIInd$DRIPostV2[i] <- dri_calc_v2(
      data = IC  %>% filter(P1 == Plist[i] | P2 == Plist[i]),
      v1 = 'R2',
      v2 = 'Q2'
    )
    
    # calculate updated DRI V3
    DRIInd$DRIPreV3[i] <- dri_calc_v3(
      data = IC  %>% filter(P1 == Plist[i] | P2 == Plist[i]),
      v1 = 'R1',
      v2 = 'Q1'
    )
    DRIInd$DRIPostV3[i] <- dri_calc_v3(
      data = IC  %>% filter(P1 == Plist[i] | P2 == Plist[i]),
      v1 = 'R2',
      v2 = 'Q2'
    )
    
  }
  
  return(DRIInd)
  
}

get_case_DRI <- function(IC, type="human_only") {
   
  ## Group DRI level ##
  DRI_PRE <- dri_calc(data = IC, v1 = 'R1', v2 = 'Q1')
  DRI_POST <- dri_calc(data = IC, v1 = 'R2', v2 = 'Q2')
  
  ## Group DRI level V2 ##
  DRI_PRE_V2 <- dri_calc_v2(data = IC, v1 = 'R1', v2 = 'Q1')
  DRI_POST_V2 <- dri_calc_v2(data = IC, v1 = 'R2', v2 = 'Q2')
  
  ## Group DRI level V3 ##
  DRI_PRE_V3 <- dri_calc_v3(data = IC, v1 = 'R1', v2 = 'Q1')
  DRI_POST_V3 <- dri_calc_v3(data = IC, v1 = 'R2', v2 = 'Q2')
  
  # cat(survey, case, type, "\n")
  # cat(DRI_PRE, DRI_POST, "\n")
  # cat(DRI_PRE_V2, DRI_POST_V2, "\n")
  # cat(DRI_PRE_V3, DRI_POST_V3, "\n")
  
  #CaseDRI Dataframe
  DRI.Case <- tibble(
    survey = survey,
    case = case,
    type = type,
    DRI_PRE,
    DRI_PRE_V2,
    DRI_PRE_V3,
    DRI_POST,
    DRI_POST_V2,
    DRI_POST_V3,
  )
  
  #Tests for groups
  DRIOverallSig <- wilcox.test(IC$IC_POST,
                               IC$IC_PRE,
                               paired = TRUE,
                               alternative = "greater")
  DRIOverallSig_twoside <- wilcox.test(IC$IC_POST,
                                       IC$IC_PRE,
                                       paired = TRUE,
                                       alternative = "two.side")

  #Adding the results to case data
  DRI.Case$DRI_one_tailed_p <- DRIOverallSig$p.value
  DRI.Case$DRI_twoside_p <- DRIOverallSig_twoside$p.value
  
  return(DRI.Case)
  
}

mini_publics <- human_data %>%
  group_by(survey, Case) %>%
  summarise(.groups = "drop")

# get_llm_data <- function(provider, model, survey) {
#   llm_participant <- llm_data_aggregated %>%
#     filter(provider == !!provider, model == !!model, survey == !!survey)
#   return(llm_participant)
# }

get_ind_LLM_DRI <- function(data, provider, model) {
  
  llm_DRI <- data %>%
    filter(participant == 0) %>%
    select(-participant) %>%
    mutate(provider = !!provider, model = !!model) %>%
    relocate(provider, model, .before = 1)
  
  return(llm_DRI)
}

add_llm_participant <- function(data, provider, model, survey) {
  
  # get llm data
  llm_participant <- llm_data_aggregated %>%
    filter(provider == !!provider, model == !!model, survey == !!survey)
  
  # check if it exists
  if (nrow(llm_participant) == 0) {
    warn(paste("No participant found for", paste(provider, model, survey, sep = "/")))
  }
  
  # create 2 participants, PRE and POST
  llm_participants <- bind_rows(llm_participant, llm_participant)
  llm_participants$PNum <- 0 # PNum = 0 is LLM
    llm_participants$StageID <- c(1,2)
  
  data_with_llm <- bind_rows(data, llm_participants)
  
  return(data_with_llm)
  
}


DRIInd.LLMs <- list()

# for each study [1:N], N = 26
for (case_study in 1:nrow(mini_publics)) {

  # select study data
  survey <- mini_publics[case_study, ]$survey
  case <- mini_publics[case_study, ]$Case
  
  # get human data for this case study
  data_case_study <- human_data %>% filter(survey == !!survey &
                                             Case == !!case)
  
  # intersubject correlations (IC)
  IC <- get_IC(data_case_study, survey, case)

  ## GROUP DRI ##
  DRI.Case <- get_case_DRI(IC)
 
  ## INDIVIDUAL DRI ##
  DRIInd <- get_ind_DRI(IC)
  
  # get human average
  # NOTE: this should be the same as human_only group DRI
  human_ind_DRI_mean <- tibble(
    DRIPre = mean(DRIInd$DRIPre),
    DRIPost = mean(DRIInd$DRIPost)
  )
  
  human_ind_DRI_meanV2 <- tibble(
    DRIPreV2 = mean(DRIInd$DRIPreV2),
    DRIPostV2 = mean(DRIInd$DRIPostV2)
  )
  
  human_ind_DRI_meanV3 <- tibble(
    DRIPreV3 = mean(DRIInd$DRIPreV3),
    DRIPostV3 = mean(DRIInd$DRIPostV3)
  )
  
  # Global dataframes for depositing results
  # initialize *.Global
  if (case_study == 1) {
    IC.Global <- IC
    DRIInd.Global <- DRIInd
    DRI.Global <- DRI.Case
  }
  
  # append to *.Global
  else {
    IC.Global <- rbind(IC.Global, IC)
    DRIInd.Global <- rbind(DRIInd.Global, DRIInd)
    DRI.Global <- rbind(DRI.Global, DRI.Case)
  }
  
  # check if there are LLM data for this survey
  llms <- llm_surveys %>% filter(survey == !!survey)
  if (nrow(llms) == 0) {
    next
  }

  # iterate through each llm
  for (llm in 1:nrow(llms)) {
    
    provider <- llms[llm,]$provider
    model <- llms[llm,]$model
    type <- paste0("human+",paste(provider, model, sep = "/"))
    
    data_with_llm <- add_llm_participant(data_case_study, provider, model, survey)
    
    IC.LLM <- get_IC(data_with_llm, survey, case)
    DRI.Case.LLM <- get_case_DRI(IC.LLM, type)
    DRIInd.LLM <- get_ind_DRI(IC.LLM)
    DRIInd.LLM.Model <- get_ind_LLM_DRI(DRIInd.LLM, provider, model)
    
    ## DRI V1
    DRIInd.LLM.Model$human_only_DRIPre_mean <- human_ind_DRI_mean$DRIPre
    DRIInd.LLM.Model$human_only_DRIPost_mean <- human_ind_DRI_mean$DRIPost
    
    ## DRI V2
    DRIInd.LLM.Model$human_only_DRIPre_meanV2 <- human_ind_DRI_meanV2$DRIPreV2
    DRIInd.LLM.Model$human_only_DRIPost_meanV2 <- human_ind_DRI_meanV2$DRIPostV2
    
    ## DRI V3
    DRIInd.LLM.Model$human_only_DRIPre_meanV3 <- human_ind_DRI_meanV3$DRIPreV3
    DRIInd.LLM.Model$human_only_DRIPost_meanV3 <- human_ind_DRI_meanV3$DRIPostV3
    
    get_bm_index <- function(diff) {
      bm_index <- (diff + 2) / 4
      return(bm_index)
    }
    
    DRIInd.LLM.Model <- DRIInd.LLM.Model %>%
      mutate(DRIPre_diff = DRIPre - human_only_DRIPre_mean,
             DRIPost_diff = DRIPost - human_only_DRIPost_mean) %>%
      mutate(bm_index = get_bm_index(DRIPost_diff)) %>%
    
      ## benchmark V2
      mutate(DRIPre_diffV2 = DRIPreV2 - human_only_DRIPre_meanV2,
             DRIPost_diffV2 = DRIPostV2 - human_only_DRIPost_meanV2) %>%
      mutate(bm_indexV2 = get_bm_index(DRIPost_diffV2)) %>%
    
      ## benchmark V3
      mutate(DRIPre_diffV3 = DRIPreV3 - human_only_DRIPre_meanV3,
             DRIPost_diffV3 = DRIPostV3 - human_only_DRIPost_meanV3) %>%
      mutate(bm_indexV3 = get_bm_index(DRIPost_diffV3))
      
    
    DRIInd.LLMs[[length(DRIInd.LLMs) + 1]] <- DRIInd.LLM.Model
    
    DRI.Global <- rbind(DRI.Global, DRI.Case.LLM)

  }
  
} # end for each case study


DRIInd.LLMs <- bind_rows(DRIInd.LLMs)

missing <-setdiff(unique(llm_data$survey), unique(DRIInd.LLMs$survey))

if (length(missing) > 0) {
  warn(paste("Missing", missing, "from DRIInd.LLMs!"))
}

# add delta column
DRI.Global <- DRI.Global %>%
  mutate(DRI_DELTA = DRI_POST - DRI_PRE)

# write summary to file
write_csv(DRIInd.LLMs, paste(OUTPUT_DIR, "DRIInd_LLMs.csv", sep = "/"))
write_csv(DRI.Global, paste(OUTPUT_DIR, "DRI_global.csv", sep = "/"))

```

## DRI Benchmark

```{r benchmark index, fig.width=10,fig.height=6}

DRI_benchmark <- DRIInd.LLMs %>%
  filter(model %in% included_models_summary$model) %>%
  group_by(provider, model) %>%
  summarise(
    N = n(),
    .groups = "drop",
    agg_bm_index = mean(bm_indexV3, na.rm = TRUE)
  ) %>%
  filter(N == max(N)) %>% # only include models with all surveys
  arrange(desc(agg_bm_index))

included_models_summary <- included_models_summary %>%
  full_join(DRI_benchmark, by=c("provider", "model"))

DRI_benchmark %>%
  mutate(label = paste(provider, model, sep = "/")) %>%
  ggplot(aes(
    x = reorder(label, agg_bm_index),
    y = agg_bm_index,
    fill = provider
  )) +
  geom_bar(stat = "identity") +
  coord_flip() +
  geom_text(aes(label = round(agg_bm_index, 3)), hjust = -0.3, size = 3) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.background = element_rect(fill = "white"),
    legend.position = "none"
  ) +
  scale_y_continuous(limits = c(0, 1)) +
  labs(x = "", y = "DRI benchmark") -> plot

plot

ggsave(
  paste(OUTPUT_DIR, "benchmarkV1.png", sep = "/"),
  plot,
  width = 10,
  height = 6
)

DRIInd.LLMs %>%
  filter(model %in% DRI_benchmark$model) %>% # only include models with all surveys
  mutate(label = paste(provider, model, sep = "/")) %>%
  ggplot(aes(
    x = reorder(label, bm_indexV3, FUN = median, na.rm = TRUE),
    y = bm_indexV2,
    color = provider
  )) +
  geom_boxplot() +
  coord_flip() +
  theme_minimal() +
  theme(plot.background = element_rect(fill = "white"),
        legend.position = "none") + scale_y_continuous(limits = c(-0.1, 1)) +
  labs(x = "", y = "DRI benchmark") -> plot

plot

ggsave(
  paste(OUTPUT_DIR, "benchmarkV2.png", sep = "/"),
  plot,
  width = 10,
  height = 6
)


## Trying a 2-dimension benchmark
DRI_benchmark <- full_join(alpha_results, DRIInd.LLMs, by = c("provider", "model", "survey")) %>%
  filter(model %in% DRI_benchmark$model) # only include models with all surveys

DRI_benchmark %>%
  #  filter(provider == "google") %>%
  #mutate(label = paste(provider, model, sep = "/")) %>%
  group_by(provider, model, .groups = "drop") %>%
  summarise(
    mean_alpha = mean(alpha_all, na.rm = TRUE),
    mean_bm_index = mean(bm_indexV3, na.rm = TRUE),
    se_alpha = sd(alpha_all, na.rm = TRUE),
    se_bm_index = sd(bm_indexV3, na.rm = TRUE)
    
  ) %>%
  ggplot(aes(x = mean_alpha, y = mean_bm_index, color = provider)) +
  geom_point(size = 5) + # Adjust size as needed
  geom_text(aes(label = model), vjust = -2, size = 2) + # Add labels above each dot
  labs(x = "Cronbach's alpha (mean)", y = "DRI Benchmark Index (mean)", color = "Provider") +
  theme_minimal() +
  theme(plot.background = element_rect(fill = "white"),
        legend.position = "none") -> plot # Remove text from color legend

plot

ggsave(
  paste(OUTPUT_DIR, "benchmarkV3.png", sep = "/"),
  plot,
  width = 10,
  height = 6
)

```

# Francesco's DRI Analysis

```{r francescos analysis}


#library(viridis)
library(grid)
library(Metrics)  # For MAE and RMSE
library(patchwork)


DATA_LLM <- read.csv("analysis/DRIInd_LLMs.csv")

# Convert case and model to factors
DATA_LLM$case <- as.factor(DATA_LLM$case)
DATA_LLM$model <- as.factor(DATA_LLM$model)
#delta DRIPOSTV2-DRI_POST_HUMAN#
DATA_LLM$delta_post <- DATA_LLM$DRIPostV2 - DATA_LLM$human_only_DRIPost_meanV2
DATA_LLM$delta_pre <- DATA_LLM$DRIPreV2 - DATA_LLM$human_only_DRIPre_meanV2


#no outlier humans

DATA_LLM_filtered <- DATA_LLM %>%
  filter(!(case %in% c("Thalwil")))


# Subset where DRI is low
DATA_LLM_hiDRI <- subset(DATA_LLM, human_only_DRIPost_meanV2 > 0.1999999)#below randomness

# Subset where DRI is high
DATA_LLM_lowDRI <- subset(DATA_LLM, human_only_DRIPost_meanV2 <0.2)

###descriptive 1####
LLM_summary <- DATA_LLM %>%
  group_by(model,case) %>%
  summarise(
    DRI_POST = first(DRIPostV2),
    DRI_PRE = first(DRIPreV2)  )
print(LLM_summary)

#model
LLM_summary_2 <- DATA_LLM %>%
  group_by(model) %>%
  summarise(
    DRI_POST = first(DRIPostV2),
    DRI_PRE = first(DRIPreV2)  )
print(LLM_summary_2, n=47)

#provider
LLM_summary_3 <- DATA_LLM %>%
  group_by(provider) %>%
  summarise(
    DRI_POST = first(DRIPostV2),
    DRI_PRE = first(DRIPreV2)  )
print(LLM_summary_3, n=47)

####DESCRIPTIVE STATS#####
#coefficient of variation#
cv_calc <- function(x) {
  if (mean(x, na.rm = TRUE) == 0) return(NA)  # Avoid division by zero
  return((sd(x, na.rm = TRUE) / abs(mean(x, na.rm = TRUE))) * 100)  # CV formula
}

# Compute CV for each model
cv_results <- DATA_LLM %>%
  group_by(model) %>%
  summarise(
    CV_DRI_POST = cv_calc(DRIPostV2),
    CV_DRI_PRE = cv_calc(DRIPreV2)
  )

mean_results <- DATA_LLM %>%
  group_by(model) %>%
  summarise(
    CV_DRI_POST = mean(DRIPostV2),
    CV_DRI_PRE = mean(DRIPreV2)
  )
variance_results <- DATA_LLM %>%
  group_by(model) %>%
  summarise(
    CV_DRI_POST = var(DRIPostV2),
    CV_DRI_PRE = var(DRIPreV2)
  )


# Print results#
print(cv_results, n=46)
print(mean_results,n=46
)
print(variance_results, n=46)

#correlation#
cor.test(DATA_LLM$DRIPostV2, as.numeric(DATA_LLM$human_only_DRIPost_meanV2))

##PLOTS DESCRIPTIVE####

# Density plot of DRI_PRE by model
pre_dens<-ggplot(LLM_summary, aes(x = DRI_PRE, fill = model)) +
  geom_density(alpha = 0.6) +  # Add transparency to better see overlaps
  theme_minimal() +
  labs(title = "Density Plot of DRI_PRE by Model",
       x = "DRI_PRE",
       y = "Density") +
  theme(legend.position = "none")  # Remove legend

# Density plot for DRI_POST
post_dens<-ggplot(LLM_summary, aes(x = DRI_POST, fill = model)) +
  geom_density(alpha = 0.6) +
  theme_minimal() +
  labs(title = "Density Plot of DRI_POST by Model",
       x = "DRI_POST",
       y = "Density") +
  theme(legend.position = "none")
pre_dens+post_dens

#Plot Provider#
ggplot(LLM_summary_3, aes(x = DRI_PRE, y = DRI_POST, color = provider, label = provider)) +
  geom_point(size = 4) +  # Plot points
  geom_text(vjust = -1, hjust = 0.5, size = 3) +  # Add case labels
  theme_minimal() + 
  labs(title = "Comparison of DRI_PRE and DRI_POST by Model",
       x = "DRI_PRE ",
       y = "DRI_POST",
       color = "provider") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


#plot models#
ggplot(LLM_summary_2, aes(x = DRI_PRE, y = DRI_POST, color = model, label = model)) +
  geom_point(size = 4) +  # Plot points
  geom_text(vjust = -1, hjust = 0.5, size = 3) +  # Add case labels
  theme_minimal() + 
  labs(
    title = "Comparison of DRI_PRE and DRI_POST by Model",
    x = "DRI_PRE",
    y = "DRI_POST",
    color = "Model"
  ) +
  coord_fixed(xlim = c(-0.5, 0.55), ylim = c(-0.5, 0.55)) +  # Enforce same axis scale
  theme(legend.position = "none")


##plot case details##
LLM_summary_long <- LLM_summary %>%
  pivot_longer(cols = c(DRI_PRE, DRI_POST), names_to = "Metric", values_to = "Score")
ggplot(LLM_summary_long, aes(x = case, y = Score, fill = Metric)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ model) +
  theme_minimal() +
  labs(
    title = "Comparison of DRI_PRE and DRI_POST by Case and Model",
    x = NULL,
    y = "Score",
    fill = "Metric"
  ) +
  theme(axis.text.x = element_blank())

#Heatmap#
ggplot(LLM_summary_long, aes(x = case, y = model, fill = Score)) +
  geom_tile() +
  facet_wrap(~ Metric) +
  scale_fill_viridis_c() +
  theme_minimal() +
  labs(
    title = "Heatmap of DRI Scores by Case and Model",
    x = "Case",
    y = "Model",
    fill = "Score"
  ) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))


#bar plot pre post aggregated#

LLM_summary_long2 <- LLM_summary_2 %>%
  pivot_longer(cols = c(DRI_PRE, DRI_POST), names_to = "Metric", values_to = "Score") %>%
  mutate(Metric = factor(Metric, levels = c("DRI_PRE", "DRI_POST")))
ggplot(LLM_summary_long2, aes(x = case, y = Score, fill = Metric)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ model) +
  theme_minimal() +
  labs(
    title = "Comparison of DRI_PRE and DRI_POST by Case and Model",
    x = NULL,
    y = "Score",
    fill = "Metric"
  ) +
  theme(axis.text.x = element_blank())

##LLM cases DRI Boxplots#
ggplot(DATA_LLM, aes(x = case, y = DRIPostV2)) +
  geom_boxplot(fill = "lightblue") +
  geom_hline(yintercept = 0, linetype = "dotted", color = "red", size = 1) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Boxplot of DRIPost by Case",
       x = "Case",
       y = "DRIPost")

############BENCHMARKING###########

# First calculate the normalizing factor: range of human DRI (optional: use sd instead)
human_range <- max(DATA_LLM$human_only_DRIPost_meanV2, na.rm = TRUE) - min(DATA_LLM$human_only_DRIPost_meanV2, na.rm = TRUE)


# Prepare human-level points
human_points <- DATA_LLM %>%
  group_by(case) %>%
  summarise(DRIPostV2 = first(human_only_DRIPost_meanV2)) %>%
  mutate(Source = "Human Mean")

#Spearman, Delta, NRMSE, MAE#
llm_scores <- DATA_LLM %>%
  filter(!is.na(DRIPostV2) & !is.na(human_only_DRIPost_meanV2) & human_only_DRIPost_meanV2 != 0) %>%
  mutate(
    delta_post = as.numeric(delta_post),
    perc_error = abs((DRIPostV2 - human_only_DRIPost_meanV2) / human_only_DRIPost_meanV2)
  ) %>%
  group_by(model) %>%
  summarise(
    MAE = mae(DRIPostV2, human_only_DRIPost_meanV2),
    RMSE = rmse(DRIPostV2, human_only_DRIPost_meanV2),
    MAPE = mean(perc_error) * 100,  # mean absolute % error
    NMAE = MAE / human_range,   # normalized MAE by range
    NRMSE = RMSE / human_range,
    Spearman = cor(DRIPostV2, human_only_DRIPost_meanV2, method = "spearman"),
    delta = mean(delta_post, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(NRMSE)#arrange from the smallest NRMSE -> lower than 0.2 good, lower than 0.1 very good

print(llm_scores, n=46)
#MAE on average, the model's prediction is XXX units off from the human DRI score
#RMSE the root-mean-squared error is XXX, meaning larger mistakes are penalized more than small ones
#MAPE on average, the model's predictions deviate from human scores by XX% of the human value 
#NMAE the MAE is XX% (XX*100) of the full human DRI range - this is a normalized, scale-free error
#RMSE is XX% of the total range of human DRI scores - again, showing significant error relative to variability among humans
#Spearman the model preserves the rank order of human judgments (correlation ??? X.XX)
#Delta difference in DRI

# Use select() safely with dplyr's namespace
llm_scores_long <- llm_scores %>%
  dplyr::select(model,  delta) %>%#MAE, RMSE, NRMSE, Spearman,
  pivot_longer(cols = -model, names_to = "Metric", values_to = "Score")

#####PLOTS BENCHMARKS####
#plot delta#
ggplot(llm_scores_long, aes(x = reorder(model, Score), y = Score, color = Metric)) +
  geom_point(size = 3) +
  geom_hline(yintercept = 0, linetype = "dotted", color = "red") +  # vertical line at x=0 (after flip)
  facet_wrap(~ Metric, scales = "free_y") +
  coord_flip() +
  theme_minimal() +
  labs(
    title = "delta DRI: LLM Performance Compared to Human-Level DRI_POST",
    x = "Model",
    y = "Score",
    color = "Metric"
  ) +
  theme(
    axis.text.y = element_text(size = 8),
    legend.position = "none",
    strip.text = element_text(face = "bold")
  )

#plot Spearman correlation#
llm_scores_long <- llm_scores %>%
  dplyr::select(model,  Spearman) %>%#MAE, RMSE, NRMSE, ,
  pivot_longer(cols = -model, names_to = "Metric", values_to = "Score")


ggplot(llm_scores_long, aes(x = reorder(model, Score), y = Score, color = Metric)) +
  geom_point(size = 3, color = 'blue') +
  geom_hline(yintercept = 1, linetype = "dotted", color = "red") +  # vertical line at x=0 (after flip)
  facet_wrap(~ Metric, scales = "free_y") +
  coord_flip() +
  theme_minimal() +
  labs(
    title = "Spearman correlation: LLM Performance Compared to Human-Level DRI_POST",
    x = "Model",
    y = "Score",
    color = "Metric"
  ) +
  theme(
    axis.text.y = element_text(size = 8),
    legend.position = "none",
    strip.text = element_text(face = "bold")
  )

#plot NRMSE#
llm_scores_long <- llm_scores %>%
  dplyr::select(model,  NRMSE) %>%#MAE, RMSE, NRMSE, ,
  pivot_longer(cols = -model, names_to = "Metric", values_to = "Score")


ggplot(llm_scores_long, aes(x = reorder(model, Score), y = Score, color = Metric)) +
  geom_point(size = 3, color = 'darkgreen') +
  geom_hline(yintercept = 0, linetype = "dotted", color = "red") +  # vertical line at x=0 (after flip)
  facet_wrap(~ Metric, scales = "free_y") +
  coord_flip() +
  theme_minimal() +
  labs(
    title = "RMSE correlation: LLM Performance Compared to Human-Level DRI_POST",
    x = "Model",
    y = "Score",
    color = "Metric"
  ) +
  theme(
    axis.text.y = element_text(size = 8),
    legend.position = "none",
    strip.text = element_text(face = "bold"))

# Ensure Difference column exists
DATA_LLM <- DATA_LLM %>%
  mutate(Difference = DRIPostV2 - human_only_DRIPost_meanV2)

# Check if the Difference column is correctly created
head(DATA_LLM$Difference)

ggplot(DATA_LLM, aes(x = Difference, fill = model)) +
  geom_density(alpha = 0.3) +  # Density plot
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +  # Reference line at 0
  theme_minimal() +
  labs(title = "Distribution of Differences (DRIPost - DRI_POST_MEAN_H) per Model",
       x = "Difference", y = "Density") +
  facet_wrap(~ model, scales = "fixed") +  # Ensure all models have the same axis
  xlim(-3, 1) +  # Set x-axis limits
  ylim(0, 2.8
  ) +
  theme(legend.position="none")  # Set y-axis limits


#####Benchmarking without HUMAN DRI OUTLIERS####

# First calculate the normalizing factor: range of human DRI (optional: use sd instead)
human_range_out <- max(DATA_LLM_hiDRI$human_only_DRIPost_meanV2, na.rm = TRUE) - min(DATA_LLM_hiDRI$human_only_DRIPost_meanV2, na.rm = TRUE)
human_range_out


llm_scores_out <- DATA_LLM_hiDRI %>%
  filter(!is.na(DRIPostV2) & !is.na(human_only_DRIPost_meanV2) & human_only_DRIPost_meanV2 != 0) %>%
  mutate(
    delta_post = as.numeric(delta_post),
    perc_error = abs((DRIPostV2 - human_only_DRIPost_meanV2) / human_only_DRIPost_meanV2)
  ) %>%
  group_by(model) %>%
  summarise(
    MAE = mae(DRIPostV2, human_only_DRIPost_meanV2),
    RMSE = rmse(DRIPostV2, human_only_DRIPost_meanV2),
    MAPE = mean(perc_error) * 100,  # mean absolute % error
    NMAE = MAE / human_range_out,   # normalized MAE by range
    NRMSE = RMSE / human_range_out,
    Spearman = cor(DRIPostV2, human_only_DRIPost_meanV2, method = "spearman"),
    delta = mean(delta_post, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(NRMSE)

print(llm_scores_out, n=46)



######LLM score normalization####
# Define normalization functions
normalize_lower_better <- function(x) {
  if (max(x) == min(x)) {
    rep(1, length(x))  # All values are the same, so all are best
  } else {
    (max(x) - x) / (max(x) - min(x))
  }
}

normalize_higher_better <- function(x) {
  if (max(x) == min(x)) {
    rep(1, length(x))  # All values are the same, so all are best
  } else {
    (x - min(x)) / (max(x) - min(x))
  }
}

# Compute normalized scores
llm_scores$normalized_NRMSE <- normalize_lower_better(llm_scores$NRMSE)
llm_scores$normalized_Spearman <- normalize_higher_better(llm_scores$Spearman)
llm_scores$normalized_Delta <- normalize_lower_better(llm_scores$delta)

# Compute aggregate index
llm_scores$aggregate_index <- rowMeans(llm_scores[, c("normalized_NRMSE", "normalized_Spearman", "normalized_Delta")])

# Optionally, sort and display
llm_scores_sorted <- llm_scores[order(llm_scores$aggregate_index, decreasing = TRUE), ]
print(llm_scores_sorted, n=49)


######wilcoxon####
# Compute effect size for each model
wilcox_results <- DATA_LLM %>%
  group_by(model) %>%
  summarise(
    W = wilcox.test(DRIPostV2, human_only_DRIPost_meanV2, paired = FALSE, alternative = 'less')$statistic,
    p_value = wilcox.test(DRIPostV2, human_only_DRIPost_meanV2, paired = FALSE, alternative = 'less')$p.value,
    effect_size_r = W / sqrt(n())  # Effect size calculation
  ) %>%
  arrange(p_value)  # Sort by significance level

# Print results with effect size
print(wilcox_results, n=47)

```



### References
