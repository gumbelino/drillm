---
title: 'Deliberative Reasoning with(in) the Machine: Large-Language Models as Proxy for Collective Deliberation'
author: "Nardine Alnemr, Francesco Veri, Gustavo Umbelino"
date: "`r Sys.Date()`"
bibliography: ../bibliography/refs.bib
link-citations: true
csl: ../bibliography/apsa.csl
output:
  bookdown::html_document2: default
  html_document: default
  pdf_document: default
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# install.packages("gumbelino/deliberr")
library(deliberr)
library(readr)
library(readxl)
library(dplyr)
library(rstatix)
library(tidyr)
library(ggplot2)
library(psych)
library(gridExtra)
library(ggpubr)


LLM_DATA_DIR <- "../llm_data"
SURVEY_FILE <- "../data/surveys_v5.xlsx"
EXEC_LOG_FILE <- paste(LLM_DATA_DIR, "exec_log.csv", sep = "/")
LLMS_FILE <- "../private/pt2_llms.csv"
CASES_FILE <- "../data/deliberative_cases.csv"
PROMPTS_FILE <- "../prompts/prompts.csv"
OUTPUT_DIR <- "pt2/data"

```

# Large-Language Models (LLMs)

```{r models, warning=FALSE}

# get models info
models <- read_csv(LLMS_FILE, show_col_types = FALSE) %>%
  filter(included) %>% # only included "included" models]
  mutate(class = case_when(
    estimate > 1 ~ "top",
    estimate <= 1 ~"bottom",
    .default = "new")
  )

models %>%
  select(provider, model, estimate, class) %>% 
  arrange(-estimate, provider, model) %>%
  knitr::kable(caption = "LLMs", row.names = TRUE)

models_top <- models %>%
  filter(class != "bottom")

models_bottom <- models %>%
  filter(class == "bottom")

```

Building on our previous analysis, we selected models based on their performance. We chose 4 top[^1], which were consistently more consistent than chance, and 4 bottom models, which were consistently less consistent than chance in terms of deliberative reasoning.

[^1]: Note that `gemini-2.5-pro-preview-03-25` was replaced by `gemini-2.5-pro`, however, this version of the model became significantly slower and more expensive, since it has "thinking" enabled by default and cannot be toggled. As a result, we decided to use the *flash* version (`gemini-2.5-flash`), a lighter and cheaper alternative.

```{r cases, warning=FALSE, include=FALSE}

cases <- read_csv(CASES_FILE, show_col_types = FALSE)

cases %>% arrange(survey) %>%
  knitr::kable(caption = "Deliberative Cases", row.names = TRUE)

```

```{r surveys, warning=FALSE, include=FALSE}

# get deliberative survey names
survey_names <- unique(cases$survey)

get_surveys <- function(survey_names) {
  
  surveys <- list()
  
  # Iterate over each sheet in the workbook
  for (survey_name in survey_names) {

    # Read the current sheet into a data frame
    df <- read_excel(SURVEY_FILE, sheet = survey_name)
    
    # Check if required columns exist
    required_columns <- c("considerations", "policies", "scale_max", "q-method")
    missing_cols <- setdiff(required_columns, colnames(df))
    if (length(missing_cols) > 0) {
      cat(
        "Sheet",
        survey_name,
        "is missing the following columns:",
        paste(missing_cols, collapse = ", "),
        "\n\n"
      )
      next
    }
    
    # Calculate the number of non-NA rows in "considerations" column
    n_c <- sum(!is.na(df$considerations))
    
    # Calculate the number of non-NA rows in "policies" column
    n_p <- sum(!is.na(df$policies))
    
    # Extract integer values from "scale_max" column, assuming they are already integers
    scale_max <- as.integer(na.omit(df$scale_max))
    
    # Extract logical (boolean) values from "q-method" column
    q_method <- as.logical(na.omit(df$`q-method`))
    
    surveys[[length(surveys) + 1]] <- tibble(
      survey = survey_name,
      considerations = n_c,
      policies = n_p,
      scale_max,
      q_method,
    )
    
  }
  
  surveys <- bind_rows(surveys)
  surveys
  
}

surveys <- get_surveys(survey_names)

surveys %>% arrange(survey) %>%
  knitr::kable(caption = "Surveys", row.names = TRUE)

```

```{r prompts, warning=FALSE, include=FALSE}

prompts <- read_csv(PROMPTS_FILE, show_col_types = FALSE)

prompts %>% 
  group_by(type) %>%
  summarise(n = n(), .groups = "drop") %>%
  knitr::kable(caption = "Number of Prompts by Type")

prompts %>% arrange(type) %>%
  select(-article) %>%
  knitr::kable(caption = "System Prompts", row.names = TRUE)

```

```{r format LLM data, include=FALSE}

# source("get_llm_data.R")

llm_data <- read_csv("data/llm_data_clean.csv", show_col_types = FALSE)

```

# Cases, surveys, and roles

```{r climate}
# get cases
cc_cases <- cases %>% 
  filter(topic == "climate", survey != "fnqcj", survey != "forestera")
# we remove fnqcj and forestera because they are not consistent with our roles

# get surveys
cc_surveys <- unique(cc_cases$survey)

# get roles: ecologist ideology, climate perspectives, or devil's advocate
cc_roles <- prompts %>% filter(type != "ideology" | uid == "eco")

cc_cases %>%
  knitr::kable(caption = "Cases", row.names = TRUE)

surveys %>%
  filter(survey %in% cc_surveys) %>%
  knitr::kable(caption = "Surveys", row.names = TRUE)

cc_roles %>%
  knitr::kable(caption = "Roles", row.names = TRUE)

```

## System prompt

We instructed LLMs to play each of the roles described above by including a system instruction in each request following the pattern:

> _Answer the following prompts as **[article]** **[role]**, who **[description]**._

For example:

> _Answer the following prompts as **`r cc_roles[12,]$article`** **`r cc_roles[12,]$role`**, who **`r cc_roles[12,]$description`**._

```{r}

cc_llm_data <- llm_data %>%
  filter(survey %in% cc_surveys, prompt_uid %in% cc_roles$uid)

```

# Methods

## Data collection

We collected `r nrow(cc_llm_data)` responses generated by `r nrow(models)` models cross `r length(cc_surveys)` surveys and `r nrow(cc_roles)` roles described above. We prompted each LLM 5 times with the same prompt.

## Analysis

We calculated one DRI value per model/survey/role by treating each LLM response as one participant in a deliberation. The role "all" indicates that all roles were part of that deliberation (n = 60 participants, which equals 5 participants for each of the 12 roles). DRI plots are shown in Figure \@ref(fig:driplots).

```{r, include=FALSE}
res <- list()
plots <- list()


for (model in models$model) {

  for (survey in cc_surveys) {
  
    data <- llm_data %>% filter(model == !!model, survey == !!survey, prompt_uid %in% cc_roles$uid) %>%
        mutate(pnum = row_number()) # treat each response as a participant
    ic <- get_dri_ic(data)
    dri <- get_dri(ic)
    alpha <- get_dri_alpha(data)
    
    title <- paste(model, survey, sep = "/")
    # suffix <- paste("all", nrow(cc_roles), "climate roles")
    plot <- plot_dri_ic(ic, title, dri = dri)
    
    file_name <- paste0(model, "-", survey, ".png")
    
    # ggsave(
    #   paste("plots", file_name, sep="/"),
    #   plot,
    #   width = 8,
    #   height = 6
    # )
    plots[[length(plots)+1]] <- plot
    
    res[[length(res)+1]] <- tibble(
      model,
      survey,
      role = "all",
      dri,
      alpha,
      n = nrow(data),
    )
    
    
    for (role in cc_roles$uid) {
      
      data <- llm_data %>%
        filter(model == !!model, survey == !!survey, prompt_uid == role) %>%
        mutate(pnum = row_number()) # treat each response as a participant
      ic <- get_dri_ic(data)
      dri <- get_dri(ic) 
      alpha <- get_dri_alpha(data)
      
      res[[length(res)+1]] <- tibble(
        model,
        survey,
        role,
        dri,
        alpha,
        n = nrow(data),
      )
      
    }
    
    
    
  }
  
}

res <- bind_rows(res)

```

# Findings

## Consistency

We compared the compared top with bottom models in terms of consistency of DRI and Cronbach's Alpha (see top models in Figure \@ref(fig:top) and bottom models in Figure \@ref(fig:bottom)).

### Top models

```{r top, fig.cap="Top models", fig.width=14, fig.height=4}

cplots <- list()

# res %>%
#   arrange(model, survey, role) %>%
#   head(5) %>%
#   knitr::kable(caption = "Head (5) of DRI consistency cross climate roles", digits = 3)

res_top <- res %>%
  filter(model %in% models_top$model)

role_dri <- res_top %>%
  group_by(role) %>%
  get_summary_stats(type = "common") %>%
  filter(variable == "dri") %>%
  arrange(median)
  
order <- role_dri$role

res_top %>%
  ggboxplot(x = "role", y = "dri", title = "Distribution of DRI across climate roles", order = order, caption = "Each role has 12 data points: 4 top models x 3 surveys", xlab = "Role", ylab = "DRI") + geom_hline(yintercept = 0, linetype = 2, color = "red") -> plot

cplots[[length(cplots)+1]] <- plot


role_alpha_p <- res_top %>%
  group_by(role) %>%
  get_summary_stats(type = "common") %>%
  filter(variable == "alpha_p") %>%
  arrange(median)
  
order <- role_alpha_p$role

res_top %>%
  ggboxplot(x = "role", y = "alpha_p", title = "Distribution of Cronbach's alpha (policies) across climate roles", order = order, caption = "Each role has 12 data points: 4 top models x 3 surveys", xlab = "Role", ylab = "Cronbach's Alpha (policies)") -> plot

cplots[[length(cplots)+1]] <- plot


# res_top %>%
#   group_by(role) %>%
#   get_summary_stats(type = "common") %>%
#   filter(variable != "n") %>%
#   arrange(variable, -mean) %>%
#   knitr::kable(caption = "Consistency data (DRI and Cronbach's alpha)", digits = 3, row.names = TRUE)

grid.arrange(grobs = cplots, nrow = 1, ncol = 2)


```

We found that *top* LLMs are consistent across roles both in terms of DRI and Cronbach's Alpha (policies). The high DRI across roles (median = `r role_dri[role_dri$role == "all",]$median`; IQR = `r role_dri[role_dri$role == "all",]$iqr`) suggests that LLMs tend to consistenly align their considerations and policy preferences. The high Cronbach's alpha for their policy preferences (median = `r role_alpha_p[role_alpha_p$role == "all",]$median`; IQR = `r role_alpha_p[role_alpha_p$role == "all",]$iqr`) suggests that LLMs tend to agree on the ranking of their policy preferences.

### Bottom models

```{r bottom, fig.cap="Bottom models", fig.width=14, fig.height=4}

cplots <- list()

res_bottom <- res %>%
  filter(model %in% models_bottom$model)

role_dri <- res_bottom %>%
  group_by(role) %>%
  get_summary_stats(type = "common") %>%
  filter(variable == "dri") %>%
  arrange(median)
  
order <- role_dri$role

res_bottom %>%
  ggboxplot(x = "role", y = "dri", title = "Distribution of DRI across climate roles", order = order, caption = "Each role has 12 data points: 4 bottom models x 3 surveys", xlab = "Role", ylab = "DRI") + geom_hline(yintercept = 0, linetype = 2, color = "red") -> plot

cplots[[length(cplots)+1]] <- plot

role_alpha_p <- res_bottom %>%
  group_by(role) %>%
  get_summary_stats(type = "common") %>%
  filter(variable == "alpha_p") %>%
  arrange(median)
  
order <- role_alpha_p$role

res_bottom %>%
  ggboxplot(x = "role", y = "alpha_p", title = "Distribution of Cronbach's alpha (policies) across climate roles", order = order, caption = "Each role has 12 data points: 4 bottom models x 3 surveys", xlab = "Role", ylab = "Cronbach's Alpha (policies)") -> plot

cplots[[length(cplots)+1]] <- plot


# res_bottom %>%
#   group_by(role) %>%
#   get_summary_stats(type = "common") %>%
#   filter(variable != "n") %>%
#   arrange(variable, -mean) %>%
#   knitr::kable(caption = "Consistency data (DRI and Cronbach's alpha)", digits = 3, row.names = TRUE)

grid.arrange(grobs = cplots, nrow = 1, ncol = 2)

```

We also found that *bottom* LLMs are not consistent across roles in terms of DRI and less consistent than top models in terms of Cronbach's Alpha (policies). The low DRI across roles (median = `r role_dri[role_dri$role == "all",]$median`; IQR = `r role_dri[role_dri$role == "all",]$iqr`) suggests that LLMs tend to consistenly misalign their considerations and policy preferences. The Cronbach's alpha (lower than top models) for their policy preferences (median = `r role_alpha_p[role_alpha_p$role == "all",]$median`; IQR = `r role_alpha_p[role_alpha_p$role == "all",]$iqr`) suggests that LLMs tend to agree less on the ranking of their policy preferences than top models.

### Summary for each model

#### DRI
```{r dri-summary}
model_role_summary <- res %>%
  group_by(model, role) %>%
  summarise(mean_dri = mean(dri), .groups = "drop") %>%
  pivot_wider(names_from = model, values_from = mean_dri)


# find best model for each row
model_role_summary$best_model <- 
  colnames(model_role_summary)[max.col(model_role_summary[, -1],
                                       ties.method = "first") + 1]

model_role_summary %>%
  knitr::kable(caption = "Mean DRI across models and roles", digits = 3, row.names = TRUE)


```

#### Cronbach's Alpha (Policies)

```{r}
model_role_summary <- res %>%
  group_by(model, role) %>%
  summarise(mean_alpha_p = mean(alpha_p), .groups = "drop") %>%
  pivot_wider(names_from = model, values_from = mean_alpha_p)


# find best model for each row
model_role_summary$best_model <- 
  colnames(model_role_summary)[max.col(model_role_summary[, -1],
                                       ties.method = "first") + 1]

model_role_summary %>%
  knitr::kable(caption = "Mean alpha (policies) across models and roles", digits = 3, row.names = TRUE)


```

#### Cronbach's Alpha (Consideration)

```{r}
model_role_summary <- res %>%
  group_by(model, role) %>%
  summarise(mean_alpha_c = mean(alpha_c), .groups = "drop") %>%
  pivot_wider(names_from = model, values_from = mean_alpha_c)


# find best model for each row
model_role_summary$best_model <- 
  colnames(model_role_summary)[max.col(model_role_summary[, -1],
                                       ties.method = "first") + 1]

model_role_summary %>%
  knitr::kable(caption = "Mean alpha (considerations) across models and roles", digits = 3, row.names = TRUE)


```

## Model/Survey DRI Plots

These plots show a simulated deliberation across all 12 roles for each surveys and model.
Each simulated deliberation has 60 participants (12 roles with 5 participants each).

Note that bottom models are visually inconsistent.

```{r driplots, fig.cap="DRI Plots", fig.width=15, fig.height=40}

# Arrange in 4 rows x 5 columns
grid.arrange(grobs = plots,
             nrow = 8,
             ncol = 3,
             widths = rep(1, 3),
             heights = rep(1, 8)) -> plot

ggsave(
  paste("plots", "all_plots.png", sep="/"),
  plot,
  width = 15,
  height = 40
)

```

## Survey/Role DRI Plots

These plots show a simulated deliberation across all models in the same class
(i.e., top, bottom) for each role and survey. Each simulated deliberation has
20 participants (4 models with 5 participants each).

Note that top models are visually more consistent than bottom models.

### Top models

```{r, include=FALSE}

res <- list()
plots <- list()

for (role in cc_roles$uid) {

  for (survey in cc_surveys) {
    
    data <- llm_data %>%
      filter(survey == !!survey, prompt_uid == role, model %in% models_top$model) %>%
      mutate(pnum = row_number()) # treat each response as a participant
    ic <- get_dri_ic(data)
    dri <- get_dri(ic)
    alpha <- get_dri_alpha(data)
    
    title <- paste(role, survey, sep = "/")
    plot <- plot_dri_ic(ic, title, dri = dri)
    
    file_name <- paste0(role, "-", survey, ".png")
    

    plots[[length(plots)+1]] <- plot
    
    res[[length(res)+1]] <- tibble(
      role,
      survey,
      dri,
      alpha,
      n = nrow(data),
    )
    
  }
  
}

res <- bind_rows(res)


```

```{r, fig.width=15, fig.height=60}

# Arrange in 4 rows x 5 columns
grid.arrange(grobs = plots,
             nrow = 12,
             ncol = 3,
             widths = rep(1, 3),
             heights = rep(1, 12))

# ggsave(
#   paste("plots", "all_role_plots.png", sep="/"),
#   plot,
#   width = 5,
#   height = 12
# )

```

### Bottom models

```{r, include=FALSE}

res <- list()
plots <- list()

for (role in cc_roles$uid) {

  for (survey in cc_surveys) {
    
    data <- llm_data %>%
      filter(survey == !!survey, prompt_uid == role, model %in% models_bottom$model) %>%
      mutate(pnum = row_number()) # treat each response as a participant
    ic <- get_dri_ic(data)
    dri <- get_dri(ic)
    alpha <- get_dri_alpha(data)
    
    title <- paste(role, survey, sep = "/")
    plot <- plot_dri_ic(ic, title, dri = dri)
    
    file_name <- paste0(role, "-", survey, ".png")
    

    plots[[length(plots)+1]] <- plot
    
    res[[length(res)+1]] <- tibble(
      role,
      survey,
      dri,
      alpha,
      n = nrow(data),
    )
    
  }
  
}

res <- bind_rows(res)


```

```{r, fig.width=15, fig.height=60}

# Arrange in 4 rows x 5 columns
grid.arrange(grobs = plots,
             nrow = 12,
             ncol = 3,
             widths = rep(1, 3),
             heights = rep(1, 12))

# ggsave(
#   paste("plots", "all_role_plots.png", sep="/"),
#   plot,
#   width = 5,
#   height = 12
# )

```

## Permutation tests

```{r}

ITERATIONS = 10000

```

We conducted permutation tests with `r ITERATIONS` iterations to check which models are consistently 
consistent and which roles are consistently consistent.

### Models and Surveys: Which models are truly consistent across roles?

In this permutation test, we explore the likelihood that the consistency, measured by DRI, is due to chance across surveys and roles.

```{r, fig.cap="Survey/Model Permutation Test", fig.width=12, fig.height=10, warning=FALSE}

start_time <- Sys.time()

perms <- list()
summs <- list()
for (survey in cc_surveys) {
  
  for (model in models$model) {
    
    # get data (for climate roles only)
    data <- llm_data %>% filter(model == !!model, survey == !!survey, prompt_uid %in% cc_roles$uid) %>%
        mutate(pnum = row_number()) # treat each response as a participant
    
    # get raw permutation data for plotting
    perm <- permute_dri(data, iterations = ITERATIONS, summary = FALSE)
    perm$survey <- survey
    perm$model <- model
    
    perms[[length(perms)+1]] <- perm
    
    # get summary for table
    summ <- summarize_perm_dri(perm)
    summ$survey <- survey
    summ$model <- model
    
    summs[[length(summs)+1]] <- summ

  }
  
}

perms <- bind_rows(perms)
summs <- bind_rows(summs)


perms %>%
  gghistogram(
    x = "dri",
    facet.by = c("model", "survey"),
    fill = "source",
    add = "mean",
    rug = TRUE,
    title = "Survey/Model Permutation Test",
    caption = paste(ITERATIONS, "permutations"),
  ) -> plot


plot

summs %>%
  arrange(p) %>%
  knitr::kable(caption = "Survey/Model Permutation Summary", digits = 3)

end_time <- Sys.time()

elapsed_time <- difftime(end_time, start_time, units = "mins")

```

Most models seem to be consistent across roles. Few of the 10,000 permutations led to a higher DRI than the *observed* DRI, suggesting that the observed value is likely not due to chance.

Note that this permutation test took `r  round(elapsed_time, 1)` minutes to complete.

### Surveys and Roles: Are models trully consistent across roles?

In this permutation test, we explore the likelihood that the consistency, measured by DRI, is due to chance across surveys and roles.

```{r, fig.cap="Survey/Role Permutation Test", fig.width=8, fig.height=12, warning=FALSE}
perms <- list()
summs <- list()

start_time <- Sys.time()

for (survey in cc_surveys) {
  
  for (role in cc_roles$uid) {
    
    # get data
    data <- llm_data %>% filter(survey == !!survey, prompt_uid == role) %>%
        mutate(pnum = row_number()) # treat each response as a participant
    
    # get raw permutation data for plotting
    perm <- permute_dri(data, iterations = ITERATIONS, summary = FALSE)
    perm$survey <- survey
    perm$role <- role
    
    perms[[length(perms)+1]] <- perm
    
    # get summary for table
    summ <- summarize_perm_dri(perm)
    summ$survey <- survey
    summ$role <- role
    
    summs[[length(summs)+1]] <- summ

  }
  
}

perms <- bind_rows(perms)
summs <- bind_rows(summs)


perms %>%
  gghistogram(
    x = "dri",
    facet.by = c("role", "survey"),
    fill = "source",
    add = "mean",
    rug = TRUE,
    title = "Survey/Role Permutation Test",
    caption = paste(ITERATIONS, "permutations"),
  ) -> plot

plot

# summs %>%
#   group_by(role) %>%
#   summarise(sig = sum(p < 0.05)) %>%
#   arrange(sig) %>%
#   knitr::kable(caption = "Number of significant (p < 0.05) roles across the 3 surveys.", digits = 3)

# summs %>%
#   group_by(survey) %>%
#   summarise(sig = sum(p < 0.05)) %>%
#   arrange(sig) %>%
#   knitr::kable(caption = "Number of significant (p < 0.05) surveys across the 12 roles", digits = 3)
# 

summs %>%
  arrange(p) %>%
  knitr::kable(caption = "Survey/Role Permutation Summary", digits = 3)

end_time <- Sys.time()

elapsed_time <- difftime(end_time, start_time, units = "mins")
  
```

Note that this permutation test took `r  round(elapsed_time, 1)` minutes to complete.

# References
