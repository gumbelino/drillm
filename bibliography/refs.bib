
@inproceedings{ge_v-framer_2024,
	address = {New York, NY, USA},
	series = {{CHI} '24},
	title = {V-{FRAMER}: {Visualization} {Framework} for {Mitigating} {Reasoning} {Errors} in {Public} {Policy}},
	isbn = {9798400703300},
	shorttitle = {V-{FRAMER}},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642750},
	doi = {10.1145/3613904.3642750},
	abstract = {Existing data visualization design guidelines focus primarily on constructing grammatically-correct visualizations that faithfully convey the values and relationships in the underlying data. However, a designer may create a grammatically-correct visualization that still leaves audiences susceptible to reasoning misleaders, e.g. by failing to normalize data or using unrepresentative samples. Reasoning misleaders are especially pernicious when presenting public policy data, where data-driven decisions can affect public health, safety, and economic development. Through textual analysis, a formative evaluation, and iterative design with 19 policy communicators, we construct an actionable visualization design framework, V-FRAMER, that effectively synthesizes ways of mitigating reasoning misleaders. We discuss important design considerations for frameworks like V-FRAMER, including using concrete examples to help designers understand reasoning misleaders, and using a hierarchical structure to support example-based accessing. We further describe V-FRAMER’s congruence with current practice and how practitioners might integrate the framework into their existing workflows. Related materials available at: https://osf.io/q3uta/.},
	urldate = {2025-01-12},
	booktitle = {Proceedings of the 2024 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ge, Lily W. and Easterday, Matthew and Kay, Matthew and Dimara, Evanthia and Cheng, Peter and Franconeri, Steven L},
	month = may,
	year = {2024},
	pages = {1--15},
	file = {Full Text PDF:/Users/gus/Zotero/storage/3UZRN39G/Ge et al. - 2024 - V-FRAMER Visualization Framework for Mitigating R.pdf:application/pdf},
}

@incollection{ercan_deliberative_2022,
	edition = {1},
	title = {Deliberative {Reason} {Index}},
	copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
	isbn = {978-0-19-284892-5 978-0-19-194419-2},
	url = {https://academic.oup.com/book/44646/chapter/378695400},
	abstract = {Abstract
            The Deliberative Reasoning Index (DRI) is a method for assessing the positions formed by a group taking part in deliberation and their relationship to deliberative ideals of reasoning. The method is informed by a theory of reasoning as intrinsically a group process, and so it is best understood as an emergent group-level property. DRI captures the extent that a group coheres towards a shared understanding of the issue and takes seriously all relevant considerations in forming positions. The method involves measuring and aggregating the proportionality (intersubjective consistency) of differences between individuals regarding considerations, compared to their differences regarding preferred actions. DRI can be applied to analysis of how deliberative reasoning is improved, as well as perform comparisons across different cohorts. The method fills an important niche in the methodological inventory of deliberative democracy, evaluating outcomes without imposing external standards on their content.},
	language = {en},
	urldate = {2025-01-12},
	booktitle = {Research {Methods} in {Deliberative} {Democracy}},
	publisher = {Oxford University PressOxford},
	author = {Niemeyer, Simon and Veri, Francesco},
	editor = {Ercan, Selen A. and Asenbaum, Hans and Curato, Nicole and Mendonça, Ricardo F.},
	month = oct,
	year = {2022},
	doi = {10.1093/oso/9780192848925.003.0007},
	pages = {99--114},
	file = {Niemeyer and Veri - 2022 - Deliberative Reason Index.pdf:/Users/gus/Zotero/storage/PJRC4QQZ/Niemeyer and Veri - 2022 - Deliberative Reason Index.pdf:application/pdf},
}

@article{niemeyer_how_2024,
	title = {How {Deliberation} {Happens}: {Enabling} {Deliberative} {Reason}},
	volume = {118},
	copyright = {http://creativecommons.org/licenses/by/4.0},
	issn = {0003-0554, 1537-5943},
	shorttitle = {How {Deliberation} {Happens}},
	url = {https://www.cambridge.org/core/product/identifier/S0003055423000023/type/journal_article},
	doi = {10.1017/S0003055423000023},
	abstract = {We show, against skeptics, that however latent it may be in everyday life, the ability to reason effectively about politics can readily be activated when conditions are right. We justify a definition of deliberative reason, then develop and apply a Deliberative Reason Index (DRI) to analysis of 19 deliberative forums. DRI increases over the course of deliberation in the vast majority of cases, but the extent of this increase depends upon enabling conditions. Group building that activates deliberative norms makes the biggest difference, particularly in enabling participants to cope with complexity. Without group building, complexity becomes more difficult to surmount, and planned direct impact on policy decisions may actually impede reasoning where complexity is high. Our findings have implications beyond forum design for the staging of political discourse in the wider public sphere.},
	language = {en},
	number = {1},
	urldate = {2025-01-12},
	journal = {American Political Science Review},
	author = {Niemeyer, Simon and Veri, Francesco and Dryzek, John S. and Bächtiger, André},
	month = feb,
	year = {2024},
	pages = {345--362},
	file = {Niemeyer et al. - 2024 - How Deliberation Happens Enabling Deliberative Re.pdf:/Users/gus/Zotero/storage/KJJN84AG/Niemeyer et al. - 2024 - How Deliberation Happens Enabling Deliberative Re.pdf:application/pdf},
}

@article{tessler_ai_2024,
	title = {{AI} can help humans find common ground in democratic deliberation},
	volume = {386},
	url = {https://www.science.org/doi/10.1126/science.adq2852},
	doi = {10.1126/science.adq2852},
	abstract = {Finding agreement through a free exchange of views is often difficult. Collective deliberation can be slow, difficult to scale, and unequally attentive to different voices. In this study, we trained an artificial intelligence (AI) to mediate human deliberation. Using participants’ personal opinions and critiques, the AI mediator iteratively generates and refines statements that express common ground among the group on social or political issues. Participants (N = 5734) preferred AI-generated statements to those written by human mediators, rating them as more informative, clear, and unbiased. Discussants often updated their views after the deliberation, converging on a shared perspective. Text embeddings revealed that successful group statements incorporated dissenting voices while respecting the majority position. These findings were replicated in a virtual citizens’ assembly involving a demographically representative sample of the UK population.},
	number = {6719},
	urldate = {2025-01-12},
	journal = {Science},
	author = {Tessler, Michael Henry and Bakker, Michiel A. and Jarrett, Daniel and Sheahan, Hannah and Chadwick, Martin J. and Koster, Raphael and Evans, Georgina and Campbell-Gillingham, Lucy and Collins, Tantum and Parkes, David C. and Botvinick, Matthew and Summerfield, Christopher},
	month = oct,
	year = {2024},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eadq2852},
}

@article{borges_could_2024,
	title = {Could {ChatGPT} get an engineering degree? {Evaluating} higher education vulnerability to {AI} assistants},
	volume = {121},
	shorttitle = {Could {ChatGPT} get an engineering degree?},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.2414955121},
	doi = {10.1073/pnas.2414955121},
	abstract = {AI assistants, such as ChatGPT, are being increasingly used by students in higher education institutions. While these tools provide opportunities for improved teaching and education, they also pose significant challenges for assessment and learning outcomes. We conceptualize these challenges through the lens of vulnerability, the potential for university assessments and learning outcomes to be impacted by student use of generative AI. We investigate the potential scale of this vulnerability by measuring the degree to which AI assistants can complete assessment questions in standard university-level Science, Technology, Engineering, and Mathematics (STEM) courses. Specifically, we compile a dataset of textual assessment questions from 50 courses at the École polytechnique fédérale de Lausanne (EPFL) and evaluate whether two AI assistants, GPT-3.5 and GPT-4 can adequately answer these questions. We use eight prompting strategies to produce responses and find that GPT-4 answers an average of 65.8\% of questions correctly, and can even produce the correct answer across at least one prompting strategy for 85.1\% of questions. When grouping courses in our dataset by degree program, these systems already pass the nonproject assessments of large numbers of core courses in various degree programs, posing risks to higher education accreditation that will be amplified as these models improve. Our results call for revising program-level assessment design in higher education in light of advances in generative AI.},
	number = {49},
	urldate = {2025-01-12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Borges, Beatriz and Foroutan, Negar and Bayazit, Deniz and Sotnikova, Anna and Montariol, Syrielle and Nazaretzky, Tanya and Banaei, Mohammadreza and Sakhaeirad, Alireza and Servant, Philippe and Neshaei, Seyed Parsa and Frej, Jibril and Romanou, Angelika and Weiss, Gail and Mamooler, Sepideh and Chen, Zeming and Fan, Simin and Gao, Silin and Ismayilzada, Mete and Paul, Debjit and Schwaller, Philippe and Friedli, Sacha and Jermann, Patrick and Käser, Tanja and Bosselut, Antoine and {EPFL Grader Consortium} and {EPFL Data Consortium}},
	month = dec,
	year = {2024},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2414955121},
	file = {Full Text PDF:/Users/gus/Zotero/storage/JJLJY5FC/Borges et al. - 2024 - Could ChatGPT get an engineering degree Evaluatin.pdf:application/pdf},
}

@article{costello_durably_2024,
	title = {Durably reducing conspiracy beliefs through dialogues with {AI}},
	volume = {385},
	url = {https://www.science.org/doi/10.1126/science.adq1814},
	doi = {10.1126/science.adq1814},
	abstract = {Conspiracy theory beliefs are notoriously persistent. Influential hypotheses propose that they fulfill important psychological needs, thus resisting counterevidence. Yet previous failures in correcting conspiracy beliefs may be due to counterevidence being insufficiently compelling and tailored. To evaluate this possibility, we leveraged developments in generative artificial intelligence and engaged 2190 conspiracy believers in personalized evidence-based dialogues with GPT-4 Turbo. The intervention reduced conspiracy belief by {\textasciitilde}20\%. The effect remained 2 months later, generalized across a wide range of conspiracy theories, and occurred even among participants with deeply entrenched beliefs. Although the dialogues focused on a single conspiracy, they nonetheless diminished belief in unrelated conspiracies and shifted conspiracy-related behavioral intentions. These findings suggest that many conspiracy theory believers can revise their views if presented with sufficiently compelling evidence.},
	number = {6714},
	urldate = {2025-01-12},
	journal = {Science},
	author = {Costello, Thomas H. and Pennycook, Gordon and Rand, David G.},
	month = sep,
	year = {2024},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eadq1814},
	file = {Submitted Version:/Users/gus/Zotero/storage/C27RSP9F/Costello et al. - 2024 - Durably reducing conspiracy beliefs through dialog.pdf:application/pdf},
}

@article{motoki_more_2024,
	title = {More human than human: measuring {ChatGPT} political bias},
	volume = {198},
	issn = {1573-7101},
	shorttitle = {More human than human},
	url = {https://doi.org/10.1007/s11127-023-01097-2},
	doi = {10.1007/s11127-023-01097-2},
	abstract = {We investigate the political bias of a large language model (LLM), ChatGPT, which has become popular for retrieving factual information and generating content. Although ChatGPT assures that it is impartial, the literature suggests that LLMs exhibit bias involving race, gender, religion, and political orientation. Political bias in LLMs can have adverse political and electoral consequences similar to bias from traditional and social media. Moreover, political bias can be harder to detect and eradicate than gender or racial bias. We propose a novel empirical design to infer whether ChatGPT has political biases by requesting it to impersonate someone from a given side of the political spectrum and comparing these answers with its default. We also propose dose-response, placebo, and profession-politics alignment robustness tests. To reduce concerns about the randomness of the generated text, we collect answers to the same questions 100 times, with question order randomized on each round. We find robust evidence that ChatGPT presents a significant and systematic political bias toward the Democrats in the US, Lula in Brazil, and the Labour Party in the UK. These results translate into real concerns that ChatGPT, and LLMs in general, can extend or even amplify the existing challenges involving political processes posed by the Internet and social media. Our findings have important implications for policymakers, media, politics, and academia stakeholders.},
	language = {en},
	number = {1},
	urldate = {2025-01-12},
	journal = {Public Choice},
	author = {Motoki, Fabio and Pinho Neto, Valdemar and Rodrigues, Victor},
	month = jan,
	year = {2024},
	keywords = {Bias, C10, C89, ChatGPT, D83, L86, Large language models, Political bias, Z00},
	pages = {3--23},
	file = {Full Text PDF:/Users/gus/Zotero/storage/SQIHYN3L/Motoki et al. - 2024 - More human than human measuring ChatGPT political.pdf:application/pdf},
}

@inproceedings{li_llms_2024,
	address = {New York, NY, USA},
	series = {{LLM4Code} '24},
	title = {{LLMs} for {Relational} {Reasoning}: {How} {Far} are {We}?},
	isbn = {9798400705793},
	shorttitle = {{LLMs} for {Relational} {Reasoning}},
	url = {https://dl.acm.org/doi/10.1145/3643795.3648387},
	doi = {10.1145/3643795.3648387},
	abstract = {Large language models (LLMs) have revolutionized many areas (e.g. natural language processing, software engineering, etc.) by achieving state-of-the-art performance on extensive downstream tasks. Aiming to achieve robust and general artificial intelligence, there has been a surge of interest in investigating the reasoning ability of the LLMs. Whereas the textual and numerical reasoning benchmarks adopted by previous works are rather shallow and simple, it is hard to conclude that the LLMs possess strong reasoning ability by merely achieving positive results on these benchmarks. Recent efforts have demonstrated that the LLMs are poor at solving sequential decision-making problems that require common-sense planning by evaluating their performance on the reinforcement learning benchmarks. In this work, we conduct an in-depth assessment of several state-of-the-art LLMs' reasoning ability based on the inductive logic programming (ILP) benchmark, which is broadly recognized as a representative and challenging measurement for evaluating logic program induction/synthesis systems as it requires inducing strict cause-effect logic to achieve robust deduction on independent and identically distributed (IID) and out-of-distribution (OOD) test samples. Our evaluations illustrate that compared with the neural program induction systems which are much smaller in model size, the state-of-the-art LLMs are much poorer in terms of reasoning ability by achieving much lower performance and generalization using either natural language prompting or truth-value matrix prompting1.},
	urldate = {2025-01-16},
	booktitle = {Proceedings of the 1st {International} {Workshop} on {Large} {Language} {Models} for {Code}},
	publisher = {Association for Computing Machinery},
	author = {Li, Zhiming and Cao, Yushi and Xu, Xiufeng and Jiang, Junzhe and Liu, Xu and Teo, Yon Shin and Lin, Shang-Wei and Liu, Yang},
	month = sep,
	year = {2024},
	pages = {119--126},
	file = {Full Text PDF:/Users/gus/Zotero/storage/XHWZHX3I/Li et al. - 2024 - LLMs for Relational Reasoning How Far are We.pdf:application/pdf},
}

@article{sakaguchi_winogrande_2021,
	title = {{WinoGrande}: an adversarial winograd schema challenge at scale},
	volume = {64},
	issn = {0001-0782},
	shorttitle = {{WinoGrande}},
	url = {https://dl.acm.org/doi/10.1145/3474381},
	doi = {10.1145/3474381},
	abstract = {Commonsense reasoning remains a major challenge in AI, and yet, recent progresses on benchmarks may seem to suggest otherwise. In particular, the recent neural language models have reported above 90\% accuracy on the Winograd Schema Challenge (WSC), a commonsense benchmark originally designed to be unsolvable for statistical models that rely simply on word associations. This raises an important question---whether these models have truly acquired robust commonsense capabilities or they rely on spurious biases in the dataset that lead to an overestimation of the true capabilities of machine commonsense.To investigate this question, we introduce WinoGrande, a large-scale dataset of 44k problems, inspired by the original WSC, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) large-scale crowdsourcing, followed by (2) systematic bias reduction using a novel AFLITE algorithm that generalizes human-detectable word associations to machine-detectable embedding associations. Our experiments demonstrate that state-of-the-art models achieve considerably lower accuracy (59.4\%-79.1\%) on WINOGRANDE compared to humans (94\%), confirming that the high performance on the original WSC was inflated by spurious biases in the dataset.Furthermore, we report new state-of-the-art results on five related benchmarks with emphasis on their dual implications. On the one hand, they demonstrate the effectiveness of WINOGRANDE when used as a resource for transfer learning. On the other hand, the high performance on all these benchmarks suggests the extent to which spurious biases are prevalent in all such datasets, which motivates further research on algorithmic bias reduction.},
	number = {9},
	urldate = {2025-01-17},
	journal = {Commun. ACM},
	author = {Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
	month = aug,
	year = {2021},
	pages = {99--106},
	file = {Full Text PDF:/Users/gus/Zotero/storage/594NXA4L/Sakaguchi et al. - 2021 - WinoGrande an adversarial winograd schema challen.pdf:application/pdf},
}

@inproceedings{wei_chain--thought_2024,
	address = {Red Hook, NY, USA},
	series = {{NIPS} '22},
	title = {Chain-of-thought prompting elicits reasoning in large language models},
	isbn = {978-1-71387-108-8},
	abstract = {We explore how generating a chain of thought—a series of intermediate reasoning steps—significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-of-thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting.Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2025-01-20},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed H. and Le, Quoc V. and Zhou, Denny},
	month = apr,
	year = {2024},
	pages = {24824--24837},
	file = {Wei et al. - 2024 - Chain-of-thought prompting elicits reasoning in la.pdf:/Users/gus/Zotero/storage/G3YCCWGQ/Wei et al. - 2024 - Chain-of-thought prompting elicits reasoning in la.pdf:application/pdf},
}

@article{gastil_designing_2025,
	title = {Designing a personal voting guide: {A} model for electoral deliberation online},
	volume = {41},
	issn = {0197-2243},
	shorttitle = {Designing a personal voting guide},
	url = {https://doi.org/10.1080/01972243.2024.2399201},
	doi = {10.1080/01972243.2024.2399201},
	abstract = {Evidence from the Oregon Citizens’ Initiative Review (CIR) shows the efficacy of a deliberative minipublic serving as a trusted information proxy for an electorate that votes on initiatives and referenda. This paper builds on that model by envisioning a decentralized online process that offers the function of the CIR without the planning and expense of convening the minipublic itself. The resulting model, which we call the Personal Voting Guide, draws on experiments in digital deliberation and provides a scalable process for generating key findings and pro/con arguments on referenda and initiatives, as well as guidance for choosing among parties or candidates. The proposed model addresses the same challenges that inspired the CIR by ensuring trustworthy, high-quality information and motivating voters to use that information before completing their ballots. It also emphasizes the potential for users to customize their voting guides on ballot issues, candidates, and parties based on varied political values, source perceptions, and tolerance for information complexity.},
	number = {1},
	urldate = {2025-01-20},
	journal = {The Information Society},
	author = {Gastil, John and Felicetti, Andrea},
	month = jan,
	year = {2025},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/01972243.2024.2399201},
	keywords = {Democratic innovation, digital voting guides, mass deliberation, public opinion, voter decision making},
	pages = {51--67},
	file = {Gastil and Felicetti - 2025 - Designing a personal voting guide A model for ele.pdf:/Users/gus/Zotero/storage/LZBQBGGD/Gastil and Felicetti - 2025 - Designing a personal voting guide A model for ele.pdf:application/pdf},
}
