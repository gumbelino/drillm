
@inproceedings{ge_v-framer_2024,
	address = {New York, NY, USA},
	series = {{CHI} '24},
	title = {V-{FRAMER}: {Visualization} {Framework} for {Mitigating} {Reasoning} {Errors} in {Public} {Policy}},
	isbn = {9798400703300},
	shorttitle = {V-{FRAMER}},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642750},
	doi = {10.1145/3613904.3642750},
	abstract = {Existing data visualization design guidelines focus primarily on constructing grammatically-correct visualizations that faithfully convey the values and relationships in the underlying data. However, a designer may create a grammatically-correct visualization that still leaves audiences susceptible to reasoning misleaders, e.g. by failing to normalize data or using unrepresentative samples. Reasoning misleaders are especially pernicious when presenting public policy data, where data-driven decisions can affect public health, safety, and economic development. Through textual analysis, a formative evaluation, and iterative design with 19 policy communicators, we construct an actionable visualization design framework, V-FRAMER, that effectively synthesizes ways of mitigating reasoning misleaders. We discuss important design considerations for frameworks like V-FRAMER, including using concrete examples to help designers understand reasoning misleaders, and using a hierarchical structure to support example-based accessing. We further describe V-FRAMER’s congruence with current practice and how practitioners might integrate the framework into their existing workflows. Related materials available at: https://osf.io/q3uta/.},
	urldate = {2025-01-12},
	booktitle = {Proceedings of the 2024 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ge, Lily W. and Easterday, Matthew and Kay, Matthew and Dimara, Evanthia and Cheng, Peter and Franconeri, Steven L},
	month = may,
	year = {2024},
	pages = {1--15},
	file = {Full Text PDF:/Users/gus/Zotero/storage/3UZRN39G/Ge et al. - 2024 - V-FRAMER Visualization Framework for Mitigating R.pdf:application/pdf},
}

@incollection{ercan_deliberative_2022,
	edition = {1},
	title = {Deliberative {Reason} {Index}},
	copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
	isbn = {978-0-19-284892-5 978-0-19-194419-2},
	url = {https://academic.oup.com/book/44646/chapter/378695400},
	abstract = {Abstract
            The Deliberative Reasoning Index (DRI) is a method for assessing the positions formed by a group taking part in deliberation and their relationship to deliberative ideals of reasoning. The method is informed by a theory of reasoning as intrinsically a group process, and so it is best understood as an emergent group-level property. DRI captures the extent that a group coheres towards a shared understanding of the issue and takes seriously all relevant considerations in forming positions. The method involves measuring and aggregating the proportionality (intersubjective consistency) of differences between individuals regarding considerations, compared to their differences regarding preferred actions. DRI can be applied to analysis of how deliberative reasoning is improved, as well as perform comparisons across different cohorts. The method fills an important niche in the methodological inventory of deliberative democracy, evaluating outcomes without imposing external standards on their content.},
	language = {en},
	urldate = {2025-01-12},
	booktitle = {Research {Methods} in {Deliberative} {Democracy}},
	publisher = {Oxford University PressOxford},
	author = {Niemeyer, Simon and Veri, Francesco},
	editor = {Ercan, Selen A. and Asenbaum, Hans and Curato, Nicole and Mendonça, Ricardo F.},
	month = oct,
	year = {2022},
	doi = {10.1093/oso/9780192848925.003.0007},
	pages = {99--114},
	file = {Niemeyer and Veri - 2022 - Deliberative Reason Index.pdf:/Users/gus/Zotero/storage/PJRC4QQZ/Niemeyer and Veri - 2022 - Deliberative Reason Index.pdf:application/pdf},
}

@article{niemeyer_how_2024,
	title = {How {Deliberation} {Happens}: {Enabling} {Deliberative} {Reason}},
	volume = {118},
	copyright = {http://creativecommons.org/licenses/by/4.0},
	issn = {0003-0554, 1537-5943},
	shorttitle = {How {Deliberation} {Happens}},
	url = {https://www.cambridge.org/core/product/identifier/S0003055423000023/type/journal_article},
	doi = {10.1017/S0003055423000023},
	abstract = {We show, against skeptics, that however latent it may be in everyday life, the ability to reason effectively about politics can readily be activated when conditions are right. We justify a definition of deliberative reason, then develop and apply a Deliberative Reason Index (DRI) to analysis of 19 deliberative forums. DRI increases over the course of deliberation in the vast majority of cases, but the extent of this increase depends upon enabling conditions. Group building that activates deliberative norms makes the biggest difference, particularly in enabling participants to cope with complexity. Without group building, complexity becomes more difficult to surmount, and planned direct impact on policy decisions may actually impede reasoning where complexity is high. Our findings have implications beyond forum design for the staging of political discourse in the wider public sphere.},
	language = {en},
	number = {1},
	urldate = {2025-01-12},
	journal = {American Political Science Review},
	author = {Niemeyer, Simon and Veri, Francesco and Dryzek, John S. and Bächtiger, André},
	month = feb,
	year = {2024},
	pages = {345--362},
	file = {Niemeyer et al. - 2024 - How Deliberation Happens Enabling Deliberative Re.pdf:/Users/gus/Zotero/storage/KJJN84AG/Niemeyer et al. - 2024 - How Deliberation Happens Enabling Deliberative Re.pdf:application/pdf},
}

@article{tessler_ai_2024,
	title = {{AI} can help humans find common ground in democratic deliberation},
	volume = {386},
	url = {https://www.science.org/doi/10.1126/science.adq2852},
	doi = {10.1126/science.adq2852},
	abstract = {Finding agreement through a free exchange of views is often difficult. Collective deliberation can be slow, difficult to scale, and unequally attentive to different voices. In this study, we trained an artificial intelligence (AI) to mediate human deliberation. Using participants’ personal opinions and critiques, the AI mediator iteratively generates and refines statements that express common ground among the group on social or political issues. Participants (N = 5734) preferred AI-generated statements to those written by human mediators, rating them as more informative, clear, and unbiased. Discussants often updated their views after the deliberation, converging on a shared perspective. Text embeddings revealed that successful group statements incorporated dissenting voices while respecting the majority position. These findings were replicated in a virtual citizens’ assembly involving a demographically representative sample of the UK population.},
	number = {6719},
	urldate = {2025-01-12},
	journal = {Science},
	author = {Tessler, Michael Henry and Bakker, Michiel A. and Jarrett, Daniel and Sheahan, Hannah and Chadwick, Martin J. and Koster, Raphael and Evans, Georgina and Campbell-Gillingham, Lucy and Collins, Tantum and Parkes, David C. and Botvinick, Matthew and Summerfield, Christopher},
	month = oct,
	year = {2024},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eadq2852},
}

@article{borges_could_2024,
	title = {Could {ChatGPT} get an engineering degree? {Evaluating} higher education vulnerability to {AI} assistants},
	volume = {121},
	shorttitle = {Could {ChatGPT} get an engineering degree?},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.2414955121},
	doi = {10.1073/pnas.2414955121},
	abstract = {AI assistants, such as ChatGPT, are being increasingly used by students in higher education institutions. While these tools provide opportunities for improved teaching and education, they also pose significant challenges for assessment and learning outcomes. We conceptualize these challenges through the lens of vulnerability, the potential for university assessments and learning outcomes to be impacted by student use of generative AI. We investigate the potential scale of this vulnerability by measuring the degree to which AI assistants can complete assessment questions in standard university-level Science, Technology, Engineering, and Mathematics (STEM) courses. Specifically, we compile a dataset of textual assessment questions from 50 courses at the École polytechnique fédérale de Lausanne (EPFL) and evaluate whether two AI assistants, GPT-3.5 and GPT-4 can adequately answer these questions. We use eight prompting strategies to produce responses and find that GPT-4 answers an average of 65.8\% of questions correctly, and can even produce the correct answer across at least one prompting strategy for 85.1\% of questions. When grouping courses in our dataset by degree program, these systems already pass the nonproject assessments of large numbers of core courses in various degree programs, posing risks to higher education accreditation that will be amplified as these models improve. Our results call for revising program-level assessment design in higher education in light of advances in generative AI.},
	number = {49},
	urldate = {2025-01-12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Borges, Beatriz and Foroutan, Negar and Bayazit, Deniz and Sotnikova, Anna and Montariol, Syrielle and Nazaretzky, Tanya and Banaei, Mohammadreza and Sakhaeirad, Alireza and Servant, Philippe and Neshaei, Seyed Parsa and Frej, Jibril and Romanou, Angelika and Weiss, Gail and Mamooler, Sepideh and Chen, Zeming and Fan, Simin and Gao, Silin and Ismayilzada, Mete and Paul, Debjit and Schwaller, Philippe and Friedli, Sacha and Jermann, Patrick and Käser, Tanja and Bosselut, Antoine and {EPFL Grader Consortium} and {EPFL Data Consortium}},
	month = dec,
	year = {2024},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2414955121},
	file = {Full Text PDF:/Users/gus/Zotero/storage/JJLJY5FC/Borges et al. - 2024 - Could ChatGPT get an engineering degree Evaluatin.pdf:application/pdf},
}

@article{costello_durably_2024,
	title = {Durably reducing conspiracy beliefs through dialogues with {AI}},
	volume = {385},
	url = {https://www.science.org/doi/10.1126/science.adq1814},
	doi = {10.1126/science.adq1814},
	abstract = {Conspiracy theory beliefs are notoriously persistent. Influential hypotheses propose that they fulfill important psychological needs, thus resisting counterevidence. Yet previous failures in correcting conspiracy beliefs may be due to counterevidence being insufficiently compelling and tailored. To evaluate this possibility, we leveraged developments in generative artificial intelligence and engaged 2190 conspiracy believers in personalized evidence-based dialogues with GPT-4 Turbo. The intervention reduced conspiracy belief by {\textasciitilde}20\%. The effect remained 2 months later, generalized across a wide range of conspiracy theories, and occurred even among participants with deeply entrenched beliefs. Although the dialogues focused on a single conspiracy, they nonetheless diminished belief in unrelated conspiracies and shifted conspiracy-related behavioral intentions. These findings suggest that many conspiracy theory believers can revise their views if presented with sufficiently compelling evidence.},
	number = {6714},
	urldate = {2025-01-12},
	journal = {Science},
	author = {Costello, Thomas H. and Pennycook, Gordon and Rand, David G.},
	month = sep,
	year = {2024},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eadq1814},
	file = {Submitted Version:/Users/gus/Zotero/storage/C27RSP9F/Costello et al. - 2024 - Durably reducing conspiracy beliefs through dialog.pdf:application/pdf},
}

@article{motoki_more_2024,
	title = {More human than human: measuring {ChatGPT} political bias},
	volume = {198},
	issn = {1573-7101},
	shorttitle = {More human than human},
	url = {https://doi.org/10.1007/s11127-023-01097-2},
	doi = {10.1007/s11127-023-01097-2},
	abstract = {We investigate the political bias of a large language model (LLM), ChatGPT, which has become popular for retrieving factual information and generating content. Although ChatGPT assures that it is impartial, the literature suggests that LLMs exhibit bias involving race, gender, religion, and political orientation. Political bias in LLMs can have adverse political and electoral consequences similar to bias from traditional and social media. Moreover, political bias can be harder to detect and eradicate than gender or racial bias. We propose a novel empirical design to infer whether ChatGPT has political biases by requesting it to impersonate someone from a given side of the political spectrum and comparing these answers with its default. We also propose dose-response, placebo, and profession-politics alignment robustness tests. To reduce concerns about the randomness of the generated text, we collect answers to the same questions 100 times, with question order randomized on each round. We find robust evidence that ChatGPT presents a significant and systematic political bias toward the Democrats in the US, Lula in Brazil, and the Labour Party in the UK. These results translate into real concerns that ChatGPT, and LLMs in general, can extend or even amplify the existing challenges involving political processes posed by the Internet and social media. Our findings have important implications for policymakers, media, politics, and academia stakeholders.},
	language = {en},
	number = {1},
	urldate = {2025-01-12},
	journal = {Public Choice},
	author = {Motoki, Fabio and Pinho Neto, Valdemar and Rodrigues, Victor},
	month = jan,
	year = {2024},
	keywords = {Bias, C10, C89, ChatGPT, D83, L86, Large language models, Political bias, Z00},
	pages = {3--23},
	file = {Full Text PDF:/Users/gus/Zotero/storage/SQIHYN3L/Motoki et al. - 2024 - More human than human measuring ChatGPT political.pdf:application/pdf},
}

@inproceedings{li_llms_2024,
	address = {New York, NY, USA},
	series = {{LLM4Code} '24},
	title = {{LLMs} for {Relational} {Reasoning}: {How} {Far} are {We}?},
	isbn = {9798400705793},
	shorttitle = {{LLMs} for {Relational} {Reasoning}},
	url = {https://dl.acm.org/doi/10.1145/3643795.3648387},
	doi = {10.1145/3643795.3648387},
	abstract = {Large language models (LLMs) have revolutionized many areas (e.g. natural language processing, software engineering, etc.) by achieving state-of-the-art performance on extensive downstream tasks. Aiming to achieve robust and general artificial intelligence, there has been a surge of interest in investigating the reasoning ability of the LLMs. Whereas the textual and numerical reasoning benchmarks adopted by previous works are rather shallow and simple, it is hard to conclude that the LLMs possess strong reasoning ability by merely achieving positive results on these benchmarks. Recent efforts have demonstrated that the LLMs are poor at solving sequential decision-making problems that require common-sense planning by evaluating their performance on the reinforcement learning benchmarks. In this work, we conduct an in-depth assessment of several state-of-the-art LLMs' reasoning ability based on the inductive logic programming (ILP) benchmark, which is broadly recognized as a representative and challenging measurement for evaluating logic program induction/synthesis systems as it requires inducing strict cause-effect logic to achieve robust deduction on independent and identically distributed (IID) and out-of-distribution (OOD) test samples. Our evaluations illustrate that compared with the neural program induction systems which are much smaller in model size, the state-of-the-art LLMs are much poorer in terms of reasoning ability by achieving much lower performance and generalization using either natural language prompting or truth-value matrix prompting1.},
	urldate = {2025-01-16},
	booktitle = {Proceedings of the 1st {International} {Workshop} on {Large} {Language} {Models} for {Code}},
	publisher = {Association for Computing Machinery},
	author = {Li, Zhiming and Cao, Yushi and Xu, Xiufeng and Jiang, Junzhe and Liu, Xu and Teo, Yon Shin and Lin, Shang-Wei and Liu, Yang},
	month = sep,
	year = {2024},
	pages = {119--126},
	file = {Full Text PDF:/Users/gus/Zotero/storage/XHWZHX3I/Li et al. - 2024 - LLMs for Relational Reasoning How Far are We.pdf:application/pdf},
}

@misc{noauthor_human_2022,
	title = {Human {Language} {Understanding} \& {Reasoning} {\textbar} {American} {Academy} of {Arts} and {Sciences}},
	url = {https://www.amacad.org/publication/daedalus/human-language-understanding-reasoning},
	abstract = {The last decade has yielded dramatic and quite surprising breakthroughs in natural language processing through the use of simple artificial neural network computations, replicated on a very large scale and trained over exceedingly large amounts of data. The resulting pretrained language models, such as BERT and GPT-3, have provided a powerful universal language understanding and generation base, which can easily be adapted to many understanding, writing, and reasoning tasks. These models show the first inklings of a more general form of artificial intelligence, which may lead to powerful foundation models in domains of sensory experience beyond just language.},
	language = {en},
	urldate = {2025-01-16},
	month = apr,
	year = {2022},
	file = {Snapshot:/Users/gus/Zotero/storage/3AZD6Q76/human-language-understanding-reasoning.html:text/html},
}

@article{sakaguchi_winogrande_2021,
	title = {{WinoGrande}: an adversarial winograd schema challenge at scale},
	volume = {64},
	issn = {0001-0782},
	shorttitle = {{WinoGrande}},
	url = {https://dl.acm.org/doi/10.1145/3474381},
	doi = {10.1145/3474381},
	abstract = {Commonsense reasoning remains a major challenge in AI, and yet, recent progresses on benchmarks may seem to suggest otherwise. In particular, the recent neural language models have reported above 90\% accuracy on the Winograd Schema Challenge (WSC), a commonsense benchmark originally designed to be unsolvable for statistical models that rely simply on word associations. This raises an important question---whether these models have truly acquired robust commonsense capabilities or they rely on spurious biases in the dataset that lead to an overestimation of the true capabilities of machine commonsense.To investigate this question, we introduce WinoGrande, a large-scale dataset of 44k problems, inspired by the original WSC, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) large-scale crowdsourcing, followed by (2) systematic bias reduction using a novel AFLITE algorithm that generalizes human-detectable word associations to machine-detectable embedding associations. Our experiments demonstrate that state-of-the-art models achieve considerably lower accuracy (59.4\%-79.1\%) on WINOGRANDE compared to humans (94\%), confirming that the high performance on the original WSC was inflated by spurious biases in the dataset.Furthermore, we report new state-of-the-art results on five related benchmarks with emphasis on their dual implications. On the one hand, they demonstrate the effectiveness of WINOGRANDE when used as a resource for transfer learning. On the other hand, the high performance on all these benchmarks suggests the extent to which spurious biases are prevalent in all such datasets, which motivates further research on algorithmic bias reduction.},
	number = {9},
	urldate = {2025-01-17},
	journal = {Commun. ACM},
	author = {Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
	month = aug,
	year = {2021},
	pages = {99--106},
	file = {Full Text PDF:/Users/gus/Zotero/storage/594NXA4L/Sakaguchi et al. - 2021 - WinoGrande an adversarial winograd schema challen.pdf:application/pdf},
}

@inproceedings{wei_chain--thought_2024,
	address = {Red Hook, NY, USA},
	series = {{NIPS} '22},
	title = {Chain-of-thought prompting elicits reasoning in large language models},
	isbn = {978-1-71387-108-8},
	abstract = {We explore how generating a chain of thought—a series of intermediate reasoning steps—significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-of-thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting.Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2025-01-20},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed H. and Le, Quoc V. and Zhou, Denny},
	month = apr,
	year = {2024},
	pages = {24824--24837},
	file = {Wei et al. - 2024 - Chain-of-thought prompting elicits reasoning in la.pdf:/Users/gus/Zotero/storage/G3YCCWGQ/Wei et al. - 2024 - Chain-of-thought prompting elicits reasoning in la.pdf:application/pdf},
}

@article{gastil_designing_2025,
	title = {Designing a personal voting guide: {A} model for electoral deliberation online},
	volume = {41},
	issn = {0197-2243},
	shorttitle = {Designing a personal voting guide},
	url = {https://doi.org/10.1080/01972243.2024.2399201},
	doi = {10.1080/01972243.2024.2399201},
	abstract = {Evidence from the Oregon Citizens’ Initiative Review (CIR) shows the efficacy of a deliberative minipublic serving as a trusted information proxy for an electorate that votes on initiatives and referenda. This paper builds on that model by envisioning a decentralized online process that offers the function of the CIR without the planning and expense of convening the minipublic itself. The resulting model, which we call the Personal Voting Guide, draws on experiments in digital deliberation and provides a scalable process for generating key findings and pro/con arguments on referenda and initiatives, as well as guidance for choosing among parties or candidates. The proposed model addresses the same challenges that inspired the CIR by ensuring trustworthy, high-quality information and motivating voters to use that information before completing their ballots. It also emphasizes the potential for users to customize their voting guides on ballot issues, candidates, and parties based on varied political values, source perceptions, and tolerance for information complexity.},
	number = {1},
	urldate = {2025-01-20},
	journal = {The Information Society},
	author = {Gastil, John and Felicetti, Andrea},
	month = jan,
	year = {2025},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/01972243.2024.2399201},
	keywords = {Democratic innovation, digital voting guides, mass deliberation, public opinion, voter decision making},
	pages = {51--67},
	file = {Gastil and Felicetti - 2025 - Designing a personal voting guide A model for ele.pdf:/Users/gus/Zotero/storage/LZBQBGGD/Gastil and Felicetti - 2025 - Designing a personal voting guide A model for ele.pdf:application/pdf},
}

@article{yang_harnessing_2024,
	title = {Harnessing the {Power} of {LLMs} in {Practice}: {A} {Survey} on {ChatGPT} and {Beyond}},
	volume = {18},
	issn = {1556-4681},
	shorttitle = {Harnessing the {Power} of {LLMs} in {Practice}},
	url = {https://dl.acm.org/doi/10.1145/3649506},
	doi = {10.1145/3649506},
	abstract = {This article presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream Natural Language Processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. First, we offer an introduction and brief summary of current language models. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, generation tasks, emergent abilities, and considerations for specific tasks. We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at . An LLMs evolutionary tree, editable yet regularly updated, can be found at  .},
	number = {6},
	urldate = {2025-02-04},
	journal = {ACM Trans. Knowl. Discov. Data},
	author = {Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han, Xiaotian and Feng, Qizhang and Jiang, Haoming and Zhong, Shaochen and Yin, Bing and Hu, Xia},
	month = apr,
	year = {2024},
	pages = {160:1--160:32},
	file = {Full Text PDF:/Users/gus/Zotero/storage/H75H3C62/Yang et al. - 2024 - Harnessing the Power of LLMs in Practice A Survey.pdf:application/pdf},
}

@article{lampinen_language_2024,
	title = {Language models, like humans, show content effects on reasoning tasks},
	volume = {3},
	issn = {2752-6542},
	url = {https://doi.org/10.1093/pnasnexus/pgae233},
	doi = {10.1093/pnasnexus/pgae233},
	abstract = {Abstract reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks but exhibit many imperfections. However, human abstract reasoning is also imperfect. Human reasoning is affected by our real-world knowledge and beliefs, and shows notable “content effects”; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns are central to debates about the fundamental nature of human intelligence. Here, we investigate whether language models—whose prior expectations capture some aspects of human knowledge—similarly mix content into their answers to logic problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art LMs, as well as humans, and find that the LMs reflect many of the same qualitative human patterns on these tasks—like humans, models answer more accurately when the semantic content of a task supports the logical inferences. These parallels are reflected in accuracy patterns, and in some lower-level features like the relationship between LM confidence over possible answers and human response times. However, in some cases the humans and models behave differently—particularly on the Wason task, where humans perform much worse than large models, and exhibit a distinct error pattern. Our findings have implications for understanding possible contributors to these human cognitive effects, as well as the factors that influence language model performance.},
	number = {7},
	urldate = {2025-02-06},
	journal = {PNAS Nexus},
	author = {Lampinen, Andrew K and Dasgupta, Ishita and Chan, Stephanie C Y and Sheahan, Hannah R and Creswell, Antonia and Kumaran, Dharshan and McClelland, James L and Hill, Felix},
	month = jul,
	year = {2024},
	pages = {pgae233},
	file = {Full Text PDF:/Users/gus/Zotero/storage/M22YMWJA/Lampinen et al. - 2024 - Language models, like humans, show content effects.pdf:application/pdf;pgae233_supplementary_data.pdf:/Users/gus/Zotero/storage/TQ8LFA7J/pgae233_supplementary_data.pdf:application/pdf;Snapshot:/Users/gus/Zotero/storage/L4L4Y2GC/7712372.html:text/html},
}

@misc{white_prompt_2023,
	title = {A {Prompt} {Pattern} {Catalog} to {Enhance} {Prompt} {Engineering} with {ChatGPT}},
	url = {http://arxiv.org/abs/2302.11382},
	doi = {10.48550/arXiv.2302.11382},
	abstract = {Prompt engineering is an increasingly important skill set needed to converse effectively with large language models (LLMs), such as ChatGPT. Prompts are instructions given to an LLM to enforce rules, automate processes, and ensure specific qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an LLM. This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs. Prompt patterns are a knowledge transfer method analogous to software patterns since they provide reusable solutions to common problems faced in a particular context, i.e., output generation and interaction when working with LLMs. This paper provides the following contributions to research on prompt engineering that apply LLMs to automate software development tasks. First, it provides a framework for documenting patterns for structuring prompts to solve a range of problems so that they can be adapted to different domains. Second, it presents a catalog of patterns that have been applied successfully to improve the outputs of LLM conversations. Third, it explains how prompts can be built from multiple patterns and illustrates prompt patterns that benefit from combination with other prompt patterns.},
	urldate = {2025-02-07},
	publisher = {arXiv},
	author = {White, Jules and Fu, Quchen and Hays, Sam and Sandborn, Michael and Olea, Carlos and Gilbert, Henry and Elnashar, Ashraf and Spencer-Smith, Jesse and Schmidt, Douglas C.},
	month = feb,
	year = {2023},
	note = {arXiv:2302.11382 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {Preprint PDF:/Users/gus/Zotero/storage/SZYPCCQU/White et al. - 2023 - A Prompt Pattern Catalog to Enhance Prompt Enginee.pdf:application/pdf;Snapshot:/Users/gus/Zotero/storage/MCRKKTCC/2302.html:text/html},
}

@misc{openai_prompt_2025,
	title = {Prompt engineering},
	url = {https://platform.openai.com},
	abstract = {Explore developer resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's platform.},
	language = {en},
	urldate = {2025-02-07},
	author = {{OpenAI}},
	year = {2025},
	file = {Snapshot:/Users/gus/Zotero/storage/6RP4KQTD/prompt-engineering.html:text/html},
}

@article{raftery_vote_2021,
	title = {The vote {Package}: {Single} {Transferable} {Vote} and {Other} {Electoral} {Systems} in {R}},
	volume = {13},
	issn = {2073-4859},
	shorttitle = {The vote {Package}},
	url = {https://rjournal.github.io/},
	abstract = {We describe the [*vote*](https://CRAN.R-project.org/package=vote) package in R, which implements the plurality (or first-past-the-post), two-round runoff, score, approval, and Single Transferable Vote (STV) electoral systems, as well as methods for selecting the Condorcet winner and loser. We emphasize the STV system, which we have found to work well in practice for multi-winner elections with small electorates, such as committee and council elections, and the selection of multiple job candidates. For single-winner elections, STV is also called Instant Runoff Voting (IRV), Ranked Choice Voting (RCV), or the alternative vote (AV) system. The package also implements the STV system with equal preferences, for the first time in a software package, to our knowledge. It also implements a new variant of STV, in which a minimum number of candidates from a specified group are required to be elected. We illustrate the package with several real examples.},
	number = {2},
	urldate = {2025-02-07},
	journal = {The R Journal},
	author = {Raftery, Adrian E. and Ševčı́ková, Hana and Silverman, Bernard W.},
	month = sep,
	year = {2021},
	pages = {673--696},
	file = {Full Text PDF:/Users/gus/Zotero/storage/DJTY6VBV/Raftery et al. - 2021 - The vote Package Single Transferable Vote and Oth.pdf:application/pdf},
}

@misc{krause_data_2024,
	title = {From {Data} to {Commonsense} {Reasoning}: {The} {Use} of {Large} {Language} {Models} for {Explainable} {AI}},
	shorttitle = {From {Data} to {Commonsense} {Reasoning}},
	url = {http://arxiv.org/abs/2407.03778},
	doi = {10.48550/arXiv.2407.03778},
	abstract = {Commonsense reasoning is a difficult task for a computer, but a critical skill for an artificial intelligence (AI). It can enhance the explainability of AI models by enabling them to provide intuitive and human-like explanations for their decisions. This is necessary in many areas especially in question answering (QA), which is one of the most important tasks of natural language processing (NLP). Over time, a multitude of methods have emerged for solving commonsense reasoning problems such as knowledge-based approaches using formal logic or linguistic analysis. In this paper, we investigate the effectiveness of large language models (LLMs) on different QA tasks with a focus on their abilities in reasoning and explainability. We study three LLMs: GPT-3.5, Gemma and Llama 3. We further evaluate the LLM results by means of a questionnaire. We demonstrate the ability of LLMs to reason with commonsense as the models outperform humans on different datasets. While GPT-3.5's accuracy ranges from 56\% to 93\% on various QA benchmarks, Llama 3 achieved a mean accuracy of 90\% on all eleven datasets. Thereby Llama 3 is outperforming humans on all datasets with an average 21\% higher accuracy over ten datasets. Furthermore, we can appraise that, in the sense of explainable artificial intelligence (XAI), GPT-3.5 provides good explanations for its decisions. Our questionnaire revealed that 66\% of participants rated GPT-3.5's explanations as either "good" or "excellent". Taken together, these findings enrich our understanding of current LLMs and pave the way for future investigations of reasoning and explainability.},
	urldate = {2025-02-07},
	publisher = {arXiv},
	author = {Krause, Stefanie and Stolzenburg, Frieder},
	month = jul,
	year = {2024},
	note = {arXiv:2407.03778 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/gus/Zotero/storage/S8NVCGDC/Krause and Stolzenburg - 2024 - From Data to Commonsense Reasoning The Use of Lar.pdf:application/pdf;Snapshot:/Users/gus/Zotero/storage/M6ERMCY6/2407.html:text/html},
}

@misc{wang_mmlu-pro_2024,
	title = {{MMLU}-{Pro}: {A} {More} {Robust} and {Challenging} {Multi}-{Task} {Language} {Understanding} {Benchmark}},
	shorttitle = {{MMLU}-{Pro}},
	url = {http://arxiv.org/abs/2406.01574},
	doi = {10.48550/arXiv.2406.01574},
	abstract = {In the age of large-scale language models, benchmarks like the Massive Multitask Language Understanding (MMLU) have been pivotal in pushing the boundaries of what AI can achieve in language comprehension and reasoning across diverse domains. However, as models continue to improve, their performance on these benchmarks has begun to plateau, making it increasingly difficult to discern differences in model capabilities. This paper introduces MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options. Additionally, MMLU-Pro eliminates the trivial and noisy questions in MMLU. Our experimental results show that MMLU-Pro not only raises the challenge, causing a significant drop in accuracy by 16\% to 33\% compared to MMLU but also demonstrates greater stability under varying prompts. With 24 different prompt styles tested, the sensitivity of model scores to prompt variations decreased from 4-5\% in MMLU to just 2\% in MMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT) reasoning achieved better performance on MMLU-Pro compared to direct answering, which is in stark contrast to the findings on the original MMLU, indicating that MMLU-Pro includes more complex reasoning questions. Our assessments confirm that MMLU-Pro is a more discriminative benchmark to better track progress in the field.},
	urldate = {2025-02-11},
	publisher = {arXiv},
	author = {Wang, Yubo and Ma, Xueguang and Zhang, Ge and Ni, Yuansheng and Chandra, Abhranil and Guo, Shiguang and Ren, Weiming and Arulraj, Aaran and He, Xuan and Jiang, Ziyan and Li, Tianle and Ku, Max and Wang, Kai and Zhuang, Alex and Fan, Rongqi and Yue, Xiang and Chen, Wenhu},
	month = nov,
	year = {2024},
	note = {arXiv:2406.01574 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/gus/Zotero/storage/P8DY86VA/Wang et al. - 2024 - MMLU-Pro A More Robust and Challenging Multi-Task.pdf:application/pdf;Snapshot:/Users/gus/Zotero/storage/6LKIG82S/2406.html:text/html},
}
